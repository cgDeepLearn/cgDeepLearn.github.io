<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[pdf-signature]]></title>
    <url>%2F2020%2F12%2F06%2Fpdf-signature%2F</url>
    <content type="text"><![CDATA[Java使用itext解析pdf签章 前言## ##]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>PDF</tag>
        <tag>itext</tag>
        <tag>signature</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单的python 微服务小框架]]></title>
    <url>%2F2020%2F05%2F18%2FMirco-service%2F</url>
    <content type="text"><![CDATA[快速开发你的小服务 前言编写文章中…… ## ## ##]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python下Select模块以及IO多路复用]]></title>
    <url>%2F2018%2F10%2F31%2FPython-Select%2F</url>
    <content type="text"><![CDATA[select模块以及IO多路复用 前言Python中的select模块专注于I/O多路复用，提供了select, poll, epoll三个方法(其中后两个在Linux中可用，windows仅支持select)，另外也提供了kqueue方法(freeBSD系统). select()的机制中提供一fd_set的数据结构，实际上是一long类型的数组， 每一个数组元素都能与一打开的文件句柄（不管是Socket句柄，还是其他文件或命名管道或设备句柄）建立联系，建立联系的工作由程序员完成， 当调用select()时，由内核根据IO状态修改fd_set的内容，由此来通知执行了select()的进程哪一Socket或文件可读或可写。 select主要用于socket通信当中，能监视我们需要的文件描述符变化。 非阻塞式I/O编程特点 如果发现一个I/O有输入，读取的过程中，另外一个也有了输入，这时候不会产生任何反应.这就需要你的程序语句去用到select函数的时候才知道有数据输入。 程序去select的时候，如果没有数据输入，程序会一直等待，直到有数据为止，也就是程序中无需循环和sleep。 Select在Socket编程中还是比较重要的，可是对于初学Socket的人来说都不太爱用Select写程序，他们只是习惯写诸如connect、accept、recv或recvfrom这样的阻塞程序（所谓阻塞方式block，顾名思义，就是进程或是线程执行到这些函数时必须等待某个事件的发生，如果事件没有发生，进程或线程就被阻塞，函数不能立即返回）。 可是使用Select就可以完成非阻塞（所谓非阻塞方式non-block，就是进程或线程执行此函数时不必非要等待事件的发生，一旦执行肯定返回，以返回值的不同来反映函数的执行情况，如果事件发生则与阻塞方式相同，若事件没有发生，则返回一个代码来告知事件未发生，而进程或线程继续执行，所以效率较高）方式工作的程序，它能够监视我们需要监视的文件描述符的变化情况——读写或是异常。 Select方法基本原理进程指定内核监听哪些文件描述符(最多监听1024个fd)的哪些事件，当没有文件描述符事件发生时，进程被阻塞；当一个或者多个文件描述符事件发生时，进程被唤醒。 当我们调用select()时： 上下文切换转换为内核态 将fd从用户空间复制到内核空间 内核遍历所有fd，查看其对应事件是否发生 如果没发生，将进程阻塞，当设备驱动产生中断或者timeout时间后，将进程唤醒，再次进行遍历 返回遍历后的fd 将fd从内核空间复制到用户空间 select函数方法参数1fd_r_list, fd_w_list, fd_e_list = select.select(rlist, wlist, xlist, [timeout]) 参数可接受四个参数（前三个必须）: rlist: wait until ready for reading wlist: wait until ready for writing xlist: wait for an “exceptional condition” timeout: 超时时间 返回值：三个列表select方法用来监视文件描述符(当文件描述符条件不满足时，select会阻塞)，当某个文件描述符状态改变后，会返回三个列表 当参数1 序列中的fd满足“可读”条件时，则获取发生变化的fd并添加到fd_r_list中 当参数2 序列中含有fd时，则将该序列中所有的fd添加到 fd_w_list中 当参数3 序列中的fd发生错误时，则将该发生错误的fd添加到 fd_e_list中 当超时时间为空，则select会一直阻塞，直到监听的句柄发生变化.当超时时间 ＝ n(正整数)时，那么如果监听的句柄均无任何变化，则select会阻塞n秒，之后返回三个空列表，如果监听的句柄有变化，则直接执行。 示例示例1:模拟select,同时监听多个端口 服务端 服务端select_server.py1234567891011121314151617181920212223242526272829303132333435# coding=utf-8"""模拟select,同时监听多个端口"""import socketimport selectHOST = ''PORT1, PORT2, PORT3 = 8001, 8002, 8003BUFSIZ = 1024ADDR1, ADDR2, ADDR3 = (HOST, PORT1), (HOST, PORT2), (HOST, PORT3)ss1 = socket.socket()ss1.bind(ADDR1)ss1.listen()ss2 = socket.socket()ss2.bind(ADDR2)ss2.listen()ss3 = socket.socket()ss3.bind(ADDR3)ss3.listen()inputs = [ss1, ss2, ss3]while True: r_list, w_list, e_list = select.select(inputs,[],inputs,1) for ss in r_list: # conn表示每一个连接对象 conn, address = ss.accept() conn.sendall(bytes('hello', encoding='utf-8')) conn.close() for ss in e_list: inputs.remove(ss) 客户端 客户端1select_client1.py12345678910111213141516171819# coding=utf-8"""客户端1"""import socketHOST = 'localhost'PORT = 8001BUFSIZ = 1024ADDR = (HOST, PORT)cs = socket.socket()cs.connect(ADDR)msg = cs.recv(BUFSIZ)print(msg.decode('utf-8'))cs.close() 客户端2select_client2.py12345678910111213141516171819# coding=utf-8"""客户端2"""import socketHOST = 'localhost'PORT = 8002BUFSIZ = 1024ADDR = (HOST, PORT)cs = socket.socket()cs.connect(ADDR)msg = cs.recv(BUFSIZ)print(msg.decode('utf-8'))cs.close() 运行server端和client端，客户端1,2均能连接。但是以上程序并不能同时对客户端的输入同时响应处理(两个客户端连接都没关闭的情况下)，下面就来介绍I/O多路复用的例子 示例2：IO多路复用–使用socket模拟多线程，并实现读写分离 服务端 服务端select_multi_server.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596# coding=utf-8"""使用socket模拟多线程，使多用户可以同时连接"""import socketimport selectimport queuefrom time import ctimeHOST = ''PORT = 8001BUFSIZ = 1024ADDR = (HOST, PORT)# 创建连接ss = socket.socket(socket.AF_INET, socket.SOCK_STREAM)#ss.setblocking(False)ss.bind(ADDR)ss.listen(5)inputs = [ss, ]outputs = []message_dict = &#123;&#125;while inputs: print('waiting for the next event...') r_list, w_list, e_list = select.select(inputs, outputs, inputs, 10) for s in r_list: # 判断当前触发的是不是服务端对象，当触发的是服务端对象时，说明有新客户端连接进来了 if s is ss: # 表示有新用户来连接 conn, addr = s.accept() print("connection from", addr) # 将客户端对象也加入到监听的列表中，当客户端发消息时select将触发 #conn.setblocking(0) inputs.append(conn) # 为连接的客户端单独创建一个消息队列，用来保存客户端发送的消息 message_dict[conn] = queue.Queue() else: # 有老用户发消息 try: data_bytes = s.recv(BUFSIZ) # 客户端未断开 #if data_bytes != '': data = data_bytes.decode('utf-8') print('received "%s" from %s' % (data, s.getpeername())) # 将收到的消息放到相对应的socket客户端的消息列表中 message_dict[s].put(data) # 将需要进行回复操作socket放到outputs列表中，让select监听 if s not in outputs: outputs.append(s) except Exception as e: # else: # 客户端断开了连接(或出现其他异常)，将客户端的监听从inputs列表中移除 print('closing', addr) if s in outputs: outputs.remove(s) inputs.remove(s) s.close() # 移除相应socket客户端对象的消息队列 del message_dict[s] # 处理发送消息列表 for s in w_list: try: # 如果消息队列中有消息，从消息队列中获取要发送的消息 message_queue = message_dict.get(s) send_data = '' if message_queue is not None: send_data = message_queue.get_nowait() else: # 客户端连接断开了 print('has closed') except queue.Empty: # 客户端连接断开了 print(s.getpeername()) outputs.remove(s) else: # 处理消息 if message_queue is not None: # 把接收到的数据加上时间戳再返回 s.send(("[%s] %s" % (ctime(), send_data)).encode('utf-8')) else: print("has closed") # 处理异常情况 for s in e_list: print("exception condition on", s.getpeername()) inputs.remove(s) if s in outputs: outputs.remove(s) s.close() del message_dict[s] 客户端 客户端select_multi_client.py123456789101112131415161718192021222324252627282930313233# coidng=utf-8"""客户端"""import socketHOST = 'localhost'PORT = 8001BUFSIZ = 1024ADDR = (HOST, PORT)sock_num = 2socks = [socket.socket(socket.AF_INET, socket.SOCK_STREAM) for _ in range(sock_num)]msgs = ["Hello", "I'm Robot", "Bye"]print("connecting to %s port %s..." % ADDR)# 连接到服务器for s in socks: s.connect(ADDR)for index, msg in enumerate(msgs): for s in socks: print('%s: sending "%s" %d' % (s.getpeername(), msg, index)) s.send(msg.encode('utf-8'))for s in socks: data = s.recv(BUFSIZ).decode("utf-8") print('%s: received "%s"' % (s.getpeername(),data)) # 接收到一个回复后就断开连接，我们就可以看看服务器端是如何处理之后的请求的 if data != "": print('closing socket', s.getsockname()) s.close() 运行结果 分别运行服务端和客户端程序： 服务端结果123456789101112131415161718192021222324$ python select_multi_server.pywaiting for the next event...connection from ('127.0.0.1', 9078)waiting for the next event...connection from ('127.0.0.1', 9079)received "HelloI'm Robot" from ('127.0.0.1', 9078)waiting for the next event...received "Bye" from ('127.0.0.1', 9078)received "HelloI'm RobotBye" from ('127.0.0.1', 9079)waiting for the next event...waiting for the next event...('127.0.0.1', 9078)('127.0.0.1', 9079)waiting for the next event...closing ('127.0.0.1', 9079)waiting for the next event...received "" from ('127.0.0.1', 9079)waiting for the next event...received "" from ('127.0.0.1', 9079)waiting for the next event...closing ('127.0.0.1', 9079)has closedhas closedwaiting for the next event... 客户端结果123456789101112$ python select_multi_client.pyconnecting to localhost port 8001...('127.0.0.1', 8001): sending "Hello" 0('127.0.0.1', 8001): sending "Hello" 0('127.0.0.1', 8001): sending "I'm Robot" 1('127.0.0.1', 8001): sending "I'm Robot" 1('127.0.0.1', 8001): sending "Bye" 2('127.0.0.1', 8001): sending "Bye" 2('127.0.0.1', 8001): received "[Wed Oct 31 09:41:05 2018] HelloI'm RobotBye"closing socket ('127.0.0.1', 9078)('127.0.0.1', 8001): received "[Wed Oct 31 09:41:05 2018] HelloI'm RobotBye"closing socket ('127.0.0.1', 9079) 多次运行程序，你会发现客户端程序返回结果里的received后面的略有不同，你发现其中的原因了吗！ select、poll、epoll区别select, poll, epoll 都是I/O多路复用的具体的实现，之所以有这三个存在，其实是因为他们出现是有先后顺序的。I/O多路复用这个概念被提出来以后， select是第一个实现 (1983 左右在BSD里面实现的)。 selectselect 被实现以后，很快就暴露出了很多问题: select 会修改传入的参数数组，这个对于一个需要调用很多次的函数，是非常不友好的。 每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大 select 如果任何一个sock(I/O stream)出现了数据，select仅仅会返回，但是并不会告诉你是那个sock上有数据，于是你只能自己一个一个的找，）每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大 select 只能监视1024个链接，linux 定义在头文件中的，参见FD_SETSIZE。 select 不是线程安全的，如果你把一个sock加入到select, 然后突然另外一个线程发现，尼玛，这个sock不用，要收回。对不起，这个select 不支持的，如果你丧心病狂的竟然关掉这个sock, select的标准行为是。。呃。。不可预测的， 于是14年以后(1997年）一帮人又实现了poll, poll 修复了select的很多问题 poll poll 去掉了1024个链接的限制，于是要多少链接呢， 主人你开心就好。 poll 从设计上来说，不再修改传入数组，不过这个要看你的平台了，所以行走江湖，还是小心为妙。 其实拖14年那么久也不是效率问题， 而是那个时代的硬件实在太弱，一台服务器处理1千多个链接简直就是神一样的存在了，select很长段时间已经满足需求。 但是poll仍然不是线程安全的， 这就意味着，不管服务器有多强悍，你也只能在一个线程里面处理一组I/O流。你当然可以那多进程来配合了，不过然后你就有了多进程的各种问题。 于是5年以后, 在2002, 大神 Davide Libenzi 实现了epoll. epollepoll 可以说是I/O 多路复用最新的一个实现，epoll 修复了poll 和select绝大部分问题, 比如： 对于每次需要将FD从用户态拷贝至内核态.epoll的解决方案在epoll_ctl函数中。每次注册新的事件到epoll句柄中时（在epoll_ctl中指定EPOLL_CTL_ADD），会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次。 同样epoll也没有1024的连接数限制 epoll 现在是线程安全的。 epoll 现在不仅告诉你sock组里面数据，还会告诉你具体哪个sock有数据，你不用自己去找了。 epoll的解决方案不像select或poll一样每次都把current轮流加入fd对应的设备等待队列中，而只在epoll_ctl时把current挂一遍（这一遍必不可少）并为每个fd指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而这个回调函数会把就绪的fd加入一个就绪链表）。epoll_wait的工作实际上就是在这个就绪链表中查看有没有就绪的fd（利用schedule_timeout()实现睡一会，判断一会的效果，和select实现中的第7步是类似的）。 I/O多路复用知友有话说 select/poll, epoll总结 select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。 select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。 epoll示例1:简单时间戳服务器 服务端 服务端epoll_simple_server.py1234567891011121314151617181920212223242526272829303132import socketimport selectfrom time import ctimes = socket.socket()s.bind(('127.0.0.1',8888))s.listen(5)epoll_obj = select.epoll()epoll_obj.register(s,select.EPOLLIN)connections = &#123;&#125;while True: events = epoll_obj.poll() for fd, event in events: print("fd : &#123;fd&#125; | event : &#123;event&#125;".format(fd=fd, event=event)) if fd == s.fileno(): conn, addr = s.accept() connections[conn.fileno()] = conn epoll_obj.register(conn,select.EPOLLIN) msg = conn.recv(200) conn.sendall(('OK, first input --- [%s] %s'% (ctime(), msg.decode('utf-8'))).encode()) else: try: fd_obj = connections[fd] msg = fd_obj.recv(200) fd_obj.sendall(('[%s] %s'% (ctime(), msg.decode('utf-8'))).encode()) except BrokenPipeError: epoll_obj.unregister(fd) connections[fd].close() del connections[fd]s.close()epoll_obj.close() 客户端 客户端epoll_simple_client.py1234567891011121314import socketflag = 1s = socket.socket()s.connect(('127.0.0.1',8888))while flag: input_msg = input('input&gt;&gt;&gt;') if input_msg == '0': break s.sendall(input_msg.encode()) msg = s.recv(1024) print(msg.decode())s.close() epoll示例2：读写分离的epoll 服务端 服务端epoll_server.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#!/usr/bin/env python# coding=utf-8import socketimport selectimport queuefrom time import ctime#创建socket对象ss = socket.socket(socket.AF_INET, socket.SOCK_STREAM)#设置IP地址复用ss.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)#ip地址和端口号SADDR = ("127.0.0.1", 8888)#绑定IP地址ss.bind(SADDR)#监听，并设置最大连接数ss.listen(10)print ("服务器启动成功，监听IP：" , SADDR)#服务端设置非阻塞ss.setblocking(False) #超时时间timeout = 10# bufsizeBUFSIZ = 1024#创建epoll事件对象，后续要监控的事件添加到其中epoll = select.epoll()#注册服务器监听fd到等待读事件集合epoll.register(ss.fileno(), select.EPOLLIN)#保存连接客户端消息的字典，格式为&#123;&#125;message_queues = &#123;&#125;#文件句柄到所对应对象的字典，格式为&#123;句柄：对象&#125;fd_to_socket = &#123;ss.fileno():ss,&#125;while True: print("等待活动连接......") #轮询注册的事件集合，返回值为[(文件句柄，对应的事件)，(...),....] events = epoll.poll(timeout) if not events: print("epoll超时无活动连接，重新轮询......") continue print("有&#123;num&#125;个新事件，开始处理......".format(num=len(events))) for fd, event in events: socket = fd_to_socket[fd] #如果活动socket为当前服务器socket，表示有新连接 if socket == ss: connection, address = ss.accept() print("新连接：" , address) #新连接socket设置为非阻塞 connection.setblocking(False) #注册新连接fd到待读事件集合 epoll.register(connection.fileno(), select.EPOLLIN) #把新连接的文件句柄以及对象保存到字典 fd_to_socket[connection.fileno()] = connection #以新连接的对象为键值，值存储在队列中，保存每个连接的信息 message_queues[connection] = queue.Queue() #关闭事件 elif event &amp; select.EPOLLHUP: print('client close') #在epoll中注销客户端的文件句柄 epoll.unregister(fd) #关闭客户端的文件句柄 fd_to_socket[fd].close() #在字典中删除与已关闭客户端相关的信息 del message_queues[fd_to_socket[fd]] del fd_to_socket[fd] #可读事件 elif event &amp; select.EPOLLIN: #接收数据 data = socket.recv(BUFSIZ) if data: data = data.decode("utf-8") print("收到数据：&#123;data&#125; , 客户端：&#123;client&#125;".format(data=data,client=socket.getpeername())) #将数据放入对应客户端的字典 message_queues[socket].put(data) #修改读取到消息的连接到等待写事件集合(即对应客户端收到消息后，再将其fd修改并加入写事件集合) epoll.modify(fd, select.EPOLLOUT) #可写事件 elif event &amp; select.EPOLLOUT: try: #从字典中获取对应客户端的信息 msg = message_queues[socket].get_nowait() except queue.Empty: print(socket.getpeername() , " queue empty") #修改文件句柄为读事件 epoll.modify(fd, select.EPOLLIN) else: print("发送数据: &#123;data&#125; 客户端：&#123;client&#125;".format(data=msg, client=socket.getpeername())) #发送数据 socket.send(('[%s] %s'% (ctime(), msg)).encode())#在epoll中注销服务端文件句柄epoll.unregister(ss.fileno())#关闭epollepoll.close()#关闭服务器socketss.close() 客户端 客户端epoll_client.py1234567891011121314151617181920212223import socket#创建客户端socket对象cs = socket.socket(socket.AF_INET,socket.SOCK_STREAM)#服务端IP地址和端口号元组server_address = ('127.0.0.1',8888)#客户端连接指定的IP地址和端口号cs.connect(server_address)BUFSIZE = 1024while True: #输入数据 data = input('input&gt;') if not data: break #客户端发送数据 cs.sendall(data.encode('utf-8')) #客户端接收数据 server_data = cs.recv(BUFSIZE) print('客户端收到的数据：',server_data.decode())#关闭客户端socketcs.close() 小结本文总结了I/O多路复用的三种方式select、poll、epoll，并使用python下select模块实现了以其为基础的时间戳服务端和客户端。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>select</tag>
        <tag>socket</tag>
        <tag>IO多路复用</tag>
        <tag>epoll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Socket编程]]></title>
    <url>%2F2018%2F10%2F24%2FSocket%2F</url>
    <content type="text"><![CDATA[Socket套接字 前言socket起源于Unix，而Unix/Linux基本哲学之一就是“一切皆文件”，对于文件用【打开】【读写】【关闭】模式来操作。socket就是该模式的一个实现，socket即是一种特殊的文件，一些socket函数就是对其进行的操作（读/写IO、打开、关闭）- socket和file的区别1. file模块是针对某个指定文件进行【打开】【读写】【关闭】2. socket模块是针对 服务器端 和 客户端 Socket 进行【打开】【读写】【关闭】下面我们通过几种不同方式来实现时间戳服务器端和客户端：TCP、UDP、SocketServer TCP、Twisted Reactor TCP TCP时间戳服务TCP服务器端TCP服务器端设计方式伪代码123456789ss = socket() # 创建服务器套接字ss.bind() # 套接字与地址绑定ss.listen() # 监听连接inf_loop: # 服务器无限循环 cs = ss.accept() # 接受客户端连接 comm_loop: # 通信循环 cs.recv()/cs.send() # 对话(接受/发送) cs.close() # 关闭客户端套接字ss.close() # 关闭服务器套接字(可选) 创建TCP时间戳服务器TCP时间戳服务器(tsTserv.py)12345678910111213141516171819202122232425262728293031323334#!/usr/bin/env python# coding=utf-8"""创建一个TCP服务器，它接受来自客户端的消息，然后将消息加上时间前缀并发送回客户端"""from socket import *from time import ctimeHOST = ''PORT =21567BUFSIZ = 1024ADDR = (HOST, PORT)tcpSerSock = socket(AF_INET, SOCK_STREAM)tcpSerSock.bind(ADDR)tcpSerSock.listen(5)while True: print("waiting for connection...") tcpCliSock, addr = tcpSerSock.accept() print("...connected from ", addr) while True: data = tcpCliSock.recv(BUFSIZ) if not data: break tcpCliSock.send(bytes('[%s] %s' % (ctime(), data.decode('utf-8')),'utf-8')) tcpCliSock.close()tcpSerSock.close() TCP客户端客户端伪代码12345cs = socket() # 创建客户端套接字cs.connect() # 尝试连接服务器comm_loop: # 通信循环 cs.send()/cs.recv() # 对话(发送/接收)cs.close() # 关闭客户端套接字 创建TCP时间戳客户端TCP时间戳客户端(tsTclnt.py)12345678910111213141516171819202122232425262728#!/usr/bin/env python# coding=utf-8""""""from socket import *HOST = 'localhost'PORT = 21567BUFSIZ = 1024ADDR = (HOST, PORT)tcpCliSock = socket(AF_INET, SOCK_STREAM)tcpCliSock.connect(ADDR)while True: data = input('&gt; ') if not data: break tcpCliSock.send(bytes(data,'utf-8')) data = tcpCliSock.recv(BUFSIZ) if not data: break print(data.decode('utf-8'))tcpCliSock.close() 执行TCP服务器和客户端 服务器端: 1234$ python tsTserv.pywaiting for connection......connected from ('127.0.0.1', 28182)waiting for connection 客户端 123456$ python tsTclnt.py&gt; Hi[Wed Oct 24 10:39:43 2018] Hi&gt; I'm Jack[Wed Oct 24 10:39:47 2018] I'm Jack&gt; UDP时间戳服务器UDP服务器端TCP服务器端设计方式伪代码12345ss = socket() # 创建服务器套接字ss.bind() # 绑定服务器套接字inf_loop: # 服务器无限循环 ss.recvfrom()/ss.sendto() # 接收/发送ss.close() # 关闭服务器套接字 创建UDP服务器UDP时间戳服务器(tsUserv.py)12345678910111213141516171819202122232425#!/usr/bin/env python# coding=utf-8"""UDP TimeStamp server"""from socket import *from time import ctimeHOST = ''PORT = 21567BUFSIZ = 1024ADDR = (HOST, PORT)udpSerSock = socket(AF_INET, SOCK_DGRAM)udpSerSock.bind(ADDR)while True: print("wating for message...") data, addr = udpSerSock.recvfrom(BUFSIZ) udpSerSock.sendto(bytes('[%s] %s' % (ctime(), data.decode('utf-8')), 'utf-8'), addr) print("...received from and returned to:", addr)udpSerSock.close() UDP客户端UDP客户端端设计方式伪代码1234cs = socket() # 创建客户端套接字comm_loop: # 通信循环 cs.sendto()/cs.recvfrom() # 对话(发送接收)cs.close() # 关闭客户端套接字 创建UDP客户端UDP时间戳客户端(tsUclnt.py)123456789101112131415161718192021222324252627#!/usr/bin/env python# codin=utf-8"""UDP TimeStamp Client"""from socket import *HOST = 'localhost'PORT = 21567BUFSIZ = 1024ADDR = (HOST, PORT)udpSerSock = socket(AF_INET, SOCK_DGRAM)while True: data = input('&gt; ') if not data: break udpSerSock.sendto(bytes(data,'utf-8'), ADDR) data, ADDR = udpSerSock.recvfrom(BUFSIZ) if not data: break print(data.decode('utf-8'))udpSerSock.close() SocketServer时间戳服务器端SocketServer时间戳TCP服务器(tsTservSS.py)1234567891011121314151617181920212223242526272829303132333435363738#!/usr/bin/env python# coding=utf-8"""SocketServer时间戳TCP服务器使用SocketServer类、TCPServer和StreamRequestHandler分叉，多线程windows不支持分叉"""from socketserver import (TCPServer as TCP, StreamRequestHandler as SRH, ForkingMixIn as FMI, ThreadingMixIn as TMI)from time import ctimeHOST = ''PORT = 21567ADDR = (HOST, PORT)class FServer(FMI, TCP): passclass TServer(TMI, TCP): passclass MyRequestHandler(SRH): def handle(self): print("...connected from:", self.client_address) self.wfile.write( ("[%s] %s" % (ctime(), self.rfile.readline().decode('utf-8'))).encode('utf-8'))tcpServ = TServer(ADDR, MyRequestHandler) # TCP, FSever, TServerprint("waiting for connection...")tcpServ.serve_forever() 客户端SocketServer时间戳TCP客户端(tsTclntSS.py)12345678910111213141516171819202122232425262728#!/usr/bin/env python# coding=utf-8"""SocketServer时间戳TCP客户端"""from socket import *HOST = 'localhost'PORT = 21567BUFSIZ = 1024ADDR = (HOST, PORT)while True: tcpCliSock = socket(AF_INET, SOCK_STREAM) tcpCliSock.connect(ADDR) data = input('&gt; ') if not data: break tcpCliSock.send(("%s\r\n" % data).encode('utf-8')) data = tcpCliSock.recv(BUFSIZ).decode('utf-8') if not data: break print(data.strip()) tcpCliSock.close() Twisted Reactor TCP时间戳服务器端Twisted Reactor时间戳TCP服务器(tsTservTW.py)123456789101112131415161718192021222324#!/usr/bin/env python# coding=utf-8"""Twisted Reactor时间戳TCP服务器，使用了Twisted Internet类"""from twisted.internet import protocol, reactorfrom time import ctimePORT = 21567class TSServProtocol(protocol.Protocol): def connectionMade(self): clnt = self.clnt = self.transport.getPeer().host print("...connected from:", clnt) def dataReceived(self, data): self.transport.write(("[%s] %s" % (ctime(), data.decode('utf-8'))).encode("utf-8"))factory = protocol.Factory()factory.protocol = TSServProtocolprint("waiting fro connection...")reactor.listenTCP(PORT, factory)reactor.run() 客户端Twisted Reactor时间戳TCP客户端(tsTclntTW.py)12345678910111213141516171819202122232425262728293031323334#!/usr/bin/env python# coding=utf-8"""Twisted Reactor时间戳TCP客户端"""from twisted.internet import protocol, reactorPORT = 21567HOST = 'localhost'class TSClntProtocol(protocol.Protocol): def sendData(self): data = input('&gt; ') if data: print("...sending %s..." % data) self.transport.write(data.encode('utf-8')) else: self.transport.loseConnection() def connectionMade(self): self.sendData() def dataReceived(self, data): print(data.decode()) self.sendData()class TSClntFactory(protocol.ClientFactory): protocol = TSClntProtocol clientConnectionLost = clientConnectionFailed = lambda self, connector, reason: reactor.stop()reactor.connectTCP(HOST, PORT, TSClntFactory())reactor.run() 小结我们用几种方式实现了一个时间戳服务器和客户端,下次我们将学习IO多路复用及python下的select模块]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>socket</tag>
        <tag>TCP</tag>
        <tag>UDP</tag>
        <tag>SocketServer</tag>
        <tag>Twisted</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gRPC简介及其在Python中使用]]></title>
    <url>%2F2018%2F10%2F22%2FgRPC%2F</url>
    <content type="text"><![CDATA[前言gRPC 是一个高性能、开源和通用的 RPC 框架，面向移动和 HTTP/2 设计。目前提供 C、Java 和 Go 语言版本，分别是：grpc, grpc-java, grpc-go. 其中 C 版本支持 C, C++, Node.js, Python, Ruby, Objective-C, PHP 和 C# 支持. gRPC 基于 HTTP/2 标准设计，带来诸如双向流、流控、头部压缩、单 TCP 连接上的多复用请求等特。这些特性使得其在移动设备上表现更好，更省电和节省空间占用。 简介gRPC是什么在 gRPC 里客户端应用可以像调用本地对象一样直接调用另一台不同的机器上服务端应用的方法，使得您能够更容易地创建分布式应用和服务。与许多 RPC 系统类似，gRPC 也是基于以下理念：定义一个服务，指定其能够被远程调用的方法（包含参数和返回类型）。在服务端实现这个接口，并运行一个 gRPC 服务器来处理客户端调用。在客户端拥有一个存根能够像服务端一样的方法。 gRPC 客户端和服务端可以在多种环境中运行和交互 - 从 google 内部的服务器到你自己的笔记本，并且可以用任何 gRPC 支持的语言来编写。所以，你可以很容易地用 Java 创建一个 gRPC 服务端，用 Go、Python、Ruby 来创建客户端。此外，Google 最新 API 将有 gRPC 版本的接口，使你很容易地将 Google 的功能集成到你的应用里。 使用protocol buffersgRPC 默认使用 protocol buffers，这是 Google 开源的一套成熟的结构数据序列化机制（当然也可以使用其他数据格式如 JSON）。正如你将在下方例子里所看到的，你用 proto files 创建 gRPC 服务，用 protocol buffers 消息类型来定义方法参数和返回类型。你可以在 Protocol Buffers 文档找到更多关于 Protocol Buffers 的资料。 安装准备Python环境123456$ python -m pip install --upgrade pip$ python -m pip install virtualenv$ virtualenv venv$ source venv/bin/activate$ python -m pip install --upgrade pip 安装gRPC12# 在之前激活的虚拟环境下运行pip install grpcio 同时还要安装gRPC tools: 1pip install grpcio-tools googleapis-common-protos 示例下载官方例子123git clone -b v1.15.0 https://github.com/grpc/grpc# Navigate to the &quot;hello, world&quot; Python example:$ cd grpc/examples/python/helloworld 运行一个gRPC应用在 examples/python/helloworld 目录中: 运行服务端： 1python greeter_server.py 在另一个terminal，运行客户端： 1python greeter_client.py Congratulations! You’ve just run a client-server application with gRPC. python编写一个RPC服务完整过程定义服务创建我们例子的第一步是定义一个服务：一个 RPC 服务通过参数和返回类型来指定可以远程调用的方法。 gRPC 通过 protocol buffers 来实现。我们使用 protocol buffers 接口定义语言来定义服务方法，用 protocol buffer 来定义参数和返回类型。客户端和服务端均使用服务定义生成的接口代码。 这里有我们服务定义的例子，在 helloworld.proto 里用 protocol buffers IDL 定义的。Greeter 服务有一个方法 SayHello ，可以让服务端从远程客户端接收一个包含用户名的 HelloRequest 消息后，在一个 HelloReply 里发送回一个 Greeter。这是你可以在 gRPC 里指定的最简单的 RPC - 你可以在教程里找到针对你选择的语言更多类型的例子。 123456789101112131415161718192021222324syntax = "proto3";option java_multiple_files = true;option java_package = "io.grpc.examples.helloworld";option java_outer_classname = "HelloWorldProto";option objc_class_prefix = "HLW";package helloworld;// The greeting service definition.service Greeter &#123; // Sends a greeting rpc SayHello (HelloRequest) returns (HelloReply) &#123;&#125;&#125;// The request message containing the user's name.message HelloRequest &#123; string name = 1;&#125;// The response message containing the greetingsmessage HelloReply &#123; string message = 1;&#125; 生成gRPC一旦定义好服务，我们可以使用 protocol buffer 编译器 protoc 来生成创建应用所需的特定客户端和服务端的代码 - 你可以生成任意 gRPC 支持的语言的代码，当然 PHP 和 Objective-C 仅支持创建客户端代码。生成的代码同时包括客户端的存根和服务端要实现的抽象接口，均包含 Greeter 所定义的方法。 12python -m grpc_tools.protoc -I . --python_out=. --grpc_python_out=. helloworld.proto# helloworld.proto为上面我们编写的proto文件 这生成了 helloworld_pb2.py和helloworld_pb2_grpc.py两个文件 ，包含我们生成的客户端和服务端类，此外还有用于填充、序列化、提取 HelloRequest 和 HelloResponse 消息类型的类。 编写服务器端代码服务实现greeter_server.py 实现了 Greeter 服务所需要的行为。正如你所见，Greeter 类通过实现 sayHello 方法，实现了从 proto 服务定义生成的helloworld_pb2.BetaGreeterServicer 接口： 1234class Greeter(helloworld_pb2.BetaGreeterServicer)：def SayHello(self, request, context)： return helloworld_pb2.HelloReply(message='Hello, %s!' % request.name) 为了返回给客户端应答并且完成调用： 用我们的激动人心的消息构建并填充一个在我们接口定义的 HelloReply 应答对象。将 HelloReply 返回给客户端。 服务端实现需要提供一个 gRPC 服务的另一个主要功能是让这个服务实在在网络上可用。 greeter_server.py 提供了以下代码作为 Python 的例子。 12345678server = helloworld_pb2.beta_create_Greeter_server(Greeter())server.add_insecure_port('[：：]：50051')server.start()try： while True： time.sleep(_ONE_DAY_IN_SECONDS)except KeyboardInterrupt： server.stop() 在这里我们创建了合理的 gRPC 服务器，将我们实现的 Greeter 服务绑定到一个端口。然后我们启动服务器：服务器现在已准备好从 Greeter 服务客户端接收请求。我们将在具体语言对应的文档里更深入地了解这所有的工作是怎样进行的。 编写客户端代码连接服务首先我们看一下我们如何连接 Greeter 服务器。我们需要创建一个 gRPC 频道，指定我们要连接的主机名和服务器端口。然后我们用这个频道创建存根实例。 123channel = implementations.insecure_channel('localhost', 50051)stub = helloworld_pb2.beta_create_Greeter_stub(channel)... 调用 RPC现在我们可以联系服务并获得一个 greeting ： 我们创建并填充一个 HelloRequest 发送给服务。 我们用请求调用存根的 SayHello()，如果 RPC 成功，会得到一个填充的 HelloReply ，从其中我们可以获得 greeting。 12response = stub.SayHello(helloworld_pb2.HelloRequest(name='you'), _TIMEOUT_SECONDS)print "Greeter client received： " + response.message greeter_client.py完整代码如下: 123456789101112131415161718import grpcimport helloworld_pb2import helloworld_pb2_grpcdef run(): # NOTE(gRPC Python Team): .close() is possible on a channel and should be # used in circumstances in which the with statement does not fit the needs # of the code. with grpc.insecure_channel('localhost:50051') as channel: stub = helloworld_pb2_grpc.GreeterStub(channel) response = stub.SayHello(helloworld_pb2.HelloRequest(name='you')) print("Greeter client received: " + response.message)if __name__ == '__main__': run() 运行并测试服务你可以尝试用同一个语言在客户端和服务端构建并运行例子。或者你可以尝试 gRPC 最有用的一个功能 - 不同的语言间的互操作性，即在不同的语言运行客户端和服务端。每个服务端和客户端使用从同一过 proto 文件生成的接口代码，则意味着任何 Greeter 客户端可以与任何 Greeter 服务端对话。 运行服务端程序,程序会监听 50051端口: 1python route_guide_server.py 运行客户端程序 1python route_guide_client.py grpc: 4种通信方式helloworld 使用了最简单的 grpc 通信方式: 类似 http 协议的一次 request+response. 4种通信方式根据不同的业务场景, grpc 支持 4 种通信方式: 客服端一次请求, 服务器一次应答 客服端一次请求, 服务器多次应答(流式) 客服端多次请求(流式), 服务器一次应答 客服端多次请求(流式), 服务器多次应答(流式) 官方提供了一个 route guide service 的 demo, 应用到了这 4 种通信方式, 具体的业务如下: 数据源: json 格式的数据源, 存储了很多地点, 每个地点由经纬度(point)和地名(location)组成 通信方式 1: 客户端请求一个地点是否在数据源中 通信方式 2: 客户端指定一个矩形范围(矩形的对角点坐标), 服务器返回这个范围内的地点信息 通信方式 3: 客户端给服务器发送多个地点信息, 服务器返回汇总信息(summary) 通信方式 4: 客户端和服务器使用地点信息 聊天(chat) 对应的proto文件route_guide.proto123456789101112131415161718192021222324252627282930313233343536373839404142434445464748syntax = "proto3";option java_multiple_files = true;option java_package = "io.grpc.examples.routeguide";option java_outer_classname = "RouteGuideProto";option objc_class_prefix = "RTG";package routeguide;// Interface exported by the server.service RouteGuide &#123; rpc GetFeature(Point) returns (Feature) &#123;&#125; rpc ListFeatures(Rectangle) returns (stream Feature) &#123;&#125; rpc RecordRoute( stream Point) returns (RouteSummary) &#123;&#125; rpc RouteChat(stream RouteNote) returns (stream RouteNote) &#123;&#125;&#125;message Point &#123; int32 latitude = 1; int32 longitude = 2;&#125;message Rectangle &#123; Point lo = 1; Point hi = 2;&#125;message Feature &#123; string name = 1; Point location = 2;&#125;message RouteNote &#123; Point location = 1; string message = 2;&#125;message RouteSummary &#123; int32 point_count = 1; int32 feature_count = 2; int32 distance = 3; int32 elapsed_time = 4;&#125; proto 中想要表示流式传输, 只需要添加 stream 关键字即可 同样的, 使用 protoc 生成代码: 1python -m grpc_tools.protoc --python_out=. --grpc_python_out=. -I. route_guide.proto 生成了 route_guide_pb2.py 和route_guide_pb2_grpc.py 文件 处理数据源文件(route_guide_db.json)route_guide_db.py12345678910111213import jsonimport route_guide_pb2def read_route_guide_db(): feature_list = [] with open('route_guide_db.json') as f: for item in json.load(f): feature = route_guide_pb2.Feature(name=item['name'], location=route_guide_pb2.Point(latitude=item['location']['latitude'], longitude=item['location']['longitude'])) feature_list.append(feature) return feature_list 处理 json 的过程很简单, 解析 json 数据得到由坐标点组成的数组 怎么处理流式数据呢?. 答案是 for ... in + yield 完整服务器端代码route_guide_server.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114"""The Python implementation of the gRPC route guide server."""from concurrent import futuresimport mathimport timeimport grpcimport route_guide_pb2import route_guide_pb2_grpcimport route_guide_db_ONE_DAY_IN_SECONDS = 60 * 60 * 24def get_feature(feature_db, point): """returns feature at given location or None""" for feature in feature_db: if feature.location == point: return feature return Nonedef get_distance(start, end): """Distance between two points.""" coord_factor = 10000000.0 lat_1 = start.latitude / coord_factor lat_2 = end.latitude / coord_factor lon_1 = start.longitude / coord_factor lon_2 = end.longitude / coord_factor lat_rad_1 = math.radians(lat_1) lat_rad_2 = math.radians(lat_2) delta_lat_rad = math.radians(lat_2 - lat_1) delta_lon_rad = math.radians(lon_2 - lon_2) # Formula is based on http://mathforum.org/library/drmath/view/51879.html a = (pow(math.sin(delta_lat_rad / 2), 2) + (math.cos(lat_rad_1) * math.cos(lat_rad_2) * pow(math.sin(delta_lon_rad / 2), 2))) c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a)) R = 6371000 return R * cclass RouteGuideServicer(route_guide_pb2_grpc.RouteGuideServicer): """Provides methods that implement functionality of route guide server.""" def __init__(self): self.db = route_guide_db.read_route_guide_db() def GetFeature(self, request, context): feature = get_feature(self.db, request) if feature is None: return route_guide_pb2.Feature(name="", location=request) else: return feature def ListFeatures(self, request, context): left = min(request.lo.longitude, request.hi.longitude) right = max(request.lo.longitude, request.hi.longitude) top = max(request.lo.latitude, request.hi.latitude) bottom = min(request.lo.latitude, request.hi.latitude) for feature in self.db: if(feature.location.longitude &gt;= left and feature.location.longitude &lt;=right and feature.location.latitude &gt;= bottom and feature.location.latitude &lt;= top): yield feature def RecordRoute(self, request_iterator, context): point_count = 0 feature_count = 0 distance = 0.0 prev_point = None start_time = time.time() for point in request_iterator: point_count += 1 if get_feature(self.db, point): feature_count += 1 if prev_point: distance += get_distance(prev_point, point) prev_point = point elapsed_time = time.time() - start_time return route_guide_pb2.RouteSummary( point_count=point_count, feature_count=feature_count, distance=int(distance), elapsed_time=int(elapsed_time)) def RouteChat(self, request_iterator, context): prev_notes = [] for new_note in request_iterator: for prev_note in prev_notes: if prev_note.location == new_note.location: yield prev_note prev_notes.append(new_note)def serve(): server = grpc.server(futures.ThreadPoolExecutor(max_workers=10)) route_guide_pb2_grpc.add_RouteGuideServicer_to_server(RouteGuideServicer(), server) server.add_insecure_port('[::]:50051') server.start() try: while True: time.sleep(_ONE_DAY_IN_SECONDS) except KeyboardInterrupt: server.stop(0)if __name__ == '__main__': print("route guide server is running...") serve() 完整客户端代码route_guide_client.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101"""The Python implementation of the gRPC route guide client."""from __future__ import print_functionimport randomimport grpcimport route_guide_pb2import route_guide_pb2_grpcimport route_guide_dbdef make_route_note(message, latitude, longitude): return route_guide_pb2.RouteNote( message=message, location=route_guide_pb2.Point(latitude=latitude, longitude=longitude) )def guide_get_one_feature(stub, point): feature = stub.GetFeature(point) if not feature.location: print("server returned incomplete feature") return if feature.name: print("Feature called %s at %s" % (feature.name, feature.location)) else: print("Found no feature at %s" % feature.location)def guide_get_feature(stub): guide_get_one_feature(stub, route_guide_pb2.Point(latitude=409146138, longitude=-746188906)) guide_get_one_feature(stub, route_guide_pb2.Point(latitude=0, longitude=0))def guide_list_features(stub): rectangle = route_guide_pb2.Rectangle( lo=route_guide_pb2.Point(latitude=400000000, longitude=-750000000), hi=route_guide_pb2.Point(latitude=420000000, longitude=-730000000)) print("looking for features between 40, -75 and 42, -73") features = stub.ListFeatures(rectangle) for feature in features: print("Feature called %s at %s" % (feature.name, feature.location))def generate_route(feature_list): for _ in range(0, 10): random_feature = feature_list[random.randint(0, len(feature_list))] print("Visiting point %s" % random_feature.location) yield random_feature.locationdef guide_record_route(stub): feature_list = route_guide_db.read_route_guide_db() route_iterator = generate_route(feature_list) route_summary = stub.RecordRoute(route_iterator) print("Finished trip with %s points " % route_summary.point_count) print("Passed %s features " % route_summary.feature_count) print("Travelled %s meters " % route_summary.distance) print("It took %s seconds " % route_summary.elapsed_time)def generate_messages(): messages = [ make_route_note("First message", 0, 0), make_route_note("Second message", 0, 1), make_route_note("Third message", 1, 0), make_route_note("Fourth message", 0, 0), make_route_note("Fifth message", 1, 0), ] for msg in messages: print("Sending %s at %s" % (msg.message, msg.location)) yield msgdef guide_route_chat(stub): responses = stub.RouteChat(generate_messages()) for response in responses: print("Received message %s at %s" % (response.message, response.location))def run(): with grpc.insecure_channel('localhost:50051') as channel: stub = route_guide_pb2_grpc.RouteGuideStub(channel) print("-------------- GetFeature --------------") guide_get_feature(stub) print("-------------- ListFeatures --------------") guide_list_features(stub) print("-------------- RecordRoute --------------") guide_record_route(stub) print("-------------- RouteChat --------------") guide_route_chat(stub)if __name__ == '__main__': run() 运行结果123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353-------------- GetFeature --------------Feature called Berkshire Valley Management Area Trail, Jefferson, NJ, USA at latitude: 409146138longitude: -746188906Found no feature at-------------- ListFeatures --------------looking for features between 40, -75 and 42, -73Feature called Patriots Path, Mendham, NJ 07945, USA at latitude: 407838351longitude: -746143763Feature called 101 New Jersey 10, Whippany, NJ 07981, USA at latitude: 408122808longitude: -743999179Feature called U.S. 6, Shohola, PA 18458, USA at latitude: 413628156longitude: -749015468Feature called 5 Conners Road, Kingston, NY 12401, USA at latitude: 419999544longitude: -740371136Feature called Mid Hudson Psychiatric Center, New Hampton, NY 10958, USA at latitude: 414008389longitude: -743951297Feature called 287 Flugertown Road, Livingston Manor, NY 12758, USA at latitude: 419611318longitude: -746524769Feature called 4001 Tremley Point Road, Linden, NJ 07036, USA at latitude: 406109563longitude: -742186778Feature called 352 South Mountain Road, Wallkill, NY 12589, USA at latitude: 416802456longitude: -742370183Feature called Bailey Turn Road, Harriman, NY 10926, USA at latitude: 412950425longitude: -741077389Feature called 193-199 Wawayanda Road, Hewitt, NJ 07421, USA at latitude: 412144655longitude: -743949739Feature called 406-496 Ward Avenue, Pine Bush, NY 12566, USA at latitude: 415736605longitude: -742847522Feature called 162 Merrill Road, Highland Mills, NY 10930, USA at latitude: 413843930longitude: -740501726Feature called Clinton Road, West Milford, NJ 07480, USA at latitude: 410873075longitude: -744459023Feature called 16 Old Brook Lane, Warwick, NY 10990, USA at latitude: 412346009longitude: -744026814Feature called 3 Drake Lane, Pennington, NJ 08534, USA at latitude: 402948455longitude: -747903913Feature called 6324 8th Avenue, Brooklyn, NY 11220, USA at latitude: 406337092longitude: -740122226Feature called 1 Merck Access Road, Whitehouse Station, NJ 08889, USA at latitude: 406421967longitude: -747727624Feature called 78-98 Schalck Road, Narrowsburg, NY 12764, USA at latitude: 416318082longitude: -749677716Feature called 282 Lakeview Drive Road, Highland Lake, NY 12743, USA at latitude: 415301720longitude: -748416257Feature called 330 Evelyn Avenue, Hamilton Township, NJ 08619, USA at latitude: 402647019longitude: -747071791Feature called New York State Reference Route 987E, Southfields, NY 10975, USA at latitude: 412567807longitude: -741058078Feature called 103-271 Tempaloni Road, Ellenville, NY 12428, USA at latitude: 416855156longitude: -744420597Feature called 1300 Airport Road, North Brunswick Township, NJ 08902, USA at latitude: 404663628longitude: -744820157Feature called at latitude: 407113723longitude: -749746483Feature called at latitude: 402133926longitude: -743613249Feature called at latitude: 400273442longitude: -741220915Feature called at latitude: 411236786longitude: -744070769Feature called 211-225 Plains Road, Augusta, NJ 07822, USA at latitude: 411633782longitude: -746784970Feature called at latitude: 415830701longitude: -742952812Feature called 165 Pedersen Ridge Road, Milford, PA 18337, USA at latitude: 413447164longitude: -748712898Feature called 100-122 Locktown Road, Frenchtown, NJ 08825, USA at latitude: 405047245longitude: -749800722Feature called at latitude: 418858923longitude: -746156790Feature called 650-652 Willi Hill Road, Swan Lake, NY 12783, USA at latitude: 417951888longitude: -748484944Feature called 26 East 3rd Street, New Providence, NJ 07974, USA at latitude: 407033786longitude: -743977337Feature called at latitude: 417548014longitude: -740075041Feature called at latitude: 410395868longitude: -744972325Feature called at latitude: 404615353longitude: -745129803Feature called 611 Lawrence Avenue, Westfield, NJ 07090, USA at latitude: 406589790longitude: -743560121Feature called 18 Lannis Avenue, New Windsor, NY 12553, USA at latitude: 414653148longitude: -740477477Feature called 82-104 Amherst Avenue, Colonia, NJ 07067, USA at latitude: 405957808longitude: -743255336Feature called 170 Seven Lakes Drive, Sloatsburg, NY 10974, USA at latitude: 411733589longitude: -741648093Feature called 1270 Lakes Road, Monroe, NY 10950, USA at latitude: 412676291longitude: -742606606Feature called 509-535 Alphano Road, Great Meadows, NJ 07838, USA at latitude: 409224445longitude: -748286738Feature called 652 Garden Street, Elizabeth, NJ 07202, USA at latitude: 406523420longitude: -742135517Feature called 349 Sea Spray Court, Neptune City, NJ 07753, USA at latitude: 401827388longitude: -740294537Feature called 13-17 Stanley Street, West Milford, NJ 07480, USA at latitude: 410564152longitude: -743685054Feature called 47 Industrial Avenue, Teterboro, NJ 07608, USA at latitude: 408472324longitude: -740726046Feature called 5 White Oak Lane, Stony Point, NY 10980, USA at latitude: 412452168longitude: -740214052Feature called Berkshire Valley Management Area Trail, Jefferson, NJ, USA at latitude: 409146138longitude: -746188906Feature called 1007 Jersey Avenue, New Brunswick, NJ 08901, USA at latitude: 404701380longitude: -744781745Feature called 6 East Emerald Isle Drive, Lake Hopatcong, NJ 07849, USA at latitude: 409642566longitude: -746017679Feature called 1358-1474 New Jersey 57, Port Murray, NJ 07865, USA at latitude: 408031728longitude: -748645385Feature called 367 Prospect Road, Chester, NY 10918, USA at latitude: 413700272longitude: -742135189Feature called 10 Simon Lake Drive, Atlantic Highlands, NJ 07716, USA at latitude: 404310607longitude: -740282632Feature called 11 Ward Street, Mount Arlington, NJ 07856, USA at latitude: 409319800longitude: -746201391Feature called 300-398 Jefferson Avenue, Elizabeth, NJ 07201, USA at latitude: 406685311longitude: -742108603Feature called 43 Dreher Road, Roscoe, NY 12776, USA at latitude: 419018117longitude: -749142781Feature called Swan Street, Pine Island, NY 10969, USA at latitude: 412856162longitude: -745148837Feature called 66 Pleasantview Avenue, Monticello, NY 12701, USA at latitude: 416560744longitude: -746721964Feature called at latitude: 405314270longitude: -749836354Feature called at latitude: 414219548longitude: -743327440Feature called 565 Winding Hills Road, Montgomery, NY 12549, USA at latitude: 415534177longitude: -742900616Feature called 231 Rocky Run Road, Glen Gardner, NJ 08826, USA at latitude: 406898530longitude: -749127080Feature called 100 Mount Pleasant Avenue, Newark, NJ 07104, USA at latitude: 407586880longitude: -741670168Feature called 517-521 Huntington Drive, Manchester Township, NJ 08759, USA at latitude: 400106455longitude: -742870190Feature called at latitude: 400066188longitude: -746793294Feature called 40 Mountain Road, Napanoch, NY 12458, USA at latitude: 418803880longitude: -744102673Feature called at latitude: 414204288longitude: -747895140Feature called at latitude: 414777405longitude: -740615601Feature called 48 North Road, Forestburgh, NY 12777, USA at latitude: 415464475longitude: -747175374Feature called at latitude: 404062378longitude: -746376177Feature called at latitude: 405688272longitude: -749285130Feature called at latitude: 400342070longitude: -748788996Feature called at latitude: 401809022longitude: -744157964Feature called 9 Thompson Avenue, Leonardo, NJ 07737, USA at latitude: 404226644longitude: -740517141Feature called at latitude: 410322033longitude: -747871659Feature called at latitude: 407100674longitude: -747742727Feature called 213 Bush Road, Stone Ridge, NY 12484, USA at latitude: 418811433longitude: -741718005Feature called at latitude: 415034302longitude: -743850945Feature called at latitude: 411349992longitude: -743694161Feature called 1-17 Bergen Court, New Brunswick, NJ 08901, USA at latitude: 404839914longitude: -744759616Feature called 35 Oakland Valley Road, Cuddebackville, NY 12729, USA at latitude: 414638017longitude: -745957854Feature called at latitude: 412127800longitude: -740173578Feature called at latitude: 401263460longitude: -747964303Feature called at latitude: 412843391longitude: -749086026Feature called at latitude: 418512773longitude: -743067823Feature called 42-102 Main Street, Belford, NJ 07718, USA at latitude: 404318328longitude: -740835638Feature called at latitude: 419020746longitude: -741172328Feature called at latitude: 404080723longitude: -746119569Feature called at latitude: 401012643longitude: -744035134Feature called at latitude: 404306372longitude: -741079661Feature called at latitude: 403966326longitude: -748519297Feature called at latitude: 405002031longitude: -748407866Feature called at latitude: 409532885longitude: -742200683Feature called at latitude: 416851321longitude: -742674555Feature called 3387 Richmond Terrace, Staten Island, NY 10303, USA at latitude: 406411633longitude: -741722051Feature called 261 Van Sickle Road, Goshen, NY 10924, USA at latitude: 413069058longitude: -744597778Feature called at latitude: 418465462longitude: -746859398Feature called at latitude: 411733222longitude: -744228360Feature called 3 Hasta Way, Newton, NJ 07860, USA at latitude: 410248224longitude: -747127767-------------- RecordRoute --------------Visiting point latitude: 405002031longitude: -748407866Visiting point latitude: 400106455longitude: -742870190Visiting point latitude: 409532885longitude: -742200683Visiting point latitude: 413628156longitude: -749015468Visiting point latitude: 413700272longitude: -742135189Visiting point latitude: 406523420longitude: -742135517Visiting point latitude: 400273442longitude: -741220915Visiting point latitude: 400066188longitude: -746793294Visiting point latitude: 415034302longitude: -743850945Visiting point latitude: 412567807longitude: -741058078Finished trip with 10 pointsPassed 10 featuresTravelled 551060 metersIt took 0 seconds-------------- RouteChat --------------Sending First message atSending Second message at longitude: 1Sending Third message at latitude: 1Sending Fourth message atSending Fifth message at latitude: 1Received message First message atReceived message Third message at latitude: 1 小结本文主要介绍了grpc下python的基本运行方式，以及grpc的四种通信方式。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>微服务</tag>
        <tag>RPC</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-Datamask-ProxySQL]]></title>
    <url>%2F2018%2F07%2F31%2FMySQL-Datamask-ProxySQL%2F</url>
    <content type="text"><![CDATA[MySQL datamasking using ProxySQL 前言操作环境一览:- 操作系统： CentOS7- MySQL: 5.5- ProxySQL: 1.4.9- ProxySQL主机IP: 192.168.48.100- MySQL主库IP: 192.168.48.120 场景描述 一张带有信用卡信息(faked)等敏感信息的顾客表， 开发、测试用户并不真正需要信用卡号的等敏感信息。 需求 开发测试用户能够通过ProxySQL访问数据 开发测试用户能够访问所有列，但是带有敏感信息的需要隐藏 开发测试用户不能在特定表上执行SELECT *操作 顾客表示例： 1234567+----+-----------+-------------+------------+------------------+----------+| id | firstname | lastname | cc_type | cc_num | cc_verif |+----+-----------+-------------+------------+------------------+----------+| 1 | Frederic | Descamps | mastercard | 5275653223285289 | 456 || 8 | Dim0 | Vanoverbeke | mastercard | 5345654523285289 | 123 || 15 | Kenny | Gryp | visa | 4916066793184589 | 456 |+----+-----------+-------------+------------+------------------+----------+ 我们可以在后端mysql主库上(192.168.48.120)创建该测试顾客表: 创建账号 1CREATE USER 'proxysql'@'192.168.48.120' IDENTIFIED BY '123456'; 创建表 12345678910create database test;create table customers( id int(3) not null primary key, firstname varchar(20) not null, lastname varchar(20) not null, cc_type varchar(20) not null, cc_num varchar(50) not null, cc_verif int(3)); 授权 123GRANT ALL ON test.customers TO 'proxysql'@'192.168.48.120';FLUSH PRIVILEGES; ProxySQL安装ProxySQL123#proxysql需要依赖一些perl库，所以使用yum安装wget https://github.com/sysown/proxysql/releases/download/v1.4.9/proxysql-1.4.9-3-centos7.x86_64.rpmyum install -y proxysql-1.4.9-3-centos7.x86_64.rpm 启动ProxySQL12345/etc/init.d/proxysql start#proxysql客户端监听在6033端口上，管理端监听6032端口连接proxysql管理端进行配置：mysql -uadmin -padmin -h127.0.0.1 -P6032#默认的管理端账号密码都是admin，登录进去之后可以修改变量进行修改账号密码 添加后端的mysql主机将mysql服务器ip换成你的mysql服务器ip 12345678910ProxySQL&gt; INSERT INTO mysql_servers(hostgroup_id,hostname,port) VALUES (1,'192.168.48.100',3306);select * from mysql_servers;MySQL [(none)]&gt; select * from mysql_servers;+--------------+-----------+------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+| hostgroup_id | hostname | port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |+--------------+-----------+------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+| 1 | 192.168.48.120 | 3306 | ONLINE | 1 | 0 | 1000 | 0 | 0 | 0 | | 添加可以访问后端主机的账号在mysql主库(192.168.48.120)中添加账号proxysql及密码，以及授权 1234567GRANT ALL ON *.* TO 'proxysql'@'192.168.48.120' IDENTIFIED BY '123456';在proxysql服务器(192.168.48.100)中添加可以增删改查后端mysql服务器的账号insert into mysql_users(username,password,default_hostgroup,transaction_persistent)values('proxysql','123456',1,1);MySQL [(none)]&gt; insert into mysql_users(username,password,default_hostgroup,transaction_persistent)values('proxysql','123456',1,1);Query OK, 1 row affected (0.00 sec) 在proxysql主机的mysql_users表中添加刚才创建的账号，proxysql客户端需要使用这个账号来访问数据库。 default_hostgroup默认组设置为写组，也就是1 当读写分离的路由规则不符合时，会访问默认组的数据库 将刚才我们修改的数据加载至RUNTIME中(参考ProxySQL的多层配置结构)： 1234load mysql users to runtime;load mysql servers to runtime;save mysql users to disk;save mysql servers to disk; DataMaskingProxySQL有查询重写(Query Rewrite)功能，如果你想要重写查询，你必匹配查询的原始语句(使用match_pattern)，因为原始查询语句需要被重写。 添加查询规则12345ProxySQL&gt; INSERT INTO mysql_query_rules (rule_id,active,username,match_pattern,error_msg) VALUES (90,1,'proxysql','^SELECT \*.*FROM.*customers', 'Query not allowed due to sensitive information, please contact dba@myapp.com');Let’s load it in runtime and testProxySQL&gt; LOAD MYSQL QUERY RULES TO RUNTIME; 另开一个终端，以6033端口(数据端口)登录: 1mysql -uproxysql -p123456 -h 192.168.48.100 -P 6033 执行SELECT *操作： 12mysql&gt; select * from test.customers;ERROR 1148 (42000): Query not allowed due to sensitive information, please contact dba@myapp.com Yeah!我们根据配置的mysql_query_rules成功阻断了对customers表上的SELECT*操作. 我们再在管理连接中插入如下一条查询规则: 123456789101112131415ProxySQL&gt; INSERT INTO mysql_query_rules (rule_id,active,username,match_pattern,replace_pattern,apply) VALUES (1,1,'proxysql','^[sS][eE][lL][eE][cC][tT] (.*)cc_num([ ,])(.*)', "SELECT \1CONCAT(REPEAT('X',12),RIGHT(cc_num,4)) cc_num\2\3",1);ProxySQL&gt; LOAD MYSQL QUERY RULES TO RUNTIME;我们在数据连接中再测试一下:mysql&gt; select firstname, cc_num from test.customers;+-----------+------------------+| firstname | cc_num |+-----------+------------------+| Frederic | XXXXXXXXXXXX5289 || Dim0 | XXXXXXXXXXXX5289 || Kenny | XXXXXXXXXXXX4589 |+-----------+------------------+ WOOhoo!我们成功实现了只显示卡号后4位！ 保存规则到磁盘 1ProxySQL&gt; SAVE MYSQL QUERY RULES TO DISK; 更多我们需要对更多的表和字段做更多的datamasking(例如姓名字段做隐藏等)，我们就需要编写更多的查询规则(mysql_query_rules),并在管理连接中添加到mysql_query_rules表中.]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>ProxySQL</tag>
        <tag>DataMask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[只在使用 Mix-in 组件制作工具类时进行多重继承]]></title>
    <url>%2F2018%2F06%2F11%2FMix-in-inherit%2F</url>
    <content type="text"><![CDATA[Python是面向对象的编程语言，它提供了一些内置的编程机制，使得开发者可以适当地实现多重继承。但是我们仍然应该尽量避开多重继承。 前言若一定要利用多重继承所带来的便利及封装性，那就编写mix-in类。mix-in是一种小型的类，它只定义了其他类可能需要提供的一套附加方法，而不定义自己的实例属性，此外，它也不要求使用者调用自己的__init__构造器。 例子1:ToDictMixin现在，要把内存中的Python对象转换成字典形式，以便将其序列化，那我们就不妨把这个功能写成通用的代码，以便其他类使用。 ToDictMixin123456789101112131415161718192021222324# ToDictMixin类class ToDictMixin(object): def to_dict(self): # 用__dict__来访问实例内部的字典 return self._traverse_dict(self.__dict__) def _traverse_dict(self, instance_dict): output = &#123;&#125; for key, value in instance_dict.items(): output[key] = self._traverse(key, value) return output def _traverse(self, key, value): # 根据value不同类型分别作处理 if isinstance(value, ToDictMixin): return value.to_dict() elif isinstance(value, dict): return self._traverse_dict(value) elif isinstance(value, list): return [self._traverse(key, i) for i in value] elif hasattr(value, '__dict__'): return self._traverse_dict(value.__dict__) else: return value BinaryTree使用ToDictMixin把二叉树表示为字典: 123456# 二叉树类class BinaryTree(ToDictMixin): def __init__(self, value, left=None, right=None): self.value= value self.left = left self.right = right 现在，我们可以把一大批互相关联的Python对象都轻松地转换成字典： 123456tree = BinaryTree(1, left=BinaryTree(2, right=BinaryTree(3)), right=BinaryTree(4, left=BinaryTree(5)))print(tree.to_dict())&gt;&gt;&gt;&#123;'value': 1, 'right': &#123;'value': 4, 'right': None, 'left': &#123;'value': 5, 'right': None, 'left': None&#125;&#125;, 'left': &#123;'value': 2, 'right': &#123;'value': 3, 'right': None, 'left': None&#125;, 'left': None&#125;&#125; BinaryTreeWithParentmix-in的最大优势在于，使用者可以随时安插这些通用的功能，并能在必要的时候覆写它们。 下面定义的这个BinaryTree子类，会持有指向父节点的引用。如果采用默认的ToDictMixin.to_dict来处理它，那么程序就会因为循环引用而陷入死循环(parent)。 1234class BinaryTreeWithParent(BinaryTree): def __init__(self, value, left=None, right=None, parent=None): super().__init__(value, left=left, right=right) self.parent = parent 解决办法是在BinaryTreeWithParent里覆写ToDictMixin._traverse方法，令该方法只处理与序列化有关的值，从而使mix-in的实现代码不会陷入死循环： 1234567891011# 覆写_traverse方法，不再遍历父节点，而是只把父节点所对应的数值插入到最终生成的字典里面class BinaryTreeWithParent(BinaryTree): def __init__(self, value, left=None, right=None, parent=None): super().__init__(value, left=left, right=right) self.parent = parent def _traverse(self, key, value): if (isinstance(value, BinaryTreeWithParent) and key == 'parent'): return value.value # 返回父节点(parent)的值 else: return super()._traverse(key, value) 调用BinaryTreeWithParent.to_dict看看: 123456root = BinaryTreeWithParent(1)root.left = BinaryTreeWithParent(2, parent=root)root.left.right = BinaryTreeWithParent(4, parent=root.left)print(root.to_dict())&gt;&gt;&gt;&#123;'value': 1, 'right': None, 'left': &#123;'value': 2, 'right': &#123;'value': 4, 'right': None, 'left': None, 'parent': 2&#125;, 'left': None, 'parent': 1&#125;, 'parent': None&#125; 定义了BinaryTreeWithParent._traverse方法之后，如果其他类的某个属性也是BinaryTreeWithParent类型，那么ToDictMixin会自动处理好这些属性: 123456789class NamedSubTree(ToDictMixin): def __init__(self, name, tree_with_parent): self.name = name self.tree_with_parent = tree_with_parentmy_tree = NamedSubTree('foobar', root.left.right) # 上面定义的rootprint(my_tree.to_dict())&gt;&gt;&gt;&#123;'name': 'foobar', 'tree_with_parent': &#123;'value': 4, 'right': None, 'left': None, 'parent': 2&#125;&#125; 多个mix-in组合JsonMixin多个mix-in之间也可以相互组合。例如，可以编写这样一个mix-in，它能够为任意类提供通用的JSON序列化功能。我们可以假定：继承了mix-in的哪个类，会提供名为to_dict的方法(此方法有可能是那个类通过多重继承ToDictMixin而具备的，也有可能不是)。 JsonMixin123456789# import json firstclass JsonMixin(object): @classmethod def from_json(cls, data): kwargs = json.loads(data) return cls(**kwargs) def to_json(self): return json.dumps(self.to_dict()) 请注意，JsonMixin类既定义了实例方法，有定义了类方法。这两种行为都可以通过mix-in来提供。在本例中，凡是想继承JsonMixin的类，只需符合两个条件即可: (1) 包含名为to_dict的方法 (2) init方法接受关键字参数 组合ToDictMixin和JsonMixin我们用下面这个继承了mix-in组件的数据类来表示数据中心的拓扑结构: DatacenterRack123456789101112131415161718class DatacenterRack(ToDictMixin, JsonMixin): def __init__(self, switch=None, machines=None): self.switch = Switch(**switch) self.machines = [Machine(**kwargs) for kwargs in machines]class Switch(ToDictMixin, JsonMixin): def __init__(self, **kwargs): super().__init__() # 接受处理关键字参数 for k, w in kwargs.items(): setattr(self, k, w)class Machine(ToDictMixin, JsonMixin): def __init__(self, **kwargs): super().__init__() # 接受处理关键字参数 for k, w in kwargs.items(): setattr(self, k, w) 对这样的类进行序列化，以及从JSON中加载它，都是比较简单的。下面的这段代码，会重复执行序列化及反序列化操作，以验证这两个功能有没有正确地实现出来。 123456789101112serialized = """&#123; "switch": &#123;"ports": 5,"speed":1e9&#125;, "machines": [ &#123;"cores": 8, "ram": 32e9, "disk": 5e12&#125;, &#123;"cores": 4, "ram": 16e9, "disk": 5e12&#125;, &#123;"cores": 2, "ram": 4e9, "disk": 500e9&#125; ]&#125;"""deserialized = DatacenterRack.from_json(serialized)roundtrip = deserialized.to_json()assert json.loads(serialized) == json.loads(roundtrip) 使用这种mix-in的时候，既可以像本例这样，直接继承多个mix-in组件，也可以先令继承体系中的其他类继承相关的mix-in组件，然后再令本类继承那些类，以达到同样的效果 小结 能用mix-in组件实现的效果，就不要用多重继承来做 将各功能实现为可插拔的mix-in组件，然后令相关的类继承自己需要的那些组件，即可定制该类实例所应具备的行为。 把简单的行为封装到mix-in组件里，然后就可以用多个mix-in组合出复杂的行为了。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Mix-in</tag>
        <tag>inherit</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu下搭建个人gitlab服务器]]></title>
    <url>%2F2018%2F05%2F10%2Fstart-gitlab%2F</url>
    <content type="text"><![CDATA[本文主要记录在Ubuntu 16.04操作系统中搭建GitLab服务器的操作记录 前言 GitLab 是一个用于仓库管理系统的开源项目。使用Git作为代码管理工具，并在此基础上搭建起来的web服务.可通过Web界面进行访问公开的或者私人项目,它拥有与Github类似的功能,能够浏览源代码,管理缺陷和注释.可以管理团队对仓库的访问,它非常易于浏览提交过的版本并提供一个文件历史库,团队成员可以利用内置的简单聊天程序(Wall)进行交流。它还提供一个代码片段收集功能可以轻松实现代码复用. 安装依赖包1sudo apt-get install curl openssh-server ca-certificates postfix 执行完成后,出现邮件配置，选择Internet Site这一项，确定添加清华镜像源 添加Gitlab的GPG公钥 1curl https://packages.gitlab.com/gpg.key 2&gt; /dev/null | sudo apt-key add - &amp;&gt;/dev/null 添加源 sudo vim /etc/apt/sources.list.d/gitlab-ce.list,加入如下语句： 1deb https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/ubuntu xenial main 安装gitlab-ce12sudo apt-get updatesudo apt-get install gitlab-ce 会下载400多MB的，安装完成会占用1GB多的空间，请确保服务器空间充足 启动各项服务1sudo gitlab-ctl reconfigure 检查GitLab是否安装好并且已经正确运行1sudo gitlab-ctl status 得到如下结果，说明Gitlab运行正常 12345678910111213run: gitaly: (pid 25122) 1215s; run: log: (pid 17658) 2960srun: gitlab-monitor: (pid 25134) 1214s; run: log: (pid 17872) 2948srun: gitlab-workhorse: (pid 25139) 1213s; run: log: (pid 17594) 2974srun: logrotate: (pid 25153) 1211s; run: log: (pid 17626) 2966srun: nginx: (pid 26851) 857s; run: log: (pid 17610) 2972srun: node-exporter: (pid 25180) 1210s; run: log: (pid 17842) 2954srun: postgres-exporter: (pid 25190) 1209s; run: log: (pid 17956) 2934srun: postgresql: (pid 25201) 1207s; run: log: (pid 17322) 3028srun: prometheus: (pid 25210) 1204s; run: log: (pid 17914) 2940srun: redis: (pid 25226) 1201s; run: log: (pid 17256) 3034srun: redis-exporter: (pid 25309) 1199s; run: log: (pid 17888) 2946srun: sidekiq: (pid 26311) 1071s; run: log: (pid 17576) 2976srun: unicorn: (pid 26662) 933s; run: log: (pid 17532) 2982s 配置gitlab external_url访问规则 修改gitlab.rb配置文件 1sudo vim /etc/gitlab/gitlab.rb 1234- external_url 'http://gitlab.example.com'+ external_url 'http://192.168.2.200:9876/' + ## 192.168.48.200为服务器地址，请替换为你的+ ## 9876为自定义的端口，默认为80端口。 添加防火墙规则 1234## 开放自定义端口访问(上面定义的9876端口)sudo iptables -A INPUT -p tcp -m tcp --dport 9876 -j ACCEPT## 如果上面使用的默认端口,就开放80端口## sudo iptables -A INPUT -p tcp -m tcp --dport 80 -j ACCEPT 启动sshd和postfix服务123sudo service sshd startsudo service postfix start 浏览页面并设置密码浏览器输入http://192.168.2.200:9876(如果使用的默认端口则为http://192.168.2.200),将192.168.2.200替换为你的服务器地址. 第一次进入后会出现修改密码的页面 输入密码确认。其默认用户为root 然后可以修改用户名密码或者注册新用户等 创建组、项目 当然也可以从github等仓库导入 添加ssh-key等这和github等相同]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>gitlab</tag>
        <tag>git</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Cookiecutter来初始化你的Django项目,Awesome!!!]]></title>
    <url>%2F2018%2F03%2F19%2FCookiecutter-Django%2F</url>
    <content type="text"><![CDATA[快来动手试试吧，简直太棒了 前言Cookiecutter可以让你快速从模板中建立工程，cookiecutter-django则是Django的模板，可以快速生成Django大型项目模板。其特性如下: 跨平台: Windows,Mac 和Linux都支持 在Python2.7, 3.3, 3.4, 3.5, 3.6 和PyPy下运行 工程模板可以是任何语言 简单易用 安装配置Cookiecutter-django安装cookiecutter首先, get Cookiecutter.相信我,它棒极了: 1$ pip install "cookiecutter&gt;=1.4.0" 生成项目然后用Cookiecutter-django来生成一个Django项目: 1cookiecutter https://github.com/audreyr/cookiecutter-pypackage.git 你需要在引导下填一些values,例如: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647Cloning into &apos;cookiecutter-django&apos;...remote: Counting objects: 550, done.remote: Compressing objects: 100% (310/310), done.remote: Total 550 (delta 283), reused 479 (delta 222)Receiving objects: 100% (550/550), 127.66 KiB | 58 KiB/s, done.Resolving deltas: 100% (283/283), done.project_name [Project Name]: My First Django Projectproject_slug [reddit_clone]: my_first_django_projectauthor_name [Daniel Roy Greenfeld]: cgDeepLearnemail [you@example.com]: cgDeepLearn@gmail.comdescription [A short description of the project.]: My first Django Project with Cookiecutterdomain_name [example.com]: myxxxx.comversion [0.1.0]: 0.0.1timezone [UTC]: Asia/Shanghaiuse_whitenoise [y]: nuse_celery [n]: yuse_mailhog [n]: nuse_sentry_for_error_reporting [y]: yuse_opbeat [n]: yuse_pycharm [n]: ywindows [n]: nuse_docker [y]: nuse_heroku [n]: nuse_compressor [n]: ySelect postgresql_version:1 - 10.32 - 10.23 - 10.14 - 9.65 - 9.56 - 9.47 - 9.3Choose from 1, 2, 3, 4 [1]: 4Select js_task_runner:1 - Gulp2 - Grunt3 - NoneChoose from 1, 2, 3, 4 [1]: 1custom_bootstrap_compilation [n]: nSelect open_source_license:1 - MIT2 - BSD3 - GPLv34 - Apache Software License 2.05 - Not open sourceChoose from 1, 2, 3, 4, 5 [1]: 1keep_local_envs_in_vcs [y]: y 根据你的需要来选择一些选项。注: project_slug是你的项目名(在路径中体现) 配置使用Django进入项目根目录: 12$ cd my_first_django_project$ ls 关联仓库在github创建一个repo,关联你的项目，并首次push: 12345$ git init$ git add .$ git commit -m "first awesome commit"$ git remote add origin git@github.com:yourname/yourproject.git$ git push -u origin master 配置Django选择Django安装版本(修改requirements/base.txt): 12django==1.11.2# django==2.0.3 ## django 2.0+ 数据库如果选择了Postgresql(Postgresql的安装使用情参考Postgresql安装配置),需安装psycopg2依赖: 12## requirements/local.txtpsycopg==2.7.4 在激活的虚拟环境下安装依赖: 1$ pip install -r requirements\local.txt Pycharm的配置如果生成项目时选项pycharm填入了y,下面我们来配置一下。 打开 File - Settings -&gt; Languages and Frameworks -&gt; Django. 勾选上 Enable Django Support 我们需要为Django数据库配置Postgresql数据库地址，我们点击Environment variavles 的 ... ,添加DATABASE_URL变量(注DATABASE_URL在conf.setting中使用): Run the Server 123$ python manage.py migrate$ python manage.py createsuperuser$ python manage.py runserver Cookiecutter-Django英文文档阅读英文指南: 1https://cookiecutter-django.readthedocs.io/en/latest/ 结束]]></content>
      <categories>
        <category>Python</category>
        <category>Django</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>django</tag>
        <tag>cookiecutter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python协程的演化-从yield/send到async/await]]></title>
    <url>%2F2018%2F03%2F12%2Fasyncio%2F</url>
    <content type="text"><![CDATA[python协程的演化 前言 Python由于众所周知的GIL的原因,同一时刻只能有一个线程在运行，那么对于CPU密集的程序来说，线程之间的切换开销就成了拖累，而以I/O为瓶颈的程序正是协程所擅长的： 多任务并发（非并行），每个任务在合适的时候挂起（发起I/O）和恢复(I/O结束)* Python中的协程经历了很长的一段发展历程。其大概经历了如下三个阶段： 最初的生成器进化的yield/send python3.4引入@asyncio.coroutine和yield from 在Python3.5版本中引入async/await关键字 yield/send我们用斐波那契数列做个例子 传统的方式1234567891011121314def normal_fib(n): """返回斐波那契数列前n项""" res = [0] * n index = 0 a, b = 0, 1 while index &lt; n: res[index] = b a, b = b, a + b index += 1 return resprint('-'*10 + 'test old fib' + '-'*10)for fib_res in normal_fib(20): print(fib_res) 如果我们仅仅是需要拿到斐波那契序列的第n位，或者仅仅是希望依此产生斐波那契序列，那么上面这种传统方式就会比较耗费内存。这时生成器的特性就派上用场了—&gt; yield!!! yield我们用yield实现菲波那切数列。 123456789101112def gen_fib(n): """斐波那契数列生成器""" index = 0 a, b = 0, 1 while index &lt; n: yield b a, b = b, a + b index += 1print('-'*10 + 'test yield fib' + '-'*10)for fib_res in fib(20): print(fib_res) 当一个函数中包含yield语句时，python会自动将其识别为一个生成器。这时fib(20)并不会真正调用函数体，而是以函数体生成了一个生成器对象实例。 yield在这里可以保留gen_fib函数的计算现场，暂停gen_fib的计算并将b返回。而将fib放入for…in循环中时，每次循环都会调用next(fib(20))，唤醒生成器，执行到下一个yield语句处，直到抛出StopIteration异常。此异常会被for循环捕获，导致跳出循环。 sendsend 事件驱动，生成器进化成协程 123456789101112131415161718192021222324import timeimport randomdef coro_fib(n): """斐波那契协程,send一个间隔时间，产出一个值""" index = 0 a, b = 0, 1 while index &lt; n: sleep_sec = yield b # 产出b，将send值绑定到sleep_sec, print('wait &#123;&#125; secs.'.format(sleep_sec)) time.sleep(sleep_sec) a, b = b, a + b index += 1print('-'*10 + 'test yield send' + '-'*10)N = 20cfib = coro_fib(N)fib_res = next(cfib) # 预激协程,运行至yield处暂停while True: print(fib_res) try: fib_res = cfib.send(random.uniform(0, 0.5)) # send驱动协程, 修改合适的时间清楚执行过程 except StopIteration: break 协程更多详细信息请移步python coroutine这里~ yield fromyield from用于重构生成器，简单的，可以这么使用： 1234567def copy_fib(n): print('I am copy from gen_fib') yield from gen_fib(n) # 委派给gen_fib生成器 print('Copy end')print('-'*10 + 'test yield from' + '-'*10)for fib_res in copy_fib(20): print(fib_res) 这种使用方式很简单，但远远不是yield from的全部。yield from的作用还体现可以像一个管道一样将send信息传递给内层协程，并且处理好了各种异常情况，因此，对于coro_fib也可以这样包装和使用： 1234567891011121314def copy_coro_fib(n): print('I am copy from coro_fib') yield from coro_fib(n) # 委托给coro_fib,异常也交由它处理 print('Copy end')print('-'*10 + 'test yield from and send' + '-'*10)N = 20ccfib = copy_coro_fib(N)fib_res = next(ccfib)while True: print(fib_res) try: fib_res = ccfib.send(random.uniform(0, 0.5)) except StopIteration: break asyncio/yield fromasyncio是一个基于事件循环的实现异步I/O的模块。通过yield from，我们可以将协程的控制权交给事件循环，然后挂起当前协程；之后，由事件循环决定何时唤醒协程,接着向后执行代码。 使用asyncio.coroutine装饰器 1234567891011121314151617181920212223242526272829303132333435363738# 并发处理两个快慢不一的斐波那契生成函数@asyncio.coroutinedef fast_fib(n): """smart one""" index = 0 a, b = 0, 1 while index &lt; n: sleep_secs = random.uniform(0, 0.2) yield from asyncio.sleep(sleep_secs) print('Fast one think &#123;&#125; secs to get &#123;&#125;'.format(sleep_secs, b)) a, b = b, a + b index += 1def slow_fib(n): """slow one""" index = 0 a, b = 0, 1 while index &lt; n: sleep_secs = random.uniform(0, random_sec) yield from asyncio.sleep(sleep_secs) print('Slow one think &#123;&#125; secs to get &#123;&#125;'.format(sleep_secs, b)) a, b = b, a + b index += 1if __name__ == '__main__': loop = asyncio.get_event_loop() # 获取时间循环的引用 tasks = [ asyncio.ensure_future(fast_fib(10)), asyncio.ensure_future(slow_fib(10)) # ensure_future 和create_task都可以，asyncio.async过时了 # loop.create_task(fast_fib(10)), # loop.create_task(slow_fib(10)) ] loop.run_until_complete(asyncio.wait(tasks)) print('All fib finished.') loop.close() 运行结果如下: 1234567891011...Fast one think 0.0393240884371622 secs to get 21Slow one think 0.12157996704037113 secs to get 5Fast one think 0.08259000223641344 secs to get 34Slow one think 0.15816909012449587 secs to get 8Fast one think 0.1967429201039252 secs to get 55Slow one think 0.25365548691367573 secs to get 13Slow one think 0.3235222687782598 secs to get 21Slow one think 0.35160632142878434 secs to get 34Slow one think 0.34477299780059134 secs to get 55All fib finished. async/await清楚了asyncio.coroutine和yield from之后，在Python3.5中引入的async和await就不难理解了：可以将他们理解成asyncio.coroutine/yield from的完美替身。当然，从Python设计的角度来说，async/await让协程表面上独立于生成器而存在，将细节都隐藏于asyncio模块之下，语法更清晰明了。 async/await 示例: 12345678910111213141516171819202122232425262728293031323334353637# 使用 async/await 关键字async def fast_fib(n): """smart one""" index = 0 a, b = 0, 1 while index &lt; n: sleep_secs = random.uniform(0, 0.2) await asyncio.sleep(sleep_secs) print('Fast one think &#123;&#125; secs to get &#123;&#125;'.format(sleep_secs, b)) a, b = b, a + b index += 1async def slow_fib(n): """slow one""" index = 0 a, b = 0, 1 while index &lt; n: sleep_secs = random.uniform(0, random_sec) await asyncio.sleep(sleep_secs) print('Slow one think &#123;&#125; secs to get &#123;&#125;'.format(sleep_secs, b)) a, b = b, a + b index += 1if __name__ == '__main__': loop = asyncio.get_event_loop() # 获取时间循环的引用 tasks = [ asyncio.ensure_future(fast_fib(10)), asyncio.ensure_future(slow_fib(10)) # ensure_future 和create_task都可以，asyncio.async过时了 # loop.create_task(fast_fib(10)), # loop.create_task(slow_fib(10)) ] loop.run_until_complete(asyncio.wait(tasks)) print('All fib finished.') loop.close() 可以发现相比上面yield from的版本只改变了以下两点: 函数定义前面加了async关键字，更加清晰表明这是一个协程 yield from 换成了await关键字 总结示例程序中都是以sleep为异步I/O的代表，在实际项目中，可以使用协程异步的读写网络、读写文件、渲染界面等，而在等待协程完成的同时，CPU还可以进行其他的计算。协程的作用正在于此。]]></content>
      <categories>
        <category>Python</category>
        <category>进程线程协程</category>
      </categories>
      <tags>
        <tag>yield/send</tag>
        <tag>yield from</tag>
        <tag>asyncio.coroutine</tag>
        <tag>async/await</tag>
        <tag>asyncio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 并发 concurrent.futures]]></title>
    <url>%2F2018%2F02%2F06%2Fconcurrent-futures%2F</url>
    <content type="text"><![CDATA[考虑用concurrent.futures来实现平行计算或并发处理 导读编写Python程序时,我们可以利用CPU的多核心通过平行计算来提升计算任务的速度。很遗憾，Python的全局解释器(GIL)的存在使得我们没有办法用线程实现真正的平行计算。 为了实现平行计算，我们可以考虑用C语言扩展或者使用诸如Cython和Numba等开源工具迁移到C语言。但是这样做大幅增加了测试量和风险。于是我们思考一下：有没有一种更好的方式，只需使用少量的Python代码，即可有效提升执行效率，并迅速解决复杂的计算问题。 我们可以试着通过内置的concurrent.futures模块来利用内置的multiprocessing模块实现这种需求。这样的做法会以子进程的形式，平行运行多个解释器，从而利用多核心CPU来提升执行速度(子进程与主解释器相分离，所以它们的全局解释器锁也是相互独立的)。我们可以通过下面的例子来看一下效果。 计算两数最大公约数现在给出一个列表，列表里每个元素是一对数，求出每对数的最大公约数 12numbers = [(1963309, 2265973), (2030677, 3814172), (1551645, 2229620), (2039045, 2020802)] 没有做平行计算的版本求最大公约数1234567def gcd(pair): a, b = pair low = min(a, b) for i in range(low, 0, -1): if a % i == 0 and b % i == 0: return i 我们用map来试运行一下: 1234567import timestart = time.time()results = list(map(gcd, numbers))end = time.time()print('Took %.3f seconds' % (end - start))&gt;&gt;&gt;Took 0.530 seconds 下面我们用conccurrent.futures来模拟多线程和多进程 使用concurretn.futures的ThreadPoolExecutor使用ThreadPoolExecutor多线程12345678from concurrent.futures import ThreadPoolExecutorstart = time.time()pool = ThreadPoolExecutor(max_workers=2) # cpu核心数目个工作线程 results = list(pool.map(gcd, numbers))end = time.time()print('Took %.3f seconds' % (end - start))&gt;&gt;&gt;Took 0.535 seconds 两个线程用了和上面差不多的时间，而且比上面还慢一些，说明多线程并不能平行计算，而且开线程也有耗费。 使用concurrent.futures的ProcessPoollExecutor将ThreadPoolExecutor换成ProcessPoolExecutor12345678from concurrent.futures import ProcessPoolExecutorstart = time.time()pool = ProcessPoolExecutor(max_workers=2) # cpu核心数目个工作进程 results = list(pool.map(gcd, numbers))end = time.time()print('Took %.3f seconds' % (end - start))&gt;&gt;&gt; Took 0.287 seconds 在双核电脑上运行上面程序发现比之前两个版本运行快很多。这是因为ProcessPoolExecutor会利用multiprocessing模块所提供的的底层机制来逐步完成下列操作： 把numbers列表中的每一项输入数据都传给map 用pickle模块对数据进行序列化，将其变成二进制形式。 通过本地套接字socket将序列化后的数据从主解释器所在的进程发送到子解释器所在的进程。 接下来在子进程中，用pickle对二进制数据进行反序列化操作,将其还原为Python对象 引入包含gcd函数的那个Python模块 各条子进程平行地针对各自的输入数据，来运行gcd函数 对运行结果进行序列化操作，将其变为字节 将这些字节通过socket复制到主进程中 主进程对这些字节执行反序列化操作，将其还原为Python对象。 最后，把每条子进程所求出的计算结果合并到一份列表中，返回给调用者 编后语为了实现平行计算，multiprocessing模块和ProcessPoolExecutor类在幕后做了大量的工作。如果改用其他的语言来写，那么开发者只需一把同步锁或一项原子操作，就可以把线程之间的通信过程协调好。而在Python中，我们却必须使用开销较高的multiprocessing模块,其开销之所以大，原因就在于主进程与子进程之间，必须进行序列化和反序列化操作，这些是导致大量开销的来源。 对于某些较为孤立，且数据利用率高的任务来说，上述方案非常适合。如果执行的运算不符合上述特征，那么multiprocessing所产生的的开销可能并不能使程序加速。在这种情况下，可以求助multiprocessing所提供的的一些高级机制，如内存共享(shared memory)、跨进程锁定(cross-process lock)、队列(queue)和代理(proxy)等。 下载进度条显示用concurrent.futures的ThreadPoolExecutor类处理对于大量I/O操作的并发任务的示例。非常值得参考的实现。 flags_common.py是一些默认参数和函数接口以及argparse。flags_sequential.py是单线程依序下载以及进度条显示实现。flags_threadpool.py是利用concurrent.futures的多线程操作实现。 flags_common.py flags_common.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149"""Utilities for second set of flag examples."""import osimport timeimport sysimport stringimport argparsefrom collections import namedtuplefrom enum import EnumResult = namedtuple('Result', 'status data')HTTPStatus = Enum('Status', 'ok not_found error')POP20_CC = ('CN IN US ID BR PK NG BD RU JP ' 'MX PH VN ET EG DE IR TR CD FR').split()DEFAULT_CONCUR_REQ = 1MAX_CONCUR_REQ = 1SERVERS = &#123; 'REMOTE': 'http://flupy.org/data/flags', 'LOCAL': 'http://localhost:8001/flags', 'DELAY': 'http://localhost:8002/flags', 'ERROR': 'http://localhost:8003/flags',&#125;DEFAULT_SERVER = 'LOCAL'DEST_DIR = 'downloads/'COUNTRY_CODES_FILE = 'country_codes.txt'def save_flag(img, filename): path = os.path.join(DEST_DIR, filename) with open(path, 'wb') as fp: fp.write(img)def initial_report(cc_list, actual_req, server_label): if len(cc_list) &lt;= 10: cc_msg = ', '.join(cc_list) else: cc_msg = 'from &#123;&#125; to &#123;&#125;'.format(cc_list[0], cc_list[-1]) print('&#123;&#125; site: &#123;&#125;'.format(server_label, SERVERS[server_label])) msg = 'Searching for &#123;&#125; flag&#123;&#125;: &#123;&#125;' plural = 's' if len(cc_list) != 1 else '' print(msg.format(len(cc_list), plural, cc_msg)) plural = 's' if actual_req != 1 else '' msg = '&#123;&#125; concurrent connection&#123;&#125; will be used.' print(msg.format(actual_req, plural))def final_report(cc_list, counter, start_time): elapsed = time.time() - start_time print('-' * 20) msg = '&#123;&#125; flag&#123;&#125; downloaded.' plural = 's' if counter[HTTPStatus.ok] != 1 else '' print(msg.format(counter[HTTPStatus.ok], plural)) if counter[HTTPStatus.not_found]: print(counter[HTTPStatus.not_found], 'not found.') if counter[HTTPStatus.error]: plural = 's' if counter[HTTPStatus.error] != 1 else '' print('&#123;&#125; error&#123;&#125;.'.format(counter[HTTPStatus.error], plural)) print('Elapsed time: &#123;:.2f&#125;s'.format(elapsed))def expand_cc_args(every_cc, all_cc, cc_args, limit): codes = set() A_Z = string.ascii_uppercase if every_cc: codes.update(a+b for a in A_Z for b in A_Z) elif all_cc: with open(COUNTRY_CODES_FILE) as fp: text = fp.read() codes.update(text.split()) else: for cc in (c.upper() for c in cc_args): if len(cc) == 1 and cc in A_Z: codes.update(cc+c for c in A_Z) elif len(cc) == 2 and all(c in A_Z for c in cc): codes.add(cc) else: msg = 'each CC argument must be A to Z or AA to ZZ.' raise ValueError('*** Usage error: '+msg) return sorted(codes)[:limit]def process_args(default_concur_req): server_options = ', '.join(sorted(SERVERS)) parser = argparse.ArgumentParser( description='Download flags for country codes. ' 'Default: top 20 countries by population.') parser.add_argument('cc', metavar='CC', nargs='*', help='country code or 1st letter (eg. B for BA...BZ)') parser.add_argument('-a', '--all', action='store_true', help='get all available flags (AD to ZW)') parser.add_argument('-e', '--every', action='store_true', help='get flags for every possible code (AA...ZZ)') parser.add_argument('-l', '--limit', metavar='N', type=int, help='limit to N first codes', default=sys.maxsize) parser.add_argument('-m', '--max_req', metavar='CONCURRENT', type=int, default=default_concur_req, help='maximum concurrent requests (default=&#123;&#125;)' .format(default_concur_req)) parser.add_argument('-s', '--server', metavar='LABEL', default=DEFAULT_SERVER, help='Server to hit; one of &#123;&#125; (default=&#123;&#125;)' .format(server_options, DEFAULT_SERVER)) parser.add_argument('-v', '--verbose', action='store_true', help='output detailed progress info') args = parser.parse_args() if args.max_req &lt; 1: print('*** Usage error: --max_req CONCURRENT must be &gt;= 1') parser.print_usage() sys.exit(1) if args.limit &lt; 1: print('*** Usage error: --limit N must be &gt;= 1') parser.print_usage() sys.exit(1) args.server = args.server.upper() if args.server not in SERVERS: print('*** Usage error: --server LABEL must be one of', server_options) parser.print_usage() sys.exit(1) try: cc_list = expand_cc_args(args.every, args.all, args.cc, args.limit) except ValueError as exc: print(exc.args[0]) parser.print_usage() sys.exit(1) if not cc_list: cc_list = sorted(POP20_CC) return args, cc_listdef main(download_many, default_concur_req, max_concur_req): args, cc_list = process_args(default_concur_req) actual_req = min(args.max_req, max_concur_req, len(cc_list)) initial_report(cc_list, actual_req, args.server) base_url = SERVERS[args.server] t0 = time.time() counter = download_many(cc_list, base_url, args.verbose, actual_req) assert sum(counter.values()) == len(cc_list), \ 'some downloads are unaccounted for' final_report(cc_list, counter, t0) falgs_sequential.py flags_sequential.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687"""Download flags of countries (with error handling).Sequential versionSample run:: $ python3 flags_sequential.py -s DELAY b DELAY site: http://localhost:8002/flags Searching for 26 flags: from BA to BZ 1 concurrent connection will be used. -------------------- 17 flags downloaded. 9 not found. Elapsed time: 13.36s"""import collectionsimport requestsimport tqdmfrom flags_common import main, save_flag, HTTPStatus, ResultDEFAULT_CONCUR_REQ = 1MAX_CONCUR_REQ = 1# BEGIN FLAGS2_BASIC_HTTP_FUNCTIONSdef get_flag(base_url, cc): url = '&#123;&#125;/&#123;cc&#125;/&#123;cc&#125;.gif'.format(base_url, cc=cc.lower()) resp = requests.get(url) if resp.status_code != 200: # &lt;1&gt; resp.raise_for_status() return resp.contentdef download_one(cc, base_url, verbose=False): try: image = get_flag(base_url, cc) except requests.exceptions.HTTPError as exc: # &lt;2&gt; res = exc.response if res.status_code == 404: status = HTTPStatus.not_found # &lt;3&gt; msg = 'not found' else: # &lt;4&gt; raise else: save_flag(image, cc.lower() + '.gif') status = HTTPStatus.ok msg = 'OK' if verbose: # &lt;5&gt; print(cc, msg) return Result(status, cc) # &lt;6&gt;# END FLAGS2_BASIC_HTTP_FUNCTIONS# BEGIN FLAGS2_DOWNLOAD_MANY_SEQUENTIALdef download_many(cc_list, base_url, verbose, max_req): counter = collections.Counter() # &lt;1&gt; cc_iter = sorted(cc_list) # &lt;2&gt; if not verbose: cc_iter = tqdm.tqdm(cc_iter) # &lt;3&gt; for cc in cc_iter: # &lt;4&gt; try: res = download_one(cc, base_url, verbose) # &lt;5&gt; except requests.exceptions.HTTPError as exc: # &lt;6&gt; error_msg = 'HTTP error &#123;res.status_code&#125; - &#123;res.reason&#125;' error_msg = error_msg.format(res=exc.response) except requests.exceptions.ConnectionError as exc: # &lt;7&gt; error_msg = 'Connection error' else: # &lt;8&gt; error_msg = '' status = res.status if error_msg: status = HTTPStatus.error # &lt;9&gt; counter[status] += 1 # &lt;10&gt; if verbose and error_msg: # &lt;11&gt; print('*** Error for &#123;&#125;: &#123;&#125;'.format(cc, error_msg)) return counter # &lt;12&gt;# END FLAGS2_DOWNLOAD_MANY_SEQUENTIALif __name__ == '__main__': main(download_many, DEFAULT_CONCUR_REQ, MAX_CONCUR_REQ) flags_threadpool.py flags_threadpool.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768"""Download flags of countries (with error handling).ThreadPool versionSample run:: $ python3 flags_threadpool.py -s REMOTE -e ERROR site: http://localhost:8003/flags Searching for 676 flags: from AA to ZZ 30 concurrent connections will be used. -------------------- 150 flags downloaded. 361 not found. 165 errors. Elapsed time: 7.46s"""# BEGIN FLAGS2_THREADPOOLimport collectionsfrom concurrent import futuresimport requestsimport tqdm # &lt;1&gt;from flags_common import main, HTTPStatus # &lt;2&gt;from flags_sequential import download_one # &lt;3&gt;DEFAULT_CONCUR_REQ = 30 # &lt;4&gt;MAX_CONCUR_REQ = 1000 # &lt;5&gt;def download_many(cc_list, base_url, verbose, concur_req): counter = collections.Counter() with futures.ThreadPoolExecutor(max_workers=concur_req) as executor: # &lt;6&gt; to_do_map = &#123;&#125; # &lt;7&gt; for cc in sorted(cc_list): # &lt;8&gt; future = executor.submit(download_one, cc, base_url, verbose) # &lt;9&gt; to_do_map[future] = cc # &lt;10&gt; done_iter = futures.as_completed(to_do_map) # &lt;11&gt; if not verbose: done_iter = tqdm.tqdm(done_iter, total=len(cc_list)) # &lt;12&gt; for future in done_iter: # &lt;13&gt; try: res = future.result() # &lt;14&gt; except requests.exceptions.HTTPError as exc: # &lt;15&gt; error_msg = 'HTTP &#123;res.status_code&#125; - &#123;res.reason&#125;' error_msg = error_msg.format(res=exc.response) except requests.exceptions.ConnectionError as exc: error_msg = 'Connection error' else: error_msg = '' status = res.status if error_msg: status = HTTPStatus.error counter[status] += 1 if verbose and error_msg: cc = to_do_map[future] # &lt;16&gt; print('*** Error for &#123;&#125;: &#123;&#125;'.format(cc, error_msg)) return counterif __name__ == '__main__': main(download_many, DEFAULT_CONCUR_REQ, MAX_CONCUR_REQ)# END FLAGS2_THREADPOOL 结束]]></content>
      <categories>
        <category>Python</category>
        <category>进程线程协程</category>
      </categories>
      <tags>
        <tag>concurrent.futures</tag>
        <tag>ProcessPoolExecutor</tag>
        <tag>ThreadPoolExecutor</tag>
        <tag>multiprocessing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python协程-coroutine]]></title>
    <url>%2F2018%2F01%2F30%2Fcoroutine%2F</url>
    <content type="text"><![CDATA[考虑用协程来并发的运行多个函数 前言我们可以用线程来运行多个函数，使这些函数看上去好像是在同一时间得到执行的。然而，线程有三个显著的缺点： 为了确保数据安全，我们必须使用特殊的工具(Lock, Queue等)来协调这些线程，这使得多线程的代码，要比单线程的过程式代码更加难懂。这些复杂的多线程代码，会逐渐令程序变得难以扩展和维护。 线程需要占用大量内存，每个正在执行的线程，大约占据8MB内存。如果只开十几个线程，多数计算机还是可以承受的。 线程启动的开销比较大。如果程序不停地依靠创建新线程来同时执行多个函数，并等待这些线程结束，那么使用线程所引发的开销，就会拖慢整个程序的速度。 Python的协程(coroutine)可以避免上述问题，它使得Python程序看上去好像是在同时运行多个函数。协程的实现方式，实际上是对生成器的一种扩展。启动生成器协程所需的开销，与调用函数的开销相仿。处于活跃状态的协程，在其耗尽之前，只会占用不到1KB的内存。 协程的工作原理每当生成器函数执行到yield表达式的时候，消耗生成器的那段代码，就通过send方法给生成器回传一个值。而生成器在手熬了经由send函数所传进来的这个值后，这个值会绑定给yield关键字左边的变量；如果yield关键字右边有表达式，那么yield表达式右侧的内容会当成send方法的返回值(没有的话其实返回的是None)，返回给外界(调用方).关键的一点是，协程在 yield 关键字所在的位置暂停执行。在赋值语句中， = 右边的代码在赋值之前执行。下面我们结合两个例子来看看。 简单的协程示例简单协程示例12345678910111213def my_coroutine(): while True: received = yield print('Received:', received)it = my_coroutine()next(it) # 1it.send('First') # 2it.send('Second')&gt;&gt;&gt;Received: FirstReceived: Second 注1: 在生成器上面调用send方法，我们要先调用next函数(这叫预激协程)，以便将生成器推进到第一条yield表达式那里 协程产出值该示例在协程每收到一个数值，就会产出当前所统计到的最大值 协程产出值123456789101112131415161718def maximize(): current = yield # 1 while True: value = yield current # 2 current = max(value, current) # 3it = maximize()next(it) # 预激协程，执行到第一个yield处print(it.send(10)) # 执行到#2处产出current值，等待接收值print(it.send(12)) # 绑定12给value，计算current，执行到#2处产出current值，等待接收值print(it.send(4)) # 同上，即执行到yield表达式右边，等待左边输入绑定print(it.send(22))&gt;&gt;&gt;10121222 上面的代码范例中，第一条yield语句中的yield关键字后面没有跟随内容，其意思是，把外面传进来的首个值，当成目前的最大值。此后生成器会屡次执行while循环中的那条yield语句，以便将当前统计到的最大值告诉外界，同时等候外界传入下一个待考察的值。 协程在yield关键字所在的位置暂停执行。在赋值语句中， = 右边的代码在赋值之前执行。即各个阶段都在yield表达式中结束，先产出值然后在yield出暂停，等待外界传入值。下一个阶段都从那一行代码开始 yield from协程可以通过yield的输出值来推进其他的生成器函数，使得那些生成器函数也执行到它们各自的下一条yield比到时处。接连推进多个独立的生成器，即可模拟出Python线程的并发行为，令程序看上去好像是在同时运行多个函数 使用yield from计算平均值并输出统计报告从一个字典中读取虚构的七年级男女学生的体重和身高。例如，’boys;m’ 键对应于 9 个男学生的身高（单位是米）， ‘girls;kg’ 键对应于 10 个女学生的体重（单位是千克）。这个脚本把各组数据传给前面定义的 averager 协程，然后生成一个报告。 使用yield from计算平均值并输出统计报告1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# -*- coding: utf-8 -*-"""使用yield from计算平均值并输出统计报告"""from collections import namedtupleResult = namedtuple('Result', 'count average')# 子生成器def averager(): # 1 total = 0.0 count = 0 average = None while True: term = yield # 2 if term is None: # 3 break total += term count += 1 average = total / count return Result(count, average) # 4# 委派生成器def grouper(results, key): # 5 while True: # 6 results[key] = yield from averager() # 7# 客户端代码，即调用方def main(data): # 8 results = &#123;&#125; for key, values in data.items(): group = grouper(results, key) # 9 next(group) # 10 for value in values: group.send(value) # 11 group.send(None) # 重要！ 12 print(results) # 如果要调试，去掉注释 report(results)# 输出报告def report(results): for key, result in sorted(results.items()): group, unit = key.split(';') print('&#123;:2&#125; &#123;:5&#125; averaging &#123;:.2f&#125;&#123;&#125;'.format( result.count, group, result.average, unit))DATA = &#123; 'girls;kg': [40.9, 38.5, 44.3, 42.2, 45.2, 41.7, 44.5, 38.0, 40.6, 44.5], 'girls;m': [1.6, 1.51, 1.4, 1.3, 1.41, 1.39, 1.33, 1.46, 1.45, 1.43], 'boys;kg': [39.0, 40.8, 43.2, 40.8, 43.1, 38.6, 41.4, 40.6, 36.3], 'boys;m': [1.38, 1.5, 1.32, 1.25, 1.37, 1.48, 1.25, 1.49, 1.46],&#125;if __name__ == '__main__': main(DATA) 1- 与示例 16-13 中的 averager 协程一样。这里作为子生成器使用。2- main 函数中的客户代码发送的各个值绑定到这里的 term 变量上。3- 至关重要的终止条件。如果不这么做，使用 yield from 调用这个协程的生成器会永远阻塞。4- 返回的 Result 会成为 grouper 函数中 yield from 表达式的值。5- grouper 是委派生成器。6- 这个循环每次迭代时会新建一个 averager 实例；每个实例都是作为协程使用的生成器对象。7- grouper 发送的每个值都会经由 yield from 处理，通过管道传给 averager 实例。 grouper 会在 yield from 表达式处暂停，等待 averager 实例处理客户端发来的值。 averager 实例运行完毕后，返回的值绑定到 results[key] 上。 while 循环会不断创建 averager 实例，处理更多的值。8- main 函数是客户端代码，用 PEP 380 定义的术语来说，是“调用方”。这是驱动一切的函数9- group 是调用 grouper 函数得到的生成器对象，传给 grouper 函数的第一个参数是results，用于收集结果；第二个参数是某个键。 group 作为协程使用。10- 预激 group 协程。11- 把各个 value 传给 grouper。传入的值最终到达 averager 函数中 term = yield 那一行； grouper 永远不知道传入的值是什么。12- 把 None 传入 grouper，导致当前的 averager 实例终止，也让 grouper 继续运行，再创建一个 averager 实例，处理下一组值。 生命游戏：演示协程的协同运作效果。游戏规则 在一个任意尺寸的二维网格中，每个细胞(即每个单元格)都处于生存(alive,用*表示)或空白(empty,用-表示)状态。 时钟每走一步，生命游戏就向前进一步。向前推进时，我们要点算每个细胞周边的那八个单元格，看看该细胞附近有多少个存活的细胞。然后根据存活的数量来判断自己下一轮是继续存活、死亡还是再生。 具体判断规则 若本细胞存活，且周围存活者不足两个，则本细胞下一轮死亡。 若本细胞存活，且周围的存活者多于3个，则本细胞下一轮死亡。 若本细胞死亡，且周围的存活者恰有3个，则本细胞下一轮再生。 建模基于规则我们可以将整个程序分成三个阶段:count_neighbors, step_cell, display count_neighbors: 计算每个细胞附近8个细胞存活的数目 step_cell: 根据细胞本轮状态和计算得到周围的细胞数量生成下一轮的状态 根据每轮的结果显示细胞状态 count_neighbors我们定义一个协程来获取周围细胞的生存状态。协程会产生一个自定义的Query对象，每个yield表达式的结果，要么是ALIVE，要么是EMPTY。其后count_neighbors生成器会根据相邻细胞的状态，来返回本细胞周围的存活细胞数(生成器return语句在python3中才可用，实际是把结果作为StopIteration异常的value属性传给了调用者) count_neighbors协程计算细胞周围的存活数目12345678910111213141516171819202122from collections import namedtupleALIVE = '*'EMPTY = '-'Query = namedtuple('Query', ('y', 'x'))def count_neighbors(y, x): n_ = yield Query(y + 1, x + 0) # North ne = yield Query(y + 1, x + 1) # Northeast e_ = yield Query(y + 0, x + 1) # East se = yield Query(y - 1, x + 1) # Southeast s_ = yield Query(y - 1, x + 0) # South sw = yield Query(y - 1, x - 1) # Southwest w_ = yield Query(y + 0, x - 1) # West nw = yield Query(y + 1, x - 1) # Northwest neighbor_states = [n_, ne, e_, se, s_, sw, w_, nw] count = 0 for state in neighbor_states: if state == ALIVE: count += 1 return count 我们用虚构的数据来测试一下这个count_neighbors协程.下面这段代码，会针对本细胞的每个相邻细胞，向生成器索要一个Query对象，并产出Query namedtuple。然后通过send方法把状态发给协程，使count_neighbors协程可以收到上一个Query对象所对应的状态(注意我们上文提到的yield表达式一行执行顺序–先右再左) 测试count_neighbors协程1234567891011121314151617&gt;&gt;&gt; it = count_neighbors(10, 5)&gt;&gt;&gt; next(it) # Get the first query, for q1Query(y=11, x=5)&gt;&gt;&gt; it.send(ALIVE) # Send q1 state, get q2Query(y=11, x=6)&gt;&gt;&gt; it.send(ALIVE) # Send q2 state, get q3Query(y=10, x=6)&gt;&gt;&gt; # Send q3 ... q7 states, get q4 ... q8&gt;&gt;&gt; [it.send(state) for state in (EMPTY)*5] # doctest: +ELLIPSIS[Query(y=9, x=6), Query(y=9, x=5), ..., Query(y=11, x=4)]&gt;&gt;&gt; try:... it.send(EMPTY) # Send q8 state, drive coroutine to end... except StopIteration as e:... count = e.value # Value from return statement...&gt;&gt;&gt; count2 step_cell计算出了细胞周围的存活数量，我们就需要根据这个数量来更新细胞的状态。并把得到的状态传给外部调用者。这里我们自定义了一个Transition对象，它表示坐标位于(y,x)的细胞的下一轮的状态。 step_cell根据count_neighbors计算出来的存活状态数量产生下一轮的状态12345678910111213141516171819Transition = namedtuple('Transition', ('y', 'x', 'state')) # state即是下一轮的状态def step_cell(y, x): current_state = yield Query(y, x) # 获取当前状态 neighbors = yield from count_neighbors(y, x) # 委派给子生成器count_neighbors next_state = game_logic(state, neighbors) # game_logic根据规则判断下一轮状态 yield Transition(y, x, next_state)def game_logic(state, neighbors): # 这里其实我们可以使用是否等于3来简化判断 if state == ALIVE: if neighbors &lt; 2: return EMPTY # Die: Too few elif neighbors &gt; 3: return EMPTY # Die: Too many else: if neighbors == 3: return ALIVE # Regenerate return state 下面我们用虚拟数据来测试一下step_cell协程： 测试step_cell协程1234567&gt;&gt;&gt; it = step_cell(10, 5)&gt;&gt;&gt; next(it) # Initial location queryQuery(y=10, x=5)&gt;&gt;&gt; [it.send(st) for st in (ALIVE)*5 + (EMPTY)*3] # doctest: +ELLIPSIS[Query(y=11, x=5), Query(y=11, x=6), ... Query(y=11, x=4)]&gt;&gt;&gt; it.send(EMPTY) # Send q8 state, get game decisionTransition(y=10, x=5, state='-') 上面演示了在网格中一个细胞的一次前进。下面我们把step_cell组合到新的simulate协程之中。新的协程会多次通过yield from 表达式，来推进网格中的每一个细胞。把每个细胞处理完后，simulate协程会产生TICK对象，用以表示当前这一代的细胞已经全部迁移完毕。 simulate12345678TICK = object()def simulate(height, width): while True: for y in range(height): for x in range(width): yield from step_cell(y, x) # 委派给子生成器step_cell yield TICK 网格显示状态为了在真实环境中运行simulate，我们需要把网格中的每个细胞状态表示出来。我们定义一个Grid类，来代表整张网格： Grid类显示网格和细胞状态123456789101112131415161718192021222324class Grid(object): def __init__(self, height, width): self.height = height self.width = width self.rows = [] for _ in range(self.height): self.rows.append([EMPTY] * self.width) def __str__(self): output = '' for row in self.rows: for cell in row: output += cell output += '\n' return output def __getitem__(self, position): y, x = position # 如果传入的坐标值越界，我们用取余来自动折回 return self.rows[y % self.height][x % self.width] def __setitem__(self, position, state): y, x = position self.rows[y % self.height][x % self.width] = state 我们定义了__getitem__和__setitem__两个元方法来设置和获取state。下面我们看一下Grid的显示： 根据参数Grid生成网格和状态123456789101112&gt;&gt;&gt; grid = Grid(5, 9)&gt;&gt;&gt; grid[0, 3] = ALIVE&gt;&gt;&gt; grid[1, 4] = ALIVE&gt;&gt;&gt; grid[2, 2] = ALIVE&gt;&gt;&gt; grid[2, 3] = ALIVE&gt;&gt;&gt; grid[2, 4] = ALIVE&gt;&gt;&gt; print(grid)---*---------*------***---------------------- live_a_generation这个函数把网格内的所有细胞都向前推进一步，待各细胞状态迁移完成后，这些细胞就构成了一张新的网格，该函数会把新的网格返回给调用者。 live_a_generation123456789101112def live_a_generation(grid, sim): # grid: 现阶段网格对象；sim: simulate生成器对象 progeny = Grid(grid.height, grid.width) # 下一代网格对象 item = next(sim) while item is not TICK: if isinstance(item, Query): #计算附近细胞 state = grid[item.y, item.x] item = sim.send(state) else: # Must be a Transition，附近细胞算完了,得到Transition对象 progeny[item.y, item.x] = item.state item = next(sim) # 生成器运行到下一个yield处，即simulate的下一个坐标处 return progeny #返回下一轮的网格对象 live_a_generation是将当前细胞向前推进一步，现在我们把每一代的结果都显示出来 ColumnPrinter12345678910111213141516171819202122class ColumnPrinter(object): def __init__(self): self.columns = [] def append(self, data): self.columns.append(data) def __str__(self): row_count = 1 for data in self.columns: row_count = max(row_count, len(data.splitlines()) + 1) rows = [''] * row_count for j in range(row_count): for i, data in enumerate(self.columns): line = data.splitlines()[max(0, j - 1)] if j == 0: rows[j] += str(i).center(len(line)) else: rows[j] += line if (i + 1) &lt; len(self.columns): rows[j] += ' | ' return '\n'.join(rows) 我们来看看效果： 12345678910111213&gt;&gt;&gt; columns = ColumnPrinter()&gt;&gt;&gt; sim = simulate(grid.height, grid.width)&gt;&gt;&gt; for i in range(5):... columns.append(str(grid))... grid = live_a_generation(grid, sim)...&gt;&gt;&gt; print(columns) # doctest: +NORMALIZE_WHITESPACE 0 | 1 | 2 | 3 | 4---*----- | --------- | --------- | --------- | -------------*---- | --*-*---- | ----*---- | ---*----- | ----*------***---- | ---**---- | --*-*---- | ----**--- | -----*------------ | ---*----- | ---**---- | ---**---- | ---***------------ | --------- | --------- | --------- | --------- 上面这套的实现方式，其最大优势在于：开发者能够在不修改game_logic函数的前提下，更新该函数外围的那些代码。上面这套范例代码，演示了如何用协程来分离程序中的各个关注点，而关注点的分离，正是一条重要的原则。 结束]]></content>
      <categories>
        <category>Python</category>
        <category>进程线程协程</category>
      </categories>
      <tags>
        <tag>yield from</tag>
        <tag>yield</tag>
        <tag>coroutine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkMLlib-Advanced-Topics]]></title>
    <url>%2F2018%2F01%2F23%2FSparkMLlib-Advanced-Topics%2F</url>
    <content type="text"><![CDATA[线性方法的优化 三种线性方法的优化方法： Limited-memory BFGS(L-BFGS)有限记忆BFGS Normal equation solver for weighted least square用于加权最小二乘法的正态方程求解器 Iteratively reweighted least squares(IRLS)迭代重新加权最小二乘 Limited-memory BFGS (L-BFGS)有限记忆BFGS L-BFGS是拟牛顿方法家族里的一个优化算法，解决min w∈R d f(w)形式的优化问题。L-BFGS方法以二次方程来逼近目标函数来构造Hessian矩阵，不考虑目标函数的二阶偏导数。Hessian矩阵由先前的迭代评估逼近，所以不像直接使用牛顿方法一样可垂直扩展（训练特征的数目）。所以L-BFGS通常比其他一阶优化方法能更快收敛。 象限有限记忆拟牛顿(OWL-QN)算法是L-BFGS的扩展，它可以有效处理L1和弹性网格正则化。L-BFGS在Spark MLlib中用于线性回归、逻辑回归、AFT生存回归和多层感知器的求解。 Normal equation solver for weighted least square用于加权最小二乘法的正态方程求解器MLlib 通过WeightedLeastSquares实现了加权最小二乘法的方程求解器。 Spark MLlib目前支持正态方程的两种求解器：Cholesky分解法和拟牛顿法(L-BFGS / OWL-QN)。乔列斯基因式分解依赖于正定的协方差矩阵（即数据矩阵的列必须是线性无关的），并且如果违反这种条件将会失败。即使协方差矩阵不是正定的，准牛顿方法仍然能够提供合理的解，所以在这种情况下，正规方程求解器也可以退回到拟牛顿法。对于LinearRegression和GeneralizedLinearRegression估计，这种回退目前总是启用的。 WeightedLeastSquares支持L1，L2和弹性网络正则化，并提供启用或禁用正则化和标准化的选项。在没有L1正则化的情况下（即α = 0），存在解析解，可以使用乔列斯基(Cholesky)或拟牛顿(Quasi-Newton)求解器。当α &gt; 0时 不存在解析解，而是使用拟牛顿求解器迭代地求出系数。 为了使正态方程有效，WeightedLeastSquares要求特征数不超过4096个。对于较大的问题，使用L-BFGS代替。 Iteratively reweighted least squares (IRLS)迭代重新加权最小二乘MLlib 通过IterativelyReweightedLeastSquares实现迭代重新加权最小二乘（IRLS）。它可以用来找到广义线性模型(GLM)的最大似然估计，在鲁棒回归和其他优化问题中找到M估计。有关更多信息，请参阅迭代重新加权的最小二乘法以获得最大似然估计，以及一些鲁棒性和抗性替代方法。 它通过以下过程迭代地解决某些优化问题： 线性化目前的解决方案的目标，并更新相应的权重。 通过WeightedLeastSquares解决加权最小二乘（WLS）问题。 重复上述步骤直到收敛。 由于它涉及到WeightedLeastSquares每次迭代求解加权最小二乘（WLS）问题，因此它还要求特征数不超过4096个。目前IRLS被用作GeneralizedLinearRegression的默认求解器。 更多详细信息请查阅Spark ml-advanced 结束]]></content>
      <categories>
        <category>Spark</category>
        <category>MLlib</category>
      </categories>
      <tags>
        <tag>线性方法的优化</tag>
        <tag>有限记忆BFGS</tag>
        <tag>用于加权最小二乘法的正态方程求解器</tag>
        <tag>迭代重新加权最小二乘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkMLlib-ML-Tuning]]></title>
    <url>%2F2018%2F01%2F23%2FSparkMLlib-ML-Tuning%2F</url>
    <content type="text"><![CDATA[模型选择, 超参调整 ML Tuning: model selection(模型选择) and hyperparameter tuning(超参调整)本节介绍如何使用MLlib的工具来调整ML算法和管道。内置的交叉验证和其他工具允许用户优化算法和管道中的超参数。 Model selection(又叫hyperparameter tuning)ML中的一个重要任务是Model Selection(选择模型)，或者使用数据为给定任务找到最佳模型或参数。这也被称为Tuning(调整)。调整可以是以针对三个Estimators算子如LogisticRegression进行调整，也可以对整个Pipeline进行调整。用户可以一次对Pipeline整体进行调整，而不是对Pipeline的每个元素单独进行调整。 MLlib支持使用CrossValidator和TrainValidationSplit的工具进行模型选择。这些工具需要下列项目： Estimator: 需要调整的算法或Pipeline Set of ParamMaps: 可供选择的参数，有时称为“parameter grid”来搜索 Evaluator: 度量标准,衡量一个拟合Model在测试数据上的表现 在较高层面上，这些模型选择工具的工作如下： 他们将输入数据分成单独的训练和测试数据集。 对每组训练数据与测试数据对，对参数表集合，用相应参数来拟合估计器，得到训练后的模型，再使用评估器来评估模型表现。 选择最好的一组参数生成的模型。 其中，对于回归问题评估器可选择RegressionEvaluator，二值数据可选择BinaryClassificationEvaluator，多分类问题可选择MulticlassClassificationEvaluator。评估器里默认的评估准则可通过setMetricName方法重写。 用户可通过ParamGridBuilder构建参数网格。 Cross-ValidationCrossValidator将数据集划分为若干子集分别地进行训练和测试。如当k＝3时，CrossValidator产生3个训练数据与测试数据对，每个数据对使用2/3的数据来训练，1/3的数据来测试。对于一组特定的参数表，CrossValidator计算基于三组不同训练数据与测试数据对训练得到的模型的评估准则的平均值。确定最佳参数表后，CrossValidator最后使用最佳参数表基于全部数据来重新拟合Estimator。 示例： 注意对参数网格进行交叉验证的成本是很高的。如下面例子中，参数网格hashingTF.numFeatures有3个值，lr.regParam有2个值，CrossValidator使用2折交叉验证。这样就会产生(3*2)*2 = 12中不同的模型需要进行训练。在实际的设置中，通常有更多的参数需要设置，且我们可能会使用更多的交叉验证折数（3折或者10折都是经使用的）。所以CrossValidator的成本是很高的，尽管如此，比起启发式的手工验证，交叉验证仍然是目前存在的参数选择方法中非常有用的一种。 Examples有关API的更多详细信息，请参阅CrossValidatorPython文档。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364from pyspark.ml import Pipelinefrom pyspark.ml.classification import LogisticRegressionfrom pyspark.ml.evaluation import BinaryClassificationEvaluatorfrom pyspark.ml.feature import HashingTF, Tokenizerfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilderfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("CrossValidatorExample").getOrCreate()# Prepare training documents, which are labeled.training = spark.createDataFrame([ (0, "a b c d e spark", 1.0), (1, "b d", 0.0), (2, "spark f g h", 1.0), (3, "hadoop mapreduce", 0.0), (4, "b spark who", 1.0), (5, "g d a y", 0.0), (6, "spark fly", 1.0), (7, "was mapreduce", 0.0), (8, "e spark program", 1.0), (9, "a e c l", 0.0), (10, "spark compile", 1.0), (11, "hadoop software", 0.0)], ["id", "text", "label"])# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.tokenizer = Tokenizer(inputCol="text", outputCol="words")hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")lr = LogisticRegression(maxIter=10)pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.# This will allow us to jointly choose parameters for all Pipeline stages.# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.# We use a ParamGridBuilder to construct a grid of parameters to search over.# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.paramGrid = ParamGridBuilder() \ .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \ .addGrid(lr.regParam, [0.1, 0.01]) \ .build()crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=BinaryClassificationEvaluator(), numFolds=2) # use 3+ folds in practice# Run cross-validation, and choose the best set of parameters.cvModel = crossval.fit(training)# Prepare test documents, which are unlabeled.test = spark.createDataFrame([ (4, "spark i j k"), (5, "l m n"), (6, "mapreduce spark"), (7, "apache hadoop")], ["id", "text"])# Make predictions on test documents. cvModel uses the best model found (lrModel).prediction = cvModel.transform(test)selected = prediction.select("id", "text", "probability", "prediction")for row in selected.collect(): print(row)selected.show()spark.stop() output: 123456789101112Row(id=4, text='spark i j k', probability=DenseVector([0.627, 0.373]), prediction=0.0)Row(id=5, text='l m n', probability=DenseVector([0.3451, 0.6549]), prediction=1.0)Row(id=6, text='mapreduce spark', probability=DenseVector([0.3351, 0.6649]), prediction=1.0)Row(id=7, text='apache hadoop', probability=DenseVector([0.2767, 0.7233]), prediction=1.0)+---+---------------+--------------------+----------+| id| text| probability|prediction|+---+---------------+--------------------+----------+| 4| spark i j k|[0.62703425702535...| 0.0|| 5| l m n|[0.34509123755317...| 1.0|| 6|mapreduce spark|[0.33514123783842...| 1.0|| 7| apache hadoop|[0.27672019766802...| 1.0|+---+---------------+--------------------+----------+ Find full example code at “examples/src/main/python/ml/cross_validator.py” in the Spark repo. Train-Validation Split除了交叉验证以外，Spark还提供 TrainValidationSplit 用以进行超参数调整。和交叉验证评估K次不同， TrainValidationSplit 只对每组参数评估一次。因此它计算代价更低，但当训练数据集不是足够大时，其结果可靠性不高。 与交叉验证不同， TrainValidationSplit仅需要一个训练数据与验证数据对。使用训练比率参数将原始数据划分为两个部分。如当训练比率为0.75时，训练验证分裂使用75%数据以训练，25%数据以验证。 与交叉验证相同，确定最佳参数表后，训练验证分裂最后使用最佳参数表基于全部数据来重新拟合Estimator。 Examples有关API的更多详细信息，请参阅TrainValidationSplitPython文档。 123456789101112131415161718192021222324252627282930313233343536373839from pyspark.ml.evaluation import RegressionEvaluatorfrom pyspark.ml.regression import LinearRegressionfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplitfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("TrainValidationSplitExample").getOrCreate()# Prepare training and test data.data = spark.read.format("libsvm")\ .load("data/mllib/sample_linear_regression_data.txt")train, test = data.randomSplit([0.9, 0.1], seed=12345)lr = LinearRegression(maxIter=10)# We use a ParamGridBuilder to construct a grid of parameters to search over.# TrainValidationSplit will try all combinations of values and determine best model using# the evaluator.paramGrid = ParamGridBuilder()\ .addGrid(lr.regParam, [0.1, 0.01]) \ .addGrid(lr.fitIntercept, [False, True])\ .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\ .build()# In this case the estimator is simply the linear regression.# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.tvs = TrainValidationSplit(estimator=lr, estimatorParamMaps=paramGrid, evaluator=RegressionEvaluator(), # 80% of the data will be used for training, 20% for validation. trainRatio=0.8)# Run TrainValidationSplit, and choose the best set of parameters.model = tvs.fit(train)# Make predictions on test data. model is the model with combination of parameters# that performed best.model.transform(test)\ .select("features", "label", "prediction")\ .show()spark.stop() output: 123456789101112131415161718192021222324+--------------------+--------------------+--------------------+| features| label| prediction|+--------------------+--------------------+--------------------+|(10,[0,1,2,3,4,5,...| -23.51088409032297| -1.6659388625179559||(10,[0,1,2,3,4,5,...| -21.432387764165806| 0.3400877302576284||(10,[0,1,2,3,4,5,...| -12.977848725392104|-0.02335359093652395||(10,[0,1,2,3,4,5,...| -11.827072996392571| 2.5642684021108417||(10,[0,1,2,3,4,5,...| -10.945919657782932| -0.1631314487734783||(10,[0,1,2,3,4,5,...| -10.58331129986813| 2.517790654691453||(10,[0,1,2,3,4,5,...| -10.288657252388708| -0.9443474180536754||(10,[0,1,2,3,4,5,...| -8.822357870425154| 0.6872889429113783||(10,[0,1,2,3,4,5,...| -8.772667465932606| -1.485408580416465||(10,[0,1,2,3,4,5,...| -8.605713514762092| 1.110272909026478||(10,[0,1,2,3,4,5,...| -6.544633229269576| 3.0454559778611285||(10,[0,1,2,3,4,5,...| -5.055293333055445| 0.6441174575094268||(10,[0,1,2,3,4,5,...| -5.039628433467326| 0.9572366607107066||(10,[0,1,2,3,4,5,...| -4.937258492902948| 0.2292114538379546||(10,[0,1,2,3,4,5,...| -3.741044592262687| 3.343205816009816||(10,[0,1,2,3,4,5,...| -3.731112242951253| -2.6826413698701064||(10,[0,1,2,3,4,5,...| -2.109441044710089| -2.1930034039595445||(10,[0,1,2,3,4,5,...| -1.8722161156986976| 0.49547270330052423||(10,[0,1,2,3,4,5,...| -1.1009750789589774| -0.9441633113006601||(10,[0,1,2,3,4,5,...|-0.48115211266405217| -0.6756196573079968|+--------------------+--------------------+--------------------+ Find full example code at “examples/src/main/python/ml/train_validation_split.py” in the Spark repo. 更多相关信息请查阅Spark ml-tuning 结束]]></content>
      <categories>
        <category>Spark</category>
        <category>MLlib</category>
      </categories>
      <tags>
        <tag>模型选择</tag>
        <tag>交叉验证</tag>
        <tag>训练-验证集划分</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkMLlib-Frequent-Pattern-Mining]]></title>
    <url>%2F2018%2F01%2F23%2FSparkMLlib-Frequent-Pattern-Mining%2F</url>
    <content type="text"><![CDATA[Frequent Pattern Mining Frequent Pattern Mining：频繁项目，项目集，子序列或其他子结构的挖掘通常是分析大规模数据集的第一步，这已经成为数据挖掘领域的一个活跃的研究课题。我们将用户引用到Wikipedia的关联规则学习中以获取更多信息。 FP-GrowthFP-growth算法在Han等人的文章“ Mining frequent patterns without candidate generation”中描述，其中“FP”代表频繁模式。给定交易数据集，FP增长的第一步是计算项目频率并识别频繁项目。与为同样目的而设计的Apriori-like算法不同，FP-growth的第二步使用后缀树（FP-tree）结构来编码事务，而不显式生成候选集合，这通常是耗费的。第二步之后，可以从FP-tree中提取频繁项目集。在这里spark.mllib，我们实现了FP-growth的并行版本，称为PFP(详细请查看Li等人：PFP:Parallel FP-growth for query recommendation)。PFP根据事务的后缀分配增长的FP-树的工作，因此比单机实现更具可扩展性。 spark.ml FP的增长实现需要以下（超）参数： minSupport一个项目组的最小支持被确定为频繁的。例如，如果一个项目在5个交易中出现3个，则它具有3/5 = 0.6的支持。 minConfidence生成关联规则的最低置信度。信心是一个关联规则被发现是真实的指标。例如，如果交易项目集X出现4次，X 并且Y只出现2次，则规则的置信度为X =&gt; Y2/4 = 0.5。该参数不会影响对频繁项目集的挖掘，但指定从频繁项集生成关联规则的最小置信度。 numPartitions用于分配工作的分区数量。默认情况下，param未设置，并使用输入数据集的分区数量。 FPGrowthModel规定： freqItemsetsDataFrame格式的频繁项目集(“items”[Array]，“freq”[Long]) associationRulesminConfidence以DataFrame(“antecedent”[Array],”consequent”[Array],“confidence”[Double])格式在上面生成的关联规则。 transform对于每个交易itemsCol，transform方法将比较其项目与每个关联规则的前提。如果记录包含特定关联规则的所有前提条件，则该规则将被视为适用，并将其结果添加到预测结果中。变换法将所有适用规则的后果总结为预测。预测列具有相同的数据类型，itemsCol并且不包含中的现有项目itemsCol。 Examples1234567891011121314151617181920212223from pyspark.ml.fpm import FPGrowthfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("FrequentPatternMiningExample").getOrCreate()df = spark.createDataFrame([ (0, [1, 2, 5]), (1, [1, 2, 3, 5]), (2, [1, 2])], ["id", "items"])fpGrowth = FPGrowth(itemsCol="items", minSupport=0.5, minConfidence=0.6)model = fpGrowth.fit(df)# Display frequent itemsets.model.freqItemsets.show()# Display generated association rules.model.associationRules.show()# transform examines the input items against all the association rules and summarize the# consequents as predictionmodel.transform(df).show()spark.stop() output: 123456789101112131415161718192021222324252627282930313233+---------+----+| items|freq|+---------+----+| [5]| 2|| [5, 1]| 2||[5, 1, 2]| 2|| [5, 2]| 2|| [2]| 3|| [1]| 3|| [1, 2]| 3|+---------+----++----------+----------+------------------+|antecedent|consequent| confidence|+----------+----------+------------------+| [5]| [1]| 1.0|| [5]| [2]| 1.0|| [1, 2]| [5]|0.6666666666666666|| [5, 2]| [1]| 1.0|| [5, 1]| [2]| 1.0|| [2]| [5]|0.6666666666666666|| [2]| [1]| 1.0|| [1]| [5]|0.6666666666666666|| [1]| [2]| 1.0|+----------+----------+------------------++---+------------+----------+| id| items|prediction|+---+------------+----------+| 0| [1, 2, 5]| []|| 1|[1, 2, 3, 5]| []|| 2| [1, 2]| [5]|+---+------------+----------+ Find full example code at “examples/src/main/python/ml/fpgrowth_example.py” in the Spark repo. 更多相关信息请查阅Spark FPGrowth 结束]]></content>
      <categories>
        <category>Spark</category>
        <category>MLlib</category>
      </categories>
      <tags>
        <tag>数据挖掘</tag>
        <tag>频繁项集</tag>
        <tag>关联规则</tag>
        <tag>FP-Growth</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkMLlib-Collaborative-Filtering]]></title>
    <url>%2F2018%2F01%2F22%2FSparkMLlib-Collaborative-Filtering%2F</url>
    <content type="text"><![CDATA[推荐算法 协同过滤常被用于推荐系统。这类技术目标在于填充“用户－商品”联系矩阵中的缺失项。Spark.ml目前支持基于模型的协同过滤，其中用户和商品以少量的潜在因子来描述，用以预测缺失项。Spark.ml使用交替最小二乘（ALS）算法来学习这些潜在因子。spark.ml有以下参数： numBlocks是为了并行化计算而将用户和项目分割成的块的数量（默认为10）。 rank是模型中潜在因素的数量（默认为10）。 maxIter是要运行的最大迭代次数（默认为10）。 regParam指定ALS中的正则化参数（默认为1.0）。 implicitPrefs指定是使用显式反馈ALS变体还是用于隐式反馈数据的变体 （默认false使用显式反馈）。 alpha是适用于ALS的隐式反馈变体的参数，其支配偏好观察值的 基线置信度（默认为1.0）。 非负指定是否对最小二乘使用非负约束（默认为false）。 注意：用于ALS的基于DataFrame的API目前仅支持整数类型的用户和项目ID。用户和项目ID列支持其他数字类型，但ID必须在整数值范围内。 Explicit vs Implict feedfack(显示与隐式反馈)基于矩阵分解的协同过滤的标准方法中，“用户－商品”矩阵中的条目是用户给予商品的显式偏好，例如，用户给电影评级。然而在现实世界中使用时，我们常常只能访问隐式反馈（如意见、点击、购买、喜欢以及分享等），在spark.ml中我们使用“隐式反馈数据集的协同过滤“来处理这类数据。本质上来说它不是直接对评分矩阵进行建模，而是将数据当作数值来看待，这些数值代表用户行为的观察值（如点击次数，用户观看一部电影的持续时间）。这些数值被用来衡量用户偏好观察值的置信水平，而不是显式地给商品一个评分。然后，模型用来寻找可以用来预测用户对商品预期偏好的潜在因子。 Scaling of the regularization parameter(正则化参数缩放)我们调整正则化参数regParam来解决用户在更新用户因子时产生新评分或者商品更新商品因子时收到的新评分带来的最小二乘问题。这个方法叫做“ALS-WR”,它降低regParam对数据集规模的依赖，所以我们可以将从部分子集中学习到的最佳参数应用到整个数据集中时获得同样的性能。 Cold-start strategy(冷启动策略)在使用ALSModel进行预测时，通常会遇到测试数据集中用户和/或物品在训练模型期间不存在的情况。这通常发生在两种情况下： 在生产中，对于没有评分历史记录且尚未训练的新用户或物品（这是“冷启动问题”）。 在交叉验证过程中，数据分为训练集和评估集。当Spark的CrossValidator或者TrainValidationSplit中的使用简单随机拆分，实际上在评估集中普遍遇到用户或物品不存在的问题，而在训练集中并未出现这样的问题 默认情况下，Spark NaN在当用户和/或物品因素不存在于模型中时，Spark在ALSModel.transform时使用NAN作为预测。这在生产系统中可能是有用的，因为它表示一个新的用户或物品，所以系统可以做出一个决定，作为预测。 然而，这在交叉验证期间是不好的，因为任何NaN预测值都将导致NaN评估度量的结果（例如在使用RegressionEvaluator时）。这使得模型无法作出选择。 Spark允许用户将coldStartStrategy参数设置为“drop”，以便删除DataFrame包含NaN值的预测中的任何行。评估指标然后在非NaN数据上计算，并且这是有效的。下面的例子说明了这个参数的用法。 注意：目前支持的冷启动策略是“nan”（上面提到的默认行为）和“drop”。未来可能会支持进一步的策略。 Examples在以下示例中，我们将从MovieLens数据集中加载评分数据 ，每行由用户，电影，评分和时间戳组成。然后，我们训练一个ALS模型，默认情况下，这个模型的评级是明确的（implicitPrefs是false）。我们通过测量评级预测的均方根误差来评估推荐模型。 123456789101112131415161718192021222324252627282930313233from pyspark.ml.evaluation import RegressionEvaluatorfrom pyspark.ml.recommendation import ALSfrom pyspark.sql import Row, SparkSessionspark = SparkSession.builder.appName("CollaborativeFilteringExample").getOrCreate()lines = spark.read.text("data/mllib/als/sample_movielens_ratings.txt").rddparts = lines.map(lambda row: row.value.split("::"))ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]), rating=float(p[2]), timestamp=int(p[3])))ratings = spark.createDataFrame(ratingsRDD)(training, test) = ratings.randomSplit([0.8, 0.2])# Build the recommendation model using ALS on the training data# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metricsals = ALS(maxIter=5, regParam=0.01, userCol="userId", itemCol="movieId", ratingCol="rating", coldStartStrategy="drop")model = als.fit(training)# Evaluate the model by computing the RMSE on the test datapredictions = model.transform(test)evaluator = RegressionEvaluator(metricName="rmse", labelCol="rating", predictionCol="prediction")rmse = evaluator.evaluate(predictions)print("Root-mean-square error = " + str(rmse))# Generate top 10 movie recommendations for each useruserRecs = model.recommendForAllUsers(10)userRecs.show()# userRecs.filter(userRecs['userId'] == 1).select('recommendations').show(truncate=False) # 看看给userId==1的用户推荐了哪10部电影# Generate top 10 user recommendations for each moviemovieRecs = model.recommendForAllItems(10)movieRecs.show()spark.stop() output: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152Root-mean-square error = 1.742790392299329+------+--------------------+|userId| recommendations|+------+--------------------+| 28|[[92,5.0226665], ...|| 26|[[81,5.6422243], ...|| 27|[[18,4.069487], [...|| 12|[[19,6.6280622], ...|| 22|[[74,5.141776], [...|| 1|[[46,4.550467], [...|| 13|[[93,3.4347346], ...|| 6|[[25,5.163864], [...|| 16|[[54,4.865331], [...|| 3|[[75,5.5034533], ...|| 20|[[22,4.563996], [...|| 5|[[46,6.402665], [...|| 19|[[94,4.0123057], ...|| 15|[[46,4.932741], [...|| 17|[[46,5.196739], [...|| 9|[[65,4.703967], [...|| 4|[[85,4.958973], [...|| 8|[[43,5.747457], [...|| 23|[[32,5.279368], [...|| 7|[[62,5.059422], [...|+------+--------------------+only showing top 20 rows+-------+--------------------+|movieId| recommendations|+-------+--------------------+| 31|[[12,3.5030043], ...|| 85|[[14,5.6425133], ...|| 65|[[23,4.9570875], ...|| 53|[[14,5.271897], [...|| 78|[[12,1.4262005], ...|| 34|[[2,3.9721959], [...|| 81|[[26,5.6422243], ...|| 28|[[18,5.0155253], ...|| 76|[[14,4.9423637], ...|| 26|[[5,4.06113], [15...|| 27|[[11,5.220525], [...|| 44|[[18,3.830072], [...|| 12|[[28,4.8217144], ...|| 91|[[12,3.090134], [...|| 22|[[18,8.003841], [...|| 93|[[2,4.621838], [2...|| 47|[[6,4.48774], [25...|| 1|[[27,3.527709], [...|| 52|[[8,5.0824013], [...|| 13|[[23,4.004786], [...|+-------+--------------------+only showing top 20 rows Find full example code at “examples/src/main/python/ml/als_example.py” in the Spark repo. 如果评分矩阵是从另一个信息源（即它是从其他信号推断）得出，可以设置implicitPrefs以true获得更好的效果： 12als = ALS(maxIter=5, regParam=0.01, implicitPrefs=True, userCol="userId", itemCol="movieId", ratingCol="rating") 更多相关信息请查阅spark 协同过滤 结束]]></content>
      <categories>
        <category>Spark</category>
        <category>MLlib</category>
      </categories>
      <tags>
        <tag>协同过滤</tag>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkMLlib-Clustering]]></title>
    <url>%2F2018%2F01%2F22%2FSparkMLlib-Clustering%2F</url>
    <content type="text"><![CDATA[聚类算法 本节介绍MLlib中的聚类算法(KMeans, LDA, GMM)。在基于RDD-API聚类指南里还提供了有关这些算法的相关信息。 K-meansK-means是一个常用的聚类算法来将数据点按预定的簇数进行聚集。K-means算法的基本思想是：以空间中k个点为中心进行聚类，对最靠近他们的对象归类。通过迭代的方法，逐次更新各聚类中心的值，直至得到最好的聚类结果。 假设要把样本集分为c个类别，算法描述如下： （1）适当选择c个类的初始中心； （2）在第k次迭代中，对任意一个样本，求其到c个中心的距离，将该样本归到距离最短的中心所在的类； （3）利用均值等方法更新该类的中心值； （4）对于所有的c个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束，否则继续迭代。 MLlib工具包含并行的K-means++算法，称为kmeans||。Kmeans是一个Estimator，它在基础模型之上产生一个KMeansModel。 Input Columns(输入列) Param name(参数名称) Type(s)(类型) Default(默认) Description(描述) featuresCol Vector “features” Feature vector(特征向量) Output Columns(输出列) Param name(参数名称) Type(s)(类型) Default(默认) Description(描述) predictionCol Int “prediction” Predicted cluster center(预测的聚类中心) Examples123456789101112131415161718192021from pyspark.ml.clustering import KMeansfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("ClusterExample").getOrCreate()# Loads data.dataset = spark.read.format("libsvm").load("data/mllib/sample_kmeans_data.txt")# Trains a k-means model.kmeans = KMeans().setK(2).setSeed(1)model = kmeans.fit(dataset)# Evaluate clustering by computing Within Set Sum of Squared Errors.wssse = model.computeCost(dataset)print("Within Set Sum of Squared Errors = " + str(wssse))# Shows the result.centers = model.clusterCenters()print("Cluster Centers: ")for center in centers: print(center)spark.stop() output: 12345Within Set Sum of Squared Errors = 0.11999999999994547Cluster Centers: [ 0.1 0.1 0.1][ 9.1 9.1 9.1] Find full example code at “examples/src/main/python/ml/kmeans_example.py” in the Spark repo. Latent Dirichlet allocation(LDA)LDA（Latent Dirichlet Allocation）是一种文档主题生成模型，也称为一个三层贝叶斯概率模型，包含词、主题和文档三层结构。所谓生成模型，就是说，我们认为一篇文章的每个词都是通过“以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语”这样一个过程得到。文档到主题服从多项式分布，主题到词服从多项式分布。 LDA是一种非监督机器学习技术，可以用来识别大规模文档集（document collection）或语料库（corpus）中潜藏的主题信息。它采用了词袋（bag of words）的方法，这种方法将每一篇文档视为一个词频向量，从而将文本信息转化为了易于建模的数字信息。但是词袋方法没有考虑词与词之间的顺序，这简化了问题的复杂性，同时也为模型的改进提供了契机。每一篇文档代表了一些主题所构成的一个概率分布，而每一个主题又代表了很多单词所构成的一个概率分布。 LDA被实现为一个Estimator,既支持EMLDAOptimizer和OnlineLDAOptimizer，并生成一个LDAModel作为基础模型。如果需要的话，专家用户可以将EMLDAOptimizer生成的LDAModel映射到一个DistributedLDAModel Examples12345678910111213141516171819202122232425from pyspark.ml.clustering import LDAfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("LDAExample").getOrCreate()# Loads data.dataset = spark.read.format("libsvm").load("data/mllib/sample_lda_libsvm_data.txt")# Trains a LDA model.lda = LDA(k=10, maxIter=10)model = lda.fit(dataset)ll = model.logLikelihood(dataset)lp = model.logPerplexity(dataset)print("The lower bound on the log likelihood of the entire corpus: " + str(ll))print("The upper bound on perplexity: " + str(lp))# Describe topics.topics = model.describeTopics(3)print("The topics described by their top-weighted terms:")topics.show(truncate=False)# Shows the resulttransformed = model.transform(dataset)transformed.show(truncate=False)spark.stop() output: 12345678910111213141516171819202122232425262728293031323334The lower bound on the log likelihood of the entire corpus: -797.8018456907539The upper bound on perplexity: 3.068468635551357The topics described by their top-weighted terms:+-----+-----------+---------------------------------------------------------------+|topic|termIndices|termWeights |+-----+-----------+---------------------------------------------------------------+|0 |[0, 4, 7] |[0.13939487929625935, 0.13346874874963285, 0.11911498796394984]||1 |[8, 6, 0] |[0.09761719173430919, 0.09664530483154511, 0.0959033498887414] ||2 |[5, 9, 1] |[0.09763288175177705, 0.0967699480930826, 0.09474971437446654] ||3 |[6, 2, 5] |[0.09993087551790403, 0.09802667103524504, 0.09669791743434605]||4 |[10, 5, 8] |[0.10838084105098059, 0.1065719519796393, 0.10564271921581836] ||5 |[2, 5, 3] |[0.09975664174839147, 0.09917147147531298, 0.09482946730767593]||6 |[1, 7, 3] |[0.1025918379349122, 0.09670884980694468, 0.09661321616852961] ||7 |[3, 10, 6] |[0.18074276445784626, 0.17140880975201497, 0.11846617165050731]||8 |[7, 9, 1] |[0.10376667278659339, 0.10266984655859988, 0.10261491999135175]||9 |[5, 9, 4] |[0.17217259005160918, 0.11130983487715354, 0.10625585388024414]|+-----+-----------+---------------------------------------------------------------++-----+---------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+|label|features |topicDistribution |+-----+---------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+|0.0 |(11,[0,1,2,4,5,6,7,10],[1.0,2.0,6.0,2.0,3.0,1.0,1.0,3.0]) |[0.004834482522877391,0.004775061546874506,0.0047750850624618665,0.00477508209536724,0.004775110752126829,0.0047751198765325934,0.0047750802565546275,0.44999380128294686,0.004775119757841731,0.5117460568464164] ||1.0 |(11,[0,1,3,4,7,10],[1.0,3.0,1.0,3.0,2.0,1.0]) |[0.9268994208923648,0.007965511763080765,0.007965521320089061,0.007965447383722308,0.007965587789582014,0.007965461329343004,0.00796558757403698,0.009276986136774072,0.007965614108681227,0.008064861702326028] ||2.0 |(11,[0,1,2,5,6,8,9],[1.0,4.0,1.0,4.0,9.0,1.0,2.0]) |[0.004202815262490896,0.004151229704235803,0.004151279248440336,0.004151250849060332,0.004151298320120848,0.004151248811452763,0.004151213592542253,0.6501025149437936,0.00415114952939257,0.3166359997384707] ||3.0 |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,3.0,9.0]) |[0.0037170513237456872,0.0036715329471578005,0.0036715360552429213,0.003671511493261907,0.003671797370463146,0.0036715102318871204,0.0036715134308361727,0.9668647838101413,0.003671504403863317,0.003717258933400576] ||4.0 |(11,[0,1,2,3,4,6,9,10],[3.0,1.0,1.0,9.0,3.0,2.0,1.0,3.0]) |[0.004027376743557338,0.003977866599137274,0.003977850254362953,0.003977835428829377,0.0039778820932092175,0.003977853048840427,0.003977852184563374,0.9641001717255747,0.0039778458818949,0.004027466040030248] ||5.0 |(11,[0,1,3,4,5,6,7,8,9],[4.0,2.0,3.0,4.0,5.0,1.0,1.0,1.0,4.0]) |[0.003717509832713523,0.0036716615407946934,0.0036716846624067615,0.0036716395255962085,0.0036717149575019995,0.0036716664005927474,0.0036716667567801204,0.27461258177043146,0.0036716647781321666,0.6959682097750503]||6.0 |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,2.0,9.0]) |[0.0038659082828356533,0.003818570338009387,0.0038185658000222077,0.0038185390646671936,0.003818726199778954,0.0038185379956121677,0.003818554784511252,0.9655379642100607,0.003818526437489602,0.003866106887012979] ||7.0 |(11,[0,1,2,3,4,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,1.0,2.0,1.0,3.0])|[0.004394125793389081,0.004340066102223131,0.004340117929521572,0.004340091402319875,0.004340183500883856,0.004340117374988447,0.004340096103563213,0.9608305723851966,0.004340058125232322,0.004394571282681922] ||8.0 |(11,[0,1,3,4,5,6,7],[4.0,4.0,3.0,4.0,2.0,1.0,3.0]) |[0.9601715212707249,0.0043400767428901635,0.004340086711133699,0.004340041546373581,0.004340093118553618,0.004340077924408194,0.004340099543124161,0.005053547015193133,0.004340064942938327,0.004394391184660286] ||9.0 |(11,[0,1,2,4,6,8,9,10],[2.0,8.0,2.0,3.0,2.0,2.0,7.0,2.0]) |[0.003332384443784424,0.0032914608990001755,0.003291474583522146,0.003291442358715674,0.003291502923651029,0.0032914477446806248,0.003291451230227242,0.9702948142666302,0.0032914840138979083,0.0033325375358905047] ||10.0 |(11,[0,1,2,3,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,2.0,3.0,3.0]) |[0.004202933475197545,0.004151218860618786,0.004151338270182237,0.004151288340705789,0.004151431515312671,0.004151332888945593,0.00415129142785515,0.9625342071313459,0.0041512327632204984,0.004203725326615945] ||11.0 |(11,[0,1,4,5,6,7,9],[4.0,1.0,4.0,5.0,1.0,3.0,1.0]) |[0.5794463100207559,0.004774699657046339,0.004774740812070836,0.0047746922036681246,0.004774755044701768,0.004774721978296648,0.0047747158288502884,0.0055583559655138,0.004774694223725667,0.38157231426537064] |+-----+---------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Find full example code at “examples/src/main/python/ml/lda_example.py” in the Spark repo. Bisecting k-means二分K均值算法是一种层次聚类算法，使用自顶向下的逼近：所有的观察值开始是一个簇，递归地向下一个层级分裂。分裂依据为选择能最大程度降低聚类代价函数（也就是误差平方和）的簇划分为两个簇。以此进行下去，直到簇的数目等于用户给定的数目k为止。二分K均值常常比传统K均值算法有更快的计算速度，但产生的簇群与传统K均值算法往往也是不同的。 BisectingKMeans是一个Estimator，在基础模型上训练得到BisectingKMeansModel。 Examples123456789101112131415161718192021from pyspark.ml.clustering import BisectingKMeansfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("BisectingKMeansExample").getOrCreate()# Loads data.dataset = spark.read.format("libsvm").load("data/mllib/sample_kmeans_data.txt")# Trains a bisecting k-means model.bkm = BisectingKMeans().setK(2).setSeed(1)model = bkm.fit(dataset)# Evaluate clustering.cost = model.computeCost(dataset)print("Within Set Sum of Squared Errors = " + str(cost))# Shows the result.print("Cluster Centers: ")centers = model.clusterCenters()for center in centers: print(center)spark.stop() output: 1234Within Set Sum of Squared Errors = 0.11999999999994547Cluster Centers: [ 0.1 0.1 0.1][ 9.1 9.1 9.1] Find full example code at “examples/src/main/python/ml/bisecting_k_means_example.py” in the Spark repo. Gaussian Mixture Model(GMM)混合高斯模型描述数据点以一定的概率服从k种高斯子分布的一种混合分布。Spark.ml使用EM算法给出一组样本的极大似然模型。 GaussianMixture被实现为一个Estimator,并生成一个GaussianMixtureModel基本模型。 Input Columns Param name Type(s) Default Description featuresCol Vector “features” Feature vector Output Columns Param name Type(s) Default Description predictionCol Int “prediction” Predicted cluster center probabilityCol Vector “probability” Probability of each cluster Examples12345678910111213from pyspark.ml.clustering import GaussianMixturefrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("GaussianMixtureExample").getOrCreate()# loads datadataset = spark.read.format("libsvm").load("data/mllib/sample_kmeans_data.txt")gmm = GaussianMixture().setK(2).setSeed(538009335)model = gmm.fit(dataset)print("Gaussians shown as a DataFrame: ")model.gaussiansDF.show(truncate=False)spark.stop() output: 1234567891011Gaussians shown as a DataFrame: +-------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+|mean |cov |+-------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+|[0.10000000000001552,0.10000000000001552,0.10000000000001552]|0.006666666666806454 0.006666666666806454 0.006666666666806454 0.006666666666806454 0.006666666666806454 0.006666666666806454 0.006666666666806454 0.006666666666806454 0.006666666666806454 ||[9.099999999999984,9.099999999999984,9.099999999999984] |0.006666666666812185 0.006666666666812185 0.006666666666812185 0.006666666666812185 0.006666666666812185 0.006666666666812185 0.006666666666812185 0.006666666666812185 0.006666666666812185 |+-------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Find full example code at “examples/src/main/python/ml/gaussian_mixture_example.py” in the Spark repo. 更多相关信息请查阅Spark Clustering文档 结束]]></content>
      <categories>
        <category>Spark</category>
        <category>MLlib</category>
      </categories>
      <tags>
        <tag>KMeans</tag>
        <tag>LDA</tag>
        <tag>Bisecting KMeans</tag>
        <tag>GMM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkMLlib-Classification-and-Regression]]></title>
    <url>%2F2018%2F01%2F22%2FSparkMLlib-Classification-and-Regression%2F</url>
    <content type="text"><![CDATA[本节涵盖分类和回归算法。它还包括讨论特定类别算法的部分，例如线性方法，树和集成方法。 ClassficationLogistic RegressionLogistic Regression(逻辑回归)是一个流行的分类问题预测方法。它是Generalized Linear models(广义线性模型)的一个特殊应用以预测结果概率。在spark.ml逻辑回归中，可以使用二分类逻辑回归来预测二分类问题，或者可以使用多分类逻辑回归来预测多类别分类问题。使用family 参数来选择这两个算法，或者不设置，Spark会推断出正确的变量。 将family参数设置为“multinomial”时，多分类逻辑回归可以用于二分类问题。它会产生两套coeficients(w)和两个inercepts(b)。 当在具无拦截的连续非零列的数据集上训练LogisticRegressionModel时，Spark MLlib输出连续非零列零系数。这种行为与R glmnet相同，但与LIBSVM不同。 Binomial Logistic Regression有关二项逻辑回归实现的更多背景和更多细节，请参阅中的logistic regression spark.mllib文档。 Examples 下面的例子显示了如何用弹性网络正则化的二元分类问题训练二项和多项逻辑回归模型。elasticNetParam参数对应于α(学习率)，regParam参数对应于λ(正则化参数)。有关参数的更多细节可以在Python API文档中找到。 1234567891011121314151617181920212223242526from pyspark.ml.classification import LogisticRegressionfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("LogisticRegressionWithElasticNetExample").getOrCreate()# Load training datatraining = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)# Fit the modellrModel = lr.fit(training)# Print the coefficients and intercept for logistic regressionprint("Coefficients: " + str(lrModel.coefficients))print("Intercept: " + str(lrModel.intercept))# We can also use the multinomial family for binary classificationmlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family="multinomial")# Fit the modelmlrModel = mlr.fit(training)# Print the coefficients and intercepts for logistic regression with multinomial familyprint("Multinomial coefficients: " + str(mlrModel.coefficientMatrix))print("Multinomial intercepts: " + str(mlrModel.interceptVector))spark.stop() output: 12345678910111213141516171819202122Coefficients: (692,[244,263,272,300,301,328,350,351,378,379,405,406,407,428,433,434,455,456,461,462,483,484,489,490,496,511,512,517,539,540,568],[-7.35398352419e-05,-9.10273850559e-05,-0.000194674305469,-0.000203006424735,-3.14761833149e-05,-6.84297760266e-05,1.58836268982e-05,1.40234970914e-05,0.00035432047525,0.000114432728982,0.000100167123837,0.00060141093038,0.000284024817912,-0.000115410847365,0.000385996886313,0.000635019557424,-0.000115064123846,-0.00015271865865,0.000280493380899,0.000607011747119,-0.000200845966325,-0.000142107557929,0.000273901034116,0.00027730456245,-9.83802702727e-05,-0.000380852244352,-0.000253151980086,0.000277477147708,-0.000244361976392,-0.00153947446876,-0.000230733284113])Intercept: 0.22456315961250325Multinomial coefficients: 2 X 692 CSRMatrix(0,244) 0.0(0,263) 0.0001(0,272) 0.0001(0,300) 0.0001(0,350) -0.0(0,351) -0.0(0,378) -0.0(0,379) -0.0(0,405) -0.0(0,406) -0.0006(0,407) -0.0001(0,428) 0.0001(0,433) -0.0(0,434) -0.0007(0,455) 0.0001(0,456) 0.0001....Multinomial intercepts: [-0.120658794459,0.120658794459] Find full example code at “examples/src/main/python/ml/logistic_regression_with_elastic_net.py” in the Spark repo. spark.ml逻辑回归工具还支持在训练集上提取模型的总结。请注意，在 BinaryLogisticRegressionSummary中存储为DataFrame的预测结果和指标被注释@transient(临时的)，因此仅适用于驱动程序。 LogisticRegressionTrainingSummary 为 LogisticRegressionModel提供了一个summary。目前只支持二分类问题。将来会增加对多分类问题模型summary的支持。 继续上面的例子 12345678910111213141516171819202122232425262728293031323334353637383940from pyspark.ml.classification import LogisticRegressionfrom pyspark.sql import SparkSessionspark = SparkSession \ .builder \ .appName("LogisticRegressionSummary") \ .getOrCreate()# Load training datatraining = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)# Fit the modellrModel = lr.fit(training)# $example on$# Extract the summary from the returned LogisticRegressionModel instance trained# in the earlier exampletrainingSummary = lrModel.summary# Obtain the objective per iterationobjectiveHistory = trainingSummary.objectiveHistoryprint("objectiveHistory:")for objective in objectiveHistory: print(objective)# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.trainingSummary.roc.show()print("areaUnderROC: " + str(trainingSummary.areaUnderROC))# Set the model threshold to maximize F-MeasurefMeasure = trainingSummary.fMeasureByThresholdmaxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \ .select('threshold').head()['threshold']lr.setThreshold(bestThreshold)# $example off$spark.stop() output: 123456789101112131415161718192021222324252627282930313233343536373839objectiveHistory:0.68331491357416720.66628757514737340.62170685460346180.61272652458878870.60603479868028730.60317506875715620.59696215348362740.59407430319831180.59060892433390220.58947245764910420.5882187775729587+---+--------------------+|FPR| TPR|+---+--------------------+|0.0| 0.0||0.0|0.017543859649122806||0.0| 0.03508771929824561||0.0| 0.05263157894736842||0.0| 0.07017543859649122||0.0| 0.08771929824561403||0.0| 0.10526315789473684||0.0| 0.12280701754385964||0.0| 0.14035087719298245||0.0| 0.15789473684210525||0.0| 0.17543859649122806||0.0| 0.19298245614035087||0.0| 0.21052631578947367||0.0| 0.22807017543859648||0.0| 0.24561403508771928||0.0| 0.2631578947368421||0.0| 0.2807017543859649||0.0| 0.2982456140350877||0.0| 0.3157894736842105||0.0| 0.3333333333333333|+---+--------------------+only showing top 20 rowsareaUnderROC: 1.0 Find full example code at “examples/src/main/python/ml/logistic_regression_summary_example.py” in the Spark repo. Multinomial Logistic Regression多分类通过多项逻辑（softmax）回归来支持。在多项逻辑回归中，算法产生K sets的系数集合(类似机器学习中的W)或维度K × J的矩阵其中K是结果分类数量和J是特征的数量。如果算法拟合时使用了偏置(类似机器学习中的b)，则偏置b也是一个K长度的向量。 多项逻辑回归的系数(coefficients)：coefficientMatrix，偏置(intercepts):interceptVector。 coefficients和intercept在用多项逻辑回归训练模型中不适用。请使用coefficientMatrix，interceptVector 结果的条件概率使用的是softmax function建模，我们使用多分类响应模型将加权负对数似然最小化，并使用elastic-net penalty来控制过拟合。 关于推导的细节请查阅这里. 下面的例子展示了如何训练具有弹性网络正则化的多类逻辑回归模型。 12345678910111213141516171819from pyspark.ml.classification import LogisticRegressionfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("MultinomialLogisticRegression").getOrCreate()# Load training datatraining = spark \ .read \ .format("libsvm") \ .load("data/mllib/sample_multiclass_classification_data.txt")lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)# Fit the modellrModel = lr.fit(training)# Print the coefficients and intercept for multinomial logistic regressionprint("Coefficients: \n" + str(lrModel.coefficientMatrix))print("Intercept: " + str(lrModel.interceptVector))spark.stop() output: 123456Coefficients: 3 X 4 CSRMatrix(0,3) 0.3176(1,2) -0.7804(1,3) -0.377Intercept: [0.0516523165983,-0.123912249909,0.0722599333102] Find full example code at “examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py” in the Spark repo. Decision Tree Classifier决策树是一种流行的分类和回归方法。关于spark.ml实现的更多信息可以在决策树部分进一步找到。 Examples 以下示例以LibSVM格式加载数据集，将其分解为训练集和测试集，在训练数据集上训练，然后在保留的测试集上进行评估。我们使用两个特征变换器来准备数据; 这些帮助建立对标签和分类特征的索引，添加元数据到决策树算法可以识别的DataFrame上。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546from pyspark.ml import Pipelinefrom pyspark.ml.classification import DecisionTreeClassifierfrom pyspark.ml.feature import StringIndexer, VectorIndexerfrom pyspark.ml.evaluation import MulticlassClassificationEvaluatorfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("DecisionTreeExample").getOrCreate()# Load the data stored in LIBSVM format as a DataFrame.data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")# Index labels, adding metadata to the label column.# Fit on whole dataset to include all labels in index.labelIndexer = StringIndexer(inputCol="label", outputCol="indexedLabel").fit(data)# Automatically identify categorical features, and index them.# We specify maxCategories so features with &gt; 4 distinct values are treated as continuous.featureIndexer =\ VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=4).fit(data)# Split the data into training and test sets (30% held out for testing)(trainingData, testData) = data.randomSplit([0.7, 0.3])# Train a DecisionTree model.dt = DecisionTreeClassifier(labelCol="indexedLabel", featuresCol="indexedFeatures")# Chain indexers and tree in a Pipelinepipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])# Train model. This also runs the indexers.model = pipeline.fit(trainingData)# Make predictions.predictions = model.transform(testData)# Select example rows to display.predictions.select("prediction", "indexedLabel", "features").show(5)# Select (prediction, true label) and compute test errorevaluator = MulticlassClassificationEvaluator( labelCol="indexedLabel", predictionCol="prediction", metricName="accuracy")accuracy = evaluator.evaluate(predictions)print("Test Error = %g " % (1.0 - accuracy))treeModel = model.stages[2]# summary onlyprint(treeModel)spark.stop() output: 12345678910111213+----------+------------+--------------------+|prediction|indexedLabel| features|+----------+------------+--------------------+| 1.0| 1.0|(692,[95,96,97,12...|| 1.0| 1.0|(692,[100,101,102...|| 1.0| 1.0|(692,[122,123,124...|| 1.0| 1.0|(692,[125,126,127...|| 1.0| 1.0|(692,[126,127,128...|+----------+------------+--------------------+only showing top 5 rowsTest Error = 0.0454545 DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4b29a1e1d3b0e6e09baf) of depth 1 with 3 nodes Find full example code at “examples/src/main/python/ml/decision_tree_classification_example.py” in the Spark repo. Random Forest Classifier随机森林是一种流行的分类和回归方法。关于spark.ml实现的更多信息可以在关于随机森林的章节中进一步找到。 Examples 以下示例以LibSVM格式加载数据集，将其分解为训练集和测试集，在训练数据集上训练，然后在测试集上进行评估。我们使用两个特征变换器来准备数据,这有助于帮助索引标签和分类特征的类别，添加元数据到DtaFrame(基于树的算法可以识别的)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from pyspark.ml import Pipelinefrom pyspark.ml.classification import RandomForestClassifierfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexerfrom pyspark.ml.evaluation import MulticlassClassificationEvaluatorfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName('RandomForestExample').getOrCreate()# Load and parse the data file, converting it to a DataFrame.data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")# Index labels, adding metadata to the label column.# Fit on whole dataset to include all labels in index.labelIndexer = StringIndexer(inputCol="label", outputCol="indexedLabel").fit(data)# Automatically identify categorical features, and index them.# Set maxCategories so features with &gt; 4 distinct values are treated as continuous.featureIndexer =\ VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=4).fit(data)# Split the data into training and test sets (30% held out for testing)(trainingData, testData) = data.randomSplit([0.7, 0.3])# Train a RandomForest model.rf = RandomForestClassifier(labelCol="indexedLabel", featuresCol="indexedFeatures", numTrees=10)# Convert indexed labels back to original labels.labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel", labels=labelIndexer.labels)# Chain indexers and forest in a Pipelinepipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])# Train model. This also runs the indexers.model = pipeline.fit(trainingData)# Make predictions.predictions = model.transform(testData)# Select example rows to display.predictions.select("predictedLabel", "label", "features").show(5)# Select (prediction, true label) and compute test errorevaluator = MulticlassClassificationEvaluator( labelCol="indexedLabel", predictionCol="prediction", metricName="accuracy")accuracy = evaluator.evaluate(predictions)print("Test Error = %g" % (1.0 - accuracy))rfModel = model.stages[2]print(rfModel) # summary onlyspark.stop() output: 12345678910111213+--------------+-----+--------------------+|predictedLabel|label| features|+--------------+-----+--------------------+| 0.0| 0.0|(692,[98,99,100,1...|| 1.0| 0.0|(692,[100,101,102...|| 0.0| 0.0|(692,[124,125,126...|| 0.0| 0.0|(692,[124,125,126...|| 0.0| 0.0|(692,[124,125,126...|+--------------+-----+--------------------+only showing top 5 rowsTest Error = 0.0416667RandomForestClassificationModel (uid=RandomForestClassifier_4e8ca5bee5432b4471d3) with 10 trees Find full example code at “examples/src/main/python/ml/random_forest_classifier_example.py” in the Spark repo. Gradient-Boosted Tree ClassifierGradient-boosted trees (GBTs) 是一种流行的分类和回归方法，是一种决策树的集成算法。关于spark.ml实现的更多信息可以在GBT的一节中找到。 Examples 123456789101112131415161718192021222324252627282930313233343536373839404142434445from pyspark.ml import Pipelinefrom pyspark.ml.classification import GBTClassifierfrom pyspark.ml.feature import StringIndexer, VectorIndexerfrom pyspark.ml.evaluation import MulticlassClassificationEvaluatorfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName('GBTExample').getOrCreate()# Load and parse the data file, converting it to a DataFrame.data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")# Index labels, adding metadata to the label column.# Fit on whole dataset to include all labels in index.labelIndexer = StringIndexer(inputCol="label", outputCol="indexedLabel").fit(data)# Automatically identify categorical features, and index them.# Set maxCategories so features with &gt; 4 distinct values are treated as continuous.featureIndexer =\ VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=4).fit(data)# Split the data into training and test sets (30% held out for testing)(trainingData, testData) = data.randomSplit([0.7, 0.3])# Train a GBT model.gbt = GBTClassifier(labelCol="indexedLabel", featuresCol="indexedFeatures", maxIter=10)# Chain indexers and GBT in a Pipelinepipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])# Train model. This also runs the indexers.model = pipeline.fit(trainingData)# Make predictions.predictions = model.transform(testData)# Select example rows to display.predictions.select("prediction", "indexedLabel", "features").show(5)# Select (prediction, true label) and compute test errorevaluator = MulticlassClassificationEvaluator( labelCol="indexedLabel", predictionCol="prediction", metricName="accuracy")accuracy = evaluator.evaluate(predictions)print("Test Error = %g" % (1.0 - accuracy))gbtModel = model.stages[2]print(gbtModel) # summary onlyspark.stop() output: 12345678910111213+----------+------------+--------------------+|prediction|indexedLabel| features|+----------+------------+--------------------+| 1.0| 1.0|(692,[98,99,100,1...|| 1.0| 1.0|(692,[100,101,102...|| 1.0| 1.0|(692,[124,125,126...|| 1.0| 1.0|(692,[124,125,126...|| 1.0| 1.0|(692,[124,125,126...|+----------+------------+--------------------+only showing top 5 rowsTest Error = 0.030303GBTClassificationModel (uid=GBTClassifier_439db0c5094b1786e321) with 10 trees Find full example code at “examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py” in the Spark repo Multilayer Perception Classifier多层感知器分类器（MLPC）是基于前馈人工神经网络的分类器。MLPC由多层节点组成。每层完全连接到网络中的下一层。输入层中的节点表示输入数据。所有其他节点通过输入与节点权重w 和偏差b的线性组合并应用激活函数将输入映射到输出。K + 1层的MPLC可写成如下的矩阵形式： 中间层节点使用sigmoid（logistic）函数：f(zi) = 1/(1 + e^-zi)\ 输出层中的节点使用softmax函数：f(zi) = e^zi/(∑e^zi) \ 输出层中N代表类别数目\ 多层感知机通过方向向传播来学习模型，我们使用逻辑损失函数优化,L-BFGS作为优化程序. 12345678910111213141516171819202122232425262728293031from pyspark.ml.classification import MultilayerPerceptronClassifierfrom pyspark.ml.evaluation import MulticlassClassificationEvaluatorfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName('MLPCExample').getOrCreate()# Load training datdata = spark.read.format("libsvm")\ .load("data/mllib/sample_multiclass_classification_data.txt")# Split the data into train and testsplits = data.randomSplit([0.6, 0.4], 1234)train = splits[0]test = splits[1]# specify layers for the neural network:# input layer of size 4 (features), two intermediate of size 5 and 4# and output of size 3 (classes)layers = [4, 5, 4, 3]# create the trainer and set its parameterstrainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)# train the modelmodel = trainer.fit(train)# compute accuracy on the test setresult = model.transform(test)predictionAndLabels = result.select("prediction", "label")evaluator = MulticlassClassificationEvaluator(metricName="accuracy")print("Test set accuracy = " + str(evaluator.evaluate(predictionAndLabels)))spark.stop() output: 1Test set accuracy = 0.8627450980392157 Find full example code at “examples/src/main/python/ml/multilayer_perceptron_classification.py” in the Spark repo. Linear Support Vector Machine一个支持向量机在高或无限维空间构建一个或一簇超平面，该空间可用于分类，回归或其他任务。直觉上，通过寻找距离任何类别的最近的训练数据点（所谓的functional margin）最大距离的超平面来实现良好的分离，因为一般而言，margin越大，分类器的泛化误差越低。Spark ML中的LinearSVC支持线性SVM的二元分类。在内部，它使用OWLQN优化器来优化hinge loss。 1234567891011121314151617from pyspark.ml.classification import LinearSVCfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("linearSVCExample").getOrCreate()# Load training datatraining = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")lsvc = LinearSVC(maxIter=10, regParam=0.1)# Fit the modellsvcModel = lsvc.fit(training)# Print the coefficients and intercept for linearsSVCprint("Coefficients: " + str(lsvcModel.coefficients))print("Intercept: " + str(lsvcModel.intercept))spark.stop() output: 1234Coefficients:[0.0,0.0,......,-5.83656045253e-05,-0.000123781942165,-0.000117507049533,-6.19711523061e-05,-5.04200964581e-05,-0.000140552602236,-0.000141033094247,-0.000192723082389,-0.000480248996468]Intercept: 0.012911305214513969 Find full example code at “examples/src/main/python/ml/linearsvc.py” in the Spark repo. One-vs-Rest Classifier(又叫 One-vs-All)OneVsRest是一个将一个给定的二分类算法有效地扩展到多分类问题应用中的算法，也叫做“One-vs-All”算法。 OneVsRest是一个被实现为Estimator。它采用一个基础的Classifier然后对于k个类别分别创建二分类问题。类别i的二分类分类器用来预测类别为i还是不为i，即将i类和其他类别区分开来。最后，通过依次对k个二分类分类器进行评估，取置信最高的分类器的标签作为i类别的标签。 Examples 下面的示例演示了如何加载Iris数据集，将其解析为DataFrame并使用其执行多类别分类OneVsRest。计算测试误差以测量算法精度。1234567891011121314151617181920212223242526272829303132from pyspark.ml.classification import LogisticRegression, OneVsRestfrom pyspark.ml.evaluation import MulticlassClassificationEvaluatorfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("oneVsRestExample").getOrCreate()# load data file.inputData = spark.read.format("libsvm") \ .load("data/mllib/sample_multiclass_classification_data.txt")# generate the train/test split.(train, test) = inputData.randomSplit([0.8, 0.2])# instantiate the base classifier.lr = LogisticRegression(maxIter=10, tol=1E-6, fitIntercept=True)# instantiate the One Vs Rest Classifier.ovr = OneVsRest(classifier=lr)# train the multiclass model.ovrModel = ovr.fit(train)# score the model on test data.predictions = ovrModel.transform(test)# obtain evaluator.evaluator = MulticlassClassificationEvaluator(metricName="accuracy")# compute the classification error on test data.accuracy = evaluator.evaluate(predictions)print("Test Error = %g" % (1.0 - accuracy))spark.stop() output:12Test Error = 0.0625 Find full example code at “examples/src/main/python/ml/one_vs_rest_example.py” in the Spark repo. Naive Bayes朴素贝叶斯分类器是一个简单的基于贝叶斯定理与特征条件独立假设的概率分类器。spark.ml目前的实现支持多项式朴素贝叶斯和伯努利朴素贝叶斯。更多的信息可以在MLlib的Naive Bayes一节中找到。12345678910111213141516171819202122232425262728293031from pyspark.ml.classification import NaiveBayesfrom pyspark.ml.evaluation import MulticlassClassificationEvaluatorfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("NaiveBayesExample").getOrCreate()# Load training datadata = spark.read.format("libsvm") \ .load("data/mllib/sample_libsvm_data.txt")# Split the data into train and testsplits = data.randomSplit([0.6, 0.4], 1234)train = splits[0]test = splits[1]# create the trainer and set its parametersnb = NaiveBayes(smoothing=1.0, modelType="multinomial")# train the modelmodel = nb.fit(train)# select example rows to display.predictions = model.transform(test)predictions.show()# compute accuracy on the test setevaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")accuracy = evaluator.evaluate(predictions)print("Test set accuracy = " + str(accuracy))spark.stop() output:12345678910111213141516171819202122232425262728+-----+--------------------+--------------------+-----------+----------+|label| features| rawPrediction|probability|prediction|+-----+--------------------+--------------------+-----------+----------+| 0.0|(692,[95,96,97,12...|[-174115.98587057...| [1.0,0.0]| 0.0|| 0.0|(692,[98,99,100,1...|[-178402.52307196...| [1.0,0.0]| 0.0|| 0.0|(692,[100,101,102...|[-100905.88974016...| [1.0,0.0]| 0.0|| 0.0|(692,[123,124,125...|[-244784.29791241...| [1.0,0.0]| 0.0|| 0.0|(692,[123,124,125...|[-196900.88506109...| [1.0,0.0]| 0.0|| 0.0|(692,[124,125,126...|[-238164.45338794...| [1.0,0.0]| 0.0|| 0.0|(692,[124,125,126...|[-184206.87833381...| [1.0,0.0]| 0.0|| 0.0|(692,[127,128,129...|[-214174.52863813...| [1.0,0.0]| 0.0|| 0.0|(692,[127,128,129...|[-182844.62193963...| [1.0,0.0]| 0.0|| 0.0|(692,[128,129,130...|[-246557.10990301...| [1.0,0.0]| 0.0|| 0.0|(692,[152,153,154...|[-208282.08496711...| [1.0,0.0]| 0.0|| 0.0|(692,[152,153,154...|[-243457.69885665...| [1.0,0.0]| 0.0|| 0.0|(692,[153,154,155...|[-260933.50931276...| [1.0,0.0]| 0.0|| 0.0|(692,[154,155,156...|[-220274.72552901...| [1.0,0.0]| 0.0|| 0.0|(692,[181,182,183...|[-154830.07125175...| [1.0,0.0]| 0.0|| 1.0|(692,[99,100,101,...|[-145978.24563975...| [0.0,1.0]| 1.0|| 1.0|(692,[100,101,102...|[-147916.32657832...| [0.0,1.0]| 1.0|| 1.0|(692,[123,124,125...|[-139663.27471685...| [0.0,1.0]| 1.0|| 1.0|(692,[124,125,126...|[-129013.44238751...| [0.0,1.0]| 1.0|| 1.0|(692,[125,126,127...|[-81829.799906049...| [0.0,1.0]| 1.0|+-----+--------------------+--------------------+-----------+----------+only showing top 20 rowsTest set accuracy = 1.0 Find full example code at “examples/src/main/python/ml/naive_bayes_example.py” in the Spark repo. RegressionLinear regression用于处理线性回归模型和模型摘要的界面与逻辑回归情况类似。 When fitting LinearRegressionModel without intercept on dataset with constant nonzero column by “l-bfgs” solver, Spark MLlib outputs zero coefficients for constant nonzero columns. This behavior is the same as R glmnet but different from LIBSVM. Examples 下面的例子演示了训练弹性网络正则化线性回归模型和提取模型总结统计。1234567891011121314151617181920212223242526from pyspark.ml.regression import LinearRegressionfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("LinearRegressionExample").getOrCreate()# Load training datatraining = spark.read.format("libsvm")\ .load("data/mllib/sample_linear_regression_data.txt")lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)# Fit the modellrModel = lr.fit(training)# Print the coefficients and intercept for linear regressionprint("Coefficients: %s" % str(lrModel.coefficients))print("Intercept: %s" % str(lrModel.intercept))# Summarize the model over the training set and print out some metricstrainingSummary = lrModel.summaryprint("numIterations: %d" % trainingSummary.totalIterations)print("objectiveHistory: %s" % str(trainingSummary.objectiveHistory))trainingSummary.residuals.show()print("RMSE: %f" % trainingSummary.rootMeanSquaredError)print("r2: %f" % trainingSummary.r2)spark.stop() output:123456789101112131415161718192021222324252627282930313233Coefficients: [0.0,0.322925166774,-0.343854803456,1.91560170235,0.0528805868039,0.76596272046,0.0,-0.151053926692,-0.215879303609,0.220253691888]Intercept: 0.1598936844239736numIterations: 7objectiveHistory: [0.49999999999999994, 0.4967620357443381, 0.4936361664340463, 0.4936351537897608, 0.4936351214177871, 0.49363512062528014, 0.4936351206216114]+--------------------+| residuals|+--------------------+| -9.889232683103197|| 0.5533794340053554|| -5.204019455758823|| -20.566686715507508|| -9.4497405180564|| -6.909112502719486|| -10.00431602969873|| 2.062397807050484|| 3.1117508432954772|| -15.893608229419382|| -5.036284254673026|| 6.483215876994333|| 12.429497299109002|| -20.32003219007654|| -2.0049838218725005|| -17.867901734183793|| 7.646455887420495|| -2.2653482182417406||-0.10308920436195645|| -1.380034070385301|+--------------------+only showing top 20 rowsRMSE: 10.189077r2: 0.022861 Find full example code at “examples/src/main/python/ml/linear_regression_with_elastic_net.py” in the Spark repo. Generalized linear regression与线性回归假设输出服从高斯分布不同，广义线性模型（GLMs）指定线性模型的因变量服从指数分布。Spark的GeneralizedLinearRegression接口允许指定GLMs包括线性回归、泊松回归、逻辑回归等来处理多种预测问题。目前spark.ml仅支持指数型分布家族中的一部分类型，如下： Family Response Type Supported Links Gaussian(高斯) Continuous(连续) Identity*, Log, Inverse Binomial(二项) Binary(二进制) Logit*, Probit, CLogLog Poisson(泊松) Count(计数) Log*, Identity, Sqrt Gamma(伽马) Continuous(连续) Inverse*, Idenity, Log Tweedie Zero-inflated continuous(零膨胀连续) Power link function 注意：目前Spark在 GeneralizedLinearRegression仅支持最多4096个特征，如果特征超过4096个将会引发异常。对于线性回归和逻辑回归，如果模型特征数量会不断增长，则可通过 LinearRegression 和LogisticRegression来训练。 GLMs要求的指数型分布可以为正则或者自然形式。自然指数型分布为如下形式： Spark的GeneralizedLinearRegression接口提供汇总统计来诊断GLM模型的拟合程度，包括残差、p值、残差、Akaike信息准则及其它。 Examples 以下示例演示使用高斯响应和标识链接函数训练GLM并提取模型摘要统计信息。1234567891011121314151617181920212223242526272829303132from pyspark.ml.regression import GeneralizedLinearRegressionfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("GeneralizedLinearRegression").getOrCreate()# Load training datadataset = spark.read.format("libsvm")\ .load("data/mllib/sample_linear_regression_data.txt")glr = GeneralizedLinearRegression(family="gaussian", link="identity", maxIter=10, regParam=0.3)# Fit the modelmodel = glr.fit(dataset)# Print the coefficients and intercept for generalized linear regression modelprint("Coefficients: " + str(model.coefficients))print("Intercept: " + str(model.intercept))# Summarize the model over the training set and print out some metricssummary = model.summaryprint("Coefficient Standard Errors: " + str(summary.coefficientStandardErrors))print("T Values: " + str(summary.tValues))print("P Values: " + str(summary.pValues))print("Dispersion: " + str(summary.dispersion))print("Null Deviance: " + str(summary.nullDeviance))print("Residual Degree Of Freedom Null: " + str(summary.residualDegreeOfFreedomNull))print("Deviance: " + str(summary.deviance))print("Residual Degree Of Freedom: " + str(summary.residualDegreeOfFreedom))print("AIC: " + str(summary.aic))print("Deviance Residuals: ")summary.residuals().show()spark.stop() output:1234567891011121314151617181920212223242526272829303132333435363738Coefficients: [0.0105418280813,0.800325310056,-0.784516554142,2.36798871714,0.501000208986,1.12223511598,-0.292682439862,-0.498371743232,-0.603579718068,0.672555006719]Intercept: 0.14592176145232041Coefficient Standard Errors: [0.7950428434287478, 0.8049713176546897, 0.7975916824772489, 0.8312649247659919, 0.7945436200517938, 0.8118992572197593, 0.7919506385542777, 0.7973378214726764, 0.8300714999626418, 0.7771333489686802, 0.463930109648428]T Values: [0.013259446542269243, 0.9942283563442594, -0.9836067393599172, 2.848657084633759, 0.6305509179635714, 1.382234441029355, -0.3695715687490668, -0.6250446546128238, -0.7271418403049983, 0.8654306337661122, 0.31453393176593286]P Values: [0.989426199114056, 0.32060241580811044, 0.3257943227369877, 0.004575078538306521, 0.5286281628105467, 0.16752945248679119, 0.7118614002322872, 0.5322327097421431, 0.467486325282384, 0.3872259825794293, 0.753249430501097]Dispersion: 105.60988356821714Null Deviance: 53229.3654338832Residual Degree Of Freedom Null: 500Deviance: 51748.8429484264Residual Degree Of Freedom: 490AIC: 3769.1895871765314Deviance Residuals: +-------------------+| devianceResiduals|+-------------------+|-10.974359174246889|| 0.8872320138420559|| -4.596541837478908||-20.411667435019638||-10.270419345342642||-6.0156058956799905||-10.663939415849267|| 2.1153960525024713|| 3.9807132379137675||-17.225218272069533|| -4.611647633532147|| 6.4176669407698546|| 11.407137945300537|| -20.70176540467664|| -2.683748540510967||-16.755494794232536|| 8.154668342638725||-1.4355057987358848||-0.6435058688185704|| -1.13802589316832|+-------------------+only showing top 20 rows Find full example code at “examples/src/main/python/ml/generalized_linear_regression_example.py” in the Spark repo. Decision tree regression决策树以及其集成算法是机器学习分类和回归问题中非常流行的算法。因其易解释性、可处理类别特征、易扩展到多分类问题、不需特征缩放等性质被广泛使用。树集成算法如随机森林以及boosting算法几乎是解决分类和回归问题中表现最优的算法。 决策树是一个贪心算法递归地将特征空间划分为两个部分，在同一个叶子节点的数据最后会拥有同样的标签。每次划分通过贪心的以获得最大信息增益为目的，从可选择的分裂方式中选择最佳的分裂节点。节点不纯度有节点所含类别的同质性来衡量。工具提供为分类提供两种不纯度衡量（基尼不纯度和熵），为回归提供一种不纯度衡量（方差）。 spark.ml支持二分类、多分类以及回归的决策树算法，适用于连续特征以及类别特征。另外，对于分类问题，工具可以返回属于每种类别的概率（类别条件概率），对于回归问题工具可以返回预测在偏置样本上的方差。 Examples 以下示例以LibSVM格式加载数据集，将其分为训练集和测试集，在训练集训练，然后在测试集上进行评估。我们使用特征转换器为分类特征建立索引，并将元数据添加到DataFrame供决策树算法使用。1234567891011121314151617181920212223242526272829303132333435363738394041424344from pyspark.ml import Pipelinefrom pyspark.ml.regression import DecisionTreeRegressorfrom pyspark.ml.feature import VectorIndexerfrom pyspark.ml.evaluation import RegressionEvaluatorfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("DecisionTreeRegressionExample").getOrCreate()# Load the data stored in LIBSVM format as a DataFrame.data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")# Automatically identify categorical features, and index them.# We specify maxCategories so features with &gt; 4 distinct values are treated as continuous.featureIndexer =\ VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=4).fit(data)# Split the data into training and test sets (30% held out for testing)(trainingData, testData) = data.randomSplit([0.7, 0.3])# Train a DecisionTree model.dt = DecisionTreeRegressor(featuresCol="indexedFeatures")# Chain indexer and tree in a Pipelinepipeline = Pipeline(stages=[featureIndexer, dt])# Train model. This also runs the indexer.model = pipeline.fit(trainingData)# Make predictions.predictions = model.transform(testData)# Select example rows to display.predictions.select("prediction", "label", "features").show(5)# Select (prediction, true label) and compute test errorevaluator = RegressionEvaluator( labelCol="label", predictionCol="prediction", metricName="rmse")rmse = evaluator.evaluate(predictions)print("Root Mean Squared Error (RMSE) on test data = %g" % rmse)treeModel = model.stages[1]# summary onlyprint(treeModel)spark.stop() output:1234567891011121314+----------+-----+--------------------+|prediction|label| features|+----------+-----+--------------------+| 0.0| 0.0|(692,[122,123,124...|| 0.0| 0.0|(692,[123,124,125...|| 0.0| 0.0|(692,[124,125,126...|| 0.0| 0.0|(692,[124,125,126...|| 0.0| 0.0|(692,[124,125,126...|+----------+-----+--------------------+only showing top 5 rowsRoot Mean Squared Error (RMSE) on test data = 0.164399DecisionTreeRegressionModel (uid=DecisionTreeRegressor_415194352f1feffc1231) of depth 1 with 3 nodes Find full example code at “examples/src/main/python/ml/decision_tree_regression_example.py” in the Spark repo. Random forest regression随机森林是决策树的集成算法。随机森林包含多个决策树来降低过拟合的风险。随机森林同样具有易解释性、可处理类别特征、易扩展到多分类问题、不需特征缩放等性质。 随机森林分别训练一系列的决策树，所以训练过程是并行的。因算法中加入随机过程，所以每个决策树又有少量区别。通过合并每个树的预测结果来减少预测的方差，提高在测试集上的性能表现。 随机性体现：1.每次迭代时，对原始数据进行二次抽样来获得不同的训练数据。 2.对于每个树节点，考虑不同的随机特征子集来进行分裂。 除此之外，决策时的训练过程和单独决策树训练过程相同。 对新实例进行预测时，随机森林需要整合其各个决策树的预测结果。回归和分类问题的整合的方式略有不同。分类问题采取投票制，每个决策树投票给一个类别，获得最多投票的类别为最终结果。回归问题每个树得到的预测结果为实数，最终的预测结果为各个树预测结果的平均值。 spark.ml支持二分类、多分类以及回归的随机森林算法，适用于连续特征以及类别特征。 Examples 12345678910111213141516171819202122232425262728293031323334353637383940414243from pyspark.ml import Pipelinefrom pyspark.ml.regression import RandomForestRegressorfrom pyspark.ml.feature import VectorIndexerfrom pyspark.ml.evaluation import RegressionEvaluatorfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("RandomForestRegressionExample").getOrCreate()# Load and parse the data file, converting it to a DataFrame.data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")# Automatically identify categorical features, and index them.# Set maxCategories so features with &gt; 4 distinct values are treated as continuous.featureIndexer =\ VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=4).fit(data)# Split the data into training and test sets (30% held out for testing)(trainingData, testData) = data.randomSplit([0.7, 0.3])# Train a RandomForest model.rf = RandomForestRegressor(featuresCol="indexedFeatures")# Chain indexer and forest in a Pipelinepipeline = Pipeline(stages=[featureIndexer, rf])# Train model. This also runs the indexer.model = pipeline.fit(trainingData)# Make predictions.predictions = model.transform(testData)# Select example rows to display.predictions.select("prediction", "label", "features").show(5)# Select (prediction, true label) and compute test errorevaluator = RegressionEvaluator( labelCol="label", predictionCol="prediction", metricName="rmse")rmse = evaluator.evaluate(predictions)print("Root Mean Squared Error (RMSE) on test data = %g" % rmse)rfModel = model.stages[1]print(rfModel) # summary onlyspark.stop() output:1234567891011121314+----------+-----+--------------------+|prediction|label| features|+----------+-----+--------------------+| 0.0| 0.0|(692,[95,96,97,12...|| 0.0| 0.0|(692,[124,125,126...|| 0.0| 0.0|(692,[126,127,128...|| 0.0| 0.0|(692,[126,127,128...|| 0.0| 0.0|(692,[126,127,128...|+----------+-----+--------------------+only showing top 5 rowsRoot Mean Squared Error (RMSE) on test data = 0.193434RandomForestRegressionModel (uid=RandomForestRegressor_4bfa98dd14412263de8e) with 20 trees Find full example code at “examples/src/main/python/ml/random_forest_regressor_example.py” in the Spark repo. Gradient-boosted tree regressionGradient-boosted tree(GBTs)梯度提升树是一种决策树的集成算法。它通过反复迭代训练决策树来最小化损失函数。与决策树类似，梯度提升树具有可处理类别特征、易扩展到多分类问题、不需特征缩放等性质。Spark.ml通过使用现有decision tree工具来实现。 梯度提升树依次迭代训练一系列的决策树。在一次迭代中，算法使用现有的集成来对每个训练实例的类别进行预测，然后将预测结果与真实的标签值进行比较。通过重新标记，来赋予预测结果不好的实例更高的权重。所以，在下次迭代中，决策树会对先前的错误进行修正。 对实例标签进行重新标记的机制由损失函数来指定。每次迭代过程中，梯度迭代树在训练数据上进一步减少损失函数的值。spark.ml为分类问题提供一种损失函数（Log Loss），为回归问题提供两种损失函数（平方误差与绝对误差）。 Spark.ml支持二分类以及回归的随机森林算法，适用于连续特征以及类别特征。 注意：梯度提升树目前不支持多分类问题。 Examples 注意：对于这个示例数据集，GBTRegressor实际上只需要1次迭代，但通常情况并非如此。12345678910111213141516171819202122232425262728293031323334353637383940414243from pyspark.ml import Pipelinefrom pyspark.ml.regression import GBTRegressorfrom pyspark.ml.feature import VectorIndexerfrom pyspark.ml.evaluation import RegressionEvaluatorfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("GBTRegressionExample").getOrCreate()# Load and parse the data file, converting it to a DataFrame.data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")# Automatically identify categorical features, and index them.# Set maxCategories so features with &gt; 4 distinct values are treated as continuous.featureIndexer =\ VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=4).fit(data)# Split the data into training and test sets (30% held out for testing)(trainingData, testData) = data.randomSplit([0.7, 0.3])# Train a GBT model.gbt = GBTRegressor(featuresCol="indexedFeatures", maxIter=10)# Chain indexer and GBT in a Pipelinepipeline = Pipeline(stages=[featureIndexer, gbt])# Train model. This also runs the indexer.model = pipeline.fit(trainingData)# Make predictions.predictions = model.transform(testData)# Select example rows to display.predictions.select("prediction", "label", "features").show(5)# Select (prediction, true label) and compute test errorevaluator = RegressionEvaluator( labelCol="label", predictionCol="prediction", metricName="rmse")rmse = evaluator.evaluate(predictions)print("Root Mean Squared Error (RMSE) on test data = %g" % rmse)gbtModel = model.stages[1]print(gbtModel) # summary onlyspark.stop() output:1234567891011121314+----------+-----+--------------------+|prediction|label| features|+----------+-----+--------------------+| 0.0| 0.0|(692,[100,101,102...|| 0.0| 0.0|(692,[121,122,123...|| 0.0| 0.0|(692,[122,123,124...|| 0.0| 0.0|(692,[123,124,125...|| 0.0| 0.0|(692,[124,125,126...|+----------+-----+--------------------+only showing top 5 rowsRoot Mean Squared Error (RMSE) on test data = 0.288675GBTRegressionModel (uid=GBTRegressor_4e5384caecb49745ae29) with 10 trees Find full example code at “examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py” in the Spark repo. Survival regression在spark.ml中，我们实施Acceleratedfailure time(加速失效时间模型)，对于截尾数据它是一个参数化生存回归的模型。它描述了一个有对数生存时间的模型，所以它也常被称为生存分析的对数线性模型。与比例危险模型不同，因AFT模型中每个实例对目标函数的贡献是独立的，其更容易并行化。 给定协变量的值x，对于可能的右截尾的随机生存时间，AFT模型下的似然函数如下： 可以证明AFT模型是一个凸优化问题，即是说找到凸函数ι(β,σ)的最小值取决于系数向量β以及尺度参数σ的对数.其中实现的优化算法为L-BFGS，该实现与R的生存函数survreg的结果相匹配。 当使用无拦截的连续非零列训练AFTSurvivalRegressionModel时，Spark MLlib为连续非零列输出零系数。这种处理与R中的生存函数survreg不同。 Examples 123456789101112131415161718192021222324from pyspark.ml.regression import AFTSurvivalRegressionfrom pyspark.ml.linalg import Vectorsfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("SurvivalRegressionExample").getOrCreate()training = spark.createDataFrame([ (1.218, 1.0, Vectors.dense(1.560, -0.605)), (2.949, 0.0, Vectors.dense(0.346, 2.158)), (3.627, 0.0, Vectors.dense(1.380, 0.231)), (0.273, 1.0, Vectors.dense(0.520, 1.151)), (4.199, 0.0, Vectors.dense(0.795, -0.226))], ["label", "censor", "features"])quantileProbabilities = [0.3, 0.6]aft = AFTSurvivalRegression(quantileProbabilities=quantileProbabilities, quantilesCol="quantiles")model = aft.fit(training)# Print the coefficients, intercept and scale parameter for AFT survival regressionprint("Coefficients: " + str(model.coefficients))print("Intercept: " + str(model.intercept))print("Scale: " + str(model.scale))model.transform(training).show(truncate=False)spark.stop() output:12345678910111213Coefficients: [-0.496304411053,0.198452172529]Intercept: 2.638089896305637Scale: 1.5472363533632303+-----+------+--------------+------------------+---------------------------------------+|label|censor|features |prediction |quantiles |+-----+------+--------------+------------------+---------------------------------------+|1.218|1.0 |[1.56,-0.605] |5.718985621018948 |[1.160322990805951,4.99546058340675] ||2.949|0.0 |[0.346,2.158] |18.07678210850554 |[3.6675919944963185,15.789837303662035]||3.627|0.0 |[1.38,0.231] |7.381908879359957 |[1.4977129086101564,6.448002719505488] ||0.273|1.0 |[0.52,1.151] |13.577717814884515|[2.754778414791514,11.859962351993207] ||4.199|0.0 |[0.795,-0.226]|9.013087597344821 |[1.82866218773319,7.8728164067854935] |+-----+------+--------------+------------------+---------------------------------------+ Find full example code at “examples/src/main/python/ml/aft_survival_regression.py” in the Spark repo. Isotonic regression保序回归是回归算法的一种。保序回归给定一个有限的实数集合(Y=y1,y2,…,yn)代表观察到的响应,以及(X=x1,x2,…,xn)代表未知的响应值，训练一个模型来最小化下列方程：f(x) = ∑ωi(yi-xi)²,其中wi为权重是正值，其结果方程称为保序回归，而且其解是唯一的。它可以被视为有顺序约束下的最小二乘法问题。实际上保序回归在拟合原始数据点时是一个单调函数。我们实现池旁者算法，它使用并行保序回归。训练数据是DataFrame格式，包含标签、特征值以及权重三列。另外保序算法还有一个参数名为isotonic，其默认值为真，它指定保序回归为保序（单调递增）或者反序（单调递减）。 训练返回一个保序回归模型，可以被用于来预测已知或者未知特征值的标签。保序回归的结果是分段线性函数，预测规则如下： 如果预测输入与训练中的特征值完全匹配，则返回相应标签。如果一个特征值对应多个预测标签值，则返回其中一个，具体是哪一个未指定。 如果预测输入比训练中的特征值都高（或者都低），则相应返回最高特征值或者最低特征值对应标签。如果一个特征值对应多个预测标签值，则相应返回最高值或者最低值。 如果预测输入落入两个特征值之间，则预测将会是一个分段线性函数，其值由两个最近的特征值的预测值计算得到。如果一个特征值对应多个预测标签值，则使用上述两种情况中的处理方式解决。 Examples 有关API的更多详细信息，请参阅IsotonicRegressionPython文档。1234567891011121314151617from pyspark.ml.regression import IsotonicRegressionfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("IsotonicRegressionExample").getOrCreate()# Loads data.dataset = spark.read.format("libsvm")\ .load("data/mllib/sample_isotonic_regression_libsvm_data.txt")# Trains an isotonic regression model.model = IsotonicRegression().fit(dataset)print("Boundaries in increasing order: %s\n" % str(model.boundaries))print("Predictions associated with the boundaries: %s\n" % str(model.predictions))# Makes predictions.model.transform(dataset).show()spark.stop() output: 123456789101112131415161718192021222324252627282930Boundaries in increasing order: [0.01,0.17,0.18,0.27,0.28,0.29,0.3,0.31,0.34,0.35,0.36,0.41,0.42,0.71,0.72,0.74,0.75,0.76,0.77,0.78,0.79,0.8,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89,1.0]Predictions associated with the boundaries: [0.157152712941,0.157152712941,0.189138196,0.189138196,0.20040796,0.29576747,0.43396226,0.5081591025,0.5081591025,0.54156043,0.550484446667,0.550484446667,0.563929967,0.563929967,0.566037736667,0.566037736667,0.56603774,0.57929628,0.64762876,0.66241713,0.67210607,0.67210607,0.674655785,0.674655785,0.73890872,0.73992861,0.84242733,0.89673636,0.89673636,0.90719021,0.9272055075,0.9272055075]+----------+--------------+-------------------+| label| features| prediction|+----------+--------------+-------------------+|0.24579296|(1,[0],[0.01])|0.15715271294117644||0.28505864|(1,[0],[0.02])|0.15715271294117644||0.31208567|(1,[0],[0.03])|0.15715271294117644||0.35900051|(1,[0],[0.04])|0.15715271294117644||0.35747068|(1,[0],[0.05])|0.15715271294117644||0.16675166|(1,[0],[0.06])|0.15715271294117644||0.17491076|(1,[0],[0.07])|0.15715271294117644|| 0.0418154|(1,[0],[0.08])|0.15715271294117644||0.04793473|(1,[0],[0.09])|0.15715271294117644||0.03926568| (1,[0],[0.1])|0.15715271294117644||0.12952575|(1,[0],[0.11])|0.15715271294117644|| 0.0|(1,[0],[0.12])|0.15715271294117644||0.01376849|(1,[0],[0.13])|0.15715271294117644||0.13105558|(1,[0],[0.14])|0.15715271294117644||0.08873024|(1,[0],[0.15])|0.15715271294117644||0.12595614|(1,[0],[0.16])|0.15715271294117644||0.15247323|(1,[0],[0.17])|0.15715271294117644||0.25956145|(1,[0],[0.18])| 0.189138196||0.20040796|(1,[0],[0.19])| 0.189138196||0.19581846| (1,[0],[0.2])| 0.189138196|+----------+--------------+-------------------+only showing top 20 rows Find full example code at “examples/src/main/python/ml/isotonic_regression_example.py” in the Spark repo. 更多请查阅spark.ml-classification-regression 结束]]></content>
      <categories>
        <category>Spark</category>
        <category>MLlib</category>
      </categories>
      <tags>
        <tag>逻辑回归</tag>
        <tag>决策树</tag>
        <tag>GBDT</tag>
        <tag>Naive Bayes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkMLlib-Working-with-Features]]></title>
    <url>%2F2018%2F01%2F21%2Fsparkmllib-working-with-features%2F</url>
    <content type="text"><![CDATA[Extracting, Transforming and Selecting features(特征的提取、转换、选择) 本节介绍用于处理特征的算法，大致分为以下几组： Extraction(提取)：从“原始”数据中提取特征 Transformation(转换)：缩放，转换或修改特征 Selection(选择)：从一大组特征集中选择一个子集 Locality Sensitive Hashing局部敏感散列（LSH）：这类算法将特征变换的各个方面与其他算法相结合。 Feature ExtractorsTF-IDF词频-逆向文档频率（TF-IDF） 是一种在文本挖掘中广泛使用的特征矢量化方法，用于反映在预料中词语在文档中的重要性。t表示一个词，d表示文档和D表示语料。词频TF(t,d)是词语t在文档d中出现的次数，而文档频率DF(t, D)是语料中文档包含词语t的的数量。如果我们只用词频来衡量重要性，那么很容易过分强调出现频率很高但是却只承载文档极少的信息的词语，例如“a”，“the”和“of”。如果一个词语经常在整个语料库中出现，这意味着对于特定的文档它并没有承载特殊的信息。逆文档频率是一个词语提供多少信息的数字度量： 1IDF(t,D) = log(|D|+1)/(DF(t, D)+1) 其中|D|是语料的文档总数。由于使用对数，所以如果一个词语在所有文档中出现，则其IDF值为0.请注意，smoothing term用来避免除以零对于那些在语料外的词语。TF-IDF度量是TF和IDF的乘积： 1TFIDF(t,d,D)=TF(t,d)⋅IDF(t,D) 词频和文档频率的定义有几个变体。在MLlib中，我们将TF和IDF分开，使它们更灵活。 TF：HashingTF与CountVectorizer都可用于生成词频向量。 HashingTF是一个Transformer，其选取词语的集合并将这些集合转换成固定长度的特征向量。在文本处理中，“set of term”可能是一堆文字。 HashingTF利用哈希技巧(hashing trick)。通过应用散列函数(hash function)将原始特征映射成一个索引（term）。这里使用的哈希函数是MurmurHash 3 。然后根据映射后的索引计算词频。这种方法避免了计算全局term-to-index的映射，而这些计算对于大型语料库来说可能是耗费的，但是它具有潜在的散列冲突，其中不同的原始特征可能在散列之后变成相同的term。为了减少碰撞的几率，我们可以增加目标特征维数，即散列表的桶数(buckets)。由于使用简单的模来将散列函数转换为列索引，所以建议使用2的幂作为特征维度，否则特征将不会均匀地映射到列。默认的特征维度是2^18=262144。一个可选的二进制切换参数控制词频计数。当设置为真时，所有非零频率计数都被设置为1.这对于模拟二进制计数而不是整数计数的离散概率模型特别有用。 CountVectorizer将文本文档转换为词条计数的向量。有关更多详细信息，请参阅CountVectorizer 。 IDF：IDF是一个Estimator,被应用于一个数据集并产生一个IDFModel。IDFModel选取特征向量（通常从HashingTF或CountVectorizer创建）并缩放每一列。直观地，它减少了频繁出现在语料库中的列的权重。 Note： spark.ml不提供用于文本分割的工具。我们推荐用户参考Stanford NLP Grou 和 scalanlp/chalk。 Examples 在下面的代码段中，我们从一组句子开始。我们使用Tokenizer每个句子分成单词。对于每个句子（词包），我们使用HashingTF把语句散列成一个特征向量。我们用IDF来重新调整特征向量; 使用文本作为特征时，这通常会提高性能。我们的特征向量可以传递给学习算法。 有关API的更多详细信息，请参阅HashingTF Python文档和IDF Python文档。 1234567891011121314151617181920212223from pyspark.ml.feature import HashingTF, IDF, Tokenizerfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("TF_IDfExample").getOrCreate()sentenceData = spark.createDataFrame([ (0.0, "Hi I heard about Spark"), (0.0, "I wish Java could use case classes"), (1.0, "Logistic regression models are neat")], ["label", "sentence"])tokenizer = Tokenizer(inputCol="sentence", outputCol="words")wordsData = tokenizer.transform(sentenceData)hashingTF = HashingTF(inputCol="words", outputCol="rawFeatures", numFeatures=20)featurizedData = hashingTF.transform(wordsData)# alternatively, CountVectorizer can also be used to get term frequency vectorsidf = IDF(inputCol="rawFeatures", outputCol="features")idfModel = idf.fit(featurizedData)rescaledData = idfModel.transform(featurizedData)rescaledData.select("label", "features").show()spark.stop() output: 12345|label| features||-----|--------------------|| 0.0|(20,[0,5,9,17],[0...|| 0.0|(20,[2,7,9,13,15]...|| 1.0|(20,[4,6,13,15,18...| Find full example code at “examples/src/main/python/ml/tf_idf_example.py” in the Spark repo. Word2VecWord2Vec是一个Estimator,它选取表征文件的单词序列(句子)来训练一个Word2VecModel。模型将每个单词映射到一个唯一的固定大小的向量vector。Word2VecModel 用所有单词在文档中的平均值将每个文档转换为一个向量vector; 然后这个vector可以用作预测，文档相似度计算等功能。请参阅Word2Vec MLlib用户指南了解更多详细信息。 Examples 在下面的代码段中，我们从一组文档开始，每个文档都被表示为一个单词序列。对于每个文档，我们把它转换成一个特征向量。这个特征向量可以传递给一个学习算法。 有关API的更多详细信息，请参阅Word2Vec Python文档。 1234567891011121314151617181920from pyspark.ml.feature import Word2Vecfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("Word2VecExample").getOrCreate()# Input data: Each row is a bag of words from a sentence or document.documentDF = spark.createDataFrame([ ("Hi I heard about Spark".split(" "), ), ("I wish Java could use case classes".split(" "), ), ("Logistic regression models are neat".split(" "), )], ["text"])# Learn a mapping from words to Vectors.word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol="text", outputCol="result")model = word2Vec.fit(documentDF)result = model.transform(documentDF)for row in result.collect(): text, vector = row print("Text: [%s] =&gt; \nVector: %s\n" % (", ".join(text), str(vector)))spark.stop() output: 123456789Text: [Hi, I, heard, about, Spark] =&gt; Vector: [0.0334007266909,0.00213784053922,-0.00239131785929]Text: [I, wish, Java, could, use, case, classes] =&gt; Vector: [0.0464252099129,0.0357359477452,-0.000244158453175]Text: [Logistic, regression, models, are, neat] =&gt; Vector: [-0.00983053222299,0.0668786892667,-0.0307074898912]--- Find full example code at “examples/src/main/python/ml/word2vec_example.py” in the Spark repo. CountVectorizer12345678910111213在拟合过程中，CountVectorizer将选择整个语料库中词频排在前面vocabSize个的词汇。一个可选参数minDF也会影响拟合过程，方法是指定词汇必须出现的文档的最小数量（或小于1.0）。另一个可选的二进制切换参数控制输出向量。如果设置为true，则所有非零计数都设置为1.这对于模拟二进制计数而不是整数计数的离散概率模型特别有用。- Examples假设我们有列如下数据帧，有id和texts列：```python id | texts----|---------- 0 | Array(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) 1 | Array(&quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;) texts列中的每一行都是一个Array [String]类型的文档。调用CountVectorizer产生CountVectorizerModel与词汇（a，b，c）。然后转换后的输出列“向量”包含： 12345 id | texts | vector----|---------------------------------|--------------- 0 | Array("a", "b", "c") | (3,[0,1,2],[1.0,1.0,1.0]) 1 | Array("a", "b", "b", "c", "a") | (3,[0,1,2],[2.0,2.0,1.0]) 每个向量表示文档在词汇表上的标记计数。 有关API的更多详细信息，请参阅CountVectorizer Python文档 和CountVectorizerModel Python文档。 12345678910111213141516171819from pyspark.ml.feature import CountVectorizerfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("CountVectorizerExample").getOrCreate()# Input data: Each row is a bag of words with a ID.df = spark.createDataFrame([ (0, "a b c".split(" ")), (1, "a b b c a".split(" "))], ["id", "words"])# fit a CountVectorizerModel from the corpus.cv = CountVectorizer(inputCol="words", outputCol="features", vocabSize=3, minDF=2.0)model = cv.fit(df)result = model.transform(df)result.show(truncate=False)spark.stop() output: 1234567+---+---------------+-------------------------+|id |words |features |+---+---------------+-------------------------+|0 |[a, b, c] |(3,[0,1,2],[1.0,1.0,1.0])||1 |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|+---+---------------+-------------------------+ Find full example code at “examples/src/main/python/ml/count_vectorizer_example.py” in the Spark repo. Feature TransformersTokenizerTokenization(分词)是将文本（如句子）分解成单个词（通常是单词）的过程。一个简单的Tokenizer类提供了这个功能。下面的例子展示了如何将句子拆分成单词序列。 RegexTokenizer允许更高级的基于正则表达式（正则表达式）匹配的分词。默认情况下，使用参数“pattern”（正则表达式，默认：”\s+”）作为分隔符来分割输入文本。或者，用户可以将参数“gaps”设置为false，指示正则表达式“pattern”而不是分割间隙来表示“tokens”，并查找所有匹配事件作为分词结果。 Examples 有关API的更多详细信息，请参阅Tokenizer Python文档和RegexTokenizer Python文档。 12345678910111213141516171819202122232425262728from pyspark.ml.feature import Tokenizer, RegexTokenizerfrom pyspark.sql.functions import col, udffrom pyspark.sql.types import IntegerTypefrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("TokenizerExample").getOrCreate()sentenceDataFrame = spark.createDataFrame([ (0, "Hi I heard about Spark"), (1, "I wish Java could use case classes"), (2, "Logistic,regression,models,are,neat")], ["id", "sentence"])tokenizer = Tokenizer(inputCol="sentence", outputCol="words")regexTokenizer = RegexTokenizer(inputCol="sentence", outputCol="words", pattern="\\W")# alternatively, pattern="\\w+", gaps(False)countTokens = udf(lambda words: len(words), IntegerType())tokenized = tokenizer.transform(sentenceDataFrame)tokenized.select("sentence", "words")\ .withColumn("tokens", countTokens(col("words"))).show(truncate=False)regexTokenized = regexTokenizer.transform(sentenceDataFrame)regexTokenized.select("sentence", "words") \ .withColumn("tokens", countTokens(col("words"))).show(truncate=False)spark.stop() output:12345678910111213141516+-----------------------------------+------------------------------------------+------+|sentence |words |tokens|+-----------------------------------+------------------------------------------+------+|Hi I heard about Spark |[hi, i, heard, about, spark] |5 ||I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7 ||Logistic,regression,models,are,neat|[logistic,regression,models,are,neat] |1 |+-----------------------------------+------------------------------------------+------++-----------------------------------+------------------------------------------+------+|sentence |words |tokens|+-----------------------------------+------------------------------------------+------+|Hi I heard about Spark |[hi, i, heard, about, spark] |5 ||I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7 ||Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5 |+-----------------------------------+------------------------------------------+------+ Find full example code at “examples/src/main/python/ml/tokenizer_example.py” in the Spark repo. StopWordsRemoverStop words(停止词)是应该从输入中排除的词，通常是因为这些词经常出现而又不具有如此多的含义。 StopWordsRemover将一串字符串（例如一个Tokenizer的输出）作为输入，并从输入序列中删除所有的停止词。停用词表由stopWords参数指定。某些语言的默认停用词可通过调用访问StopWordsRemover.loadDefaultStopWords(language)，可用的选项有“danish”, “dutch”, “english”, “finnish”, “french”, “german”, “hungarian”, “italian”, “norwegian”, “portuguese”, “russian”, “spanish”, “swedish” and “turkish”。布尔参数caseSensitive指示匹配是否区分大小写（默认为false）。 Examples 假设我们有列如下数据帧,拥有列id和raw：12345 id | raw----|---------- 0 | [I, saw, the, red, baloon] 1 | [Mary, had, a, little, lamb] 应用StopWordsRemover与raw作为输入列，filtered作为输出列，我们应该得到以下：12345 id | raw | filtered----|-----------------------------|-------------------- 0 | [I, saw, the, red, baloon] | [saw, red, baloon] 1 | [Mary, had, a, little, lamb]|[Mary, little, lamb] 在这里filtered，“I”，“the”，“had”和“a”这些停用词语已被滤除。\有关API的更多详细信息，请参阅StopWordsRemover Python文档。12345678910from pyspark.ml.feature import StopWordsRemoversentenceData = spark.createDataFrame([ (0, ["I", "saw", "the", "red", "balloon"]), (1, ["Mary", "had", "a", "little", "lamb"])], ["id", "raw"])remover = StopWordsRemover(inputCol="raw", outputCol="filtered")remover.transform(sentenceData).show(truncate=False) Find full example code at “examples/src/main/python/ml/stopwords_remover_example.py” in the Spark repo. n-gram一个n-gram是一个包含整数n个tokens（通常是单词）的序列。NGram类可用于输入特征转变成n-grams。 NGram将一串字符串（例如一个Tokenizer的输出）作为输入。参数n用于确定每个n-gram中的terms的数量。输出将由n-grams的序列组成，每个n-gram由空格分隔的n个连续的words的字符串表示。如果输入序列少于n，则没有输出。 Examples 有关API的更多细节，请参阅NGram Python文档。123456789101112131415from pyspark.ml.feature import NGramfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("n_gramExample").getOrCreate()wordDataFrame = spark.createDataFrame([ (0, ["Hi", "I", "heard", "about", "Spark"]), (1, ["I", "wish", "Java", "could", "use", "case", "classes"]), (2, ["Logistic", "regression", "models", "are", "neat"])], ["id", "words"])ngram = NGram(n=2, inputCol="words", outputCol="ngrams")ngramDataFrame = ngram.transform(wordDataFrame)ngramDataFrame.select("ngrams").show(truncate=False)spark.stop() output:12345678+------------------------------------------------------------------+|ngrams |+------------------------------------------------------------------+|[Hi I, I heard, heard about, about Spark] ||[I wish, wish Java, Java could, could use, use case, case classes]||[Logistic regression, regression models, models are, are neat] |+------------------------------------------------------------------+ Find full example code at “examples/src/main/python/ml/n_gram_example.py” in the Spark repo. BinarizerBinarization(二值化)是将数字特征阈值化为二进制（0/1）特征的过程。 Binarizer需传入参数inputCol和outputCol，以及所述threshold参数来进行二值化。大于阈值的特征值被二进制化为1.0; 等于或小于阈值的值被二值化为0.0。inputCol支持Vector和Double类型。 Examples 有关API的更多细节，请参阅Binarizer Python文档。123456789101112131415161718from pyspark.ml.feature import Binarizerfrom pyspark.sql import SparkSession spark = SparkSession.builder.appName("BinarizerExample").getOrCreate()continuousDataFrame = spark.createDataFrame([ (0, 0.1), (1, 0.8), (2, 0.2)], ["id", "feature"])binarizer = Binarizer(threshold=0.5, inputCol="feature", outputCol="binarized_feature")binarizedDataFrame = binarizer.transform(continuousDataFrame)print("Binarizer output with Threshold = %f" % binarizer.getThreshold())binarizedDataFrame.show()spark.stop() output:123456789Binarizer output with Threshold = 0.500000+---+-------+-----------------+| id|feature|binarized_feature|+---+-------+-----------------+| 0| 0.1| 0.0|| 1| 0.8| 1.0|| 2| 0.2| 0.0|+---+-------+-----------------+ Find full example code at “examples/src/main/python/ml/binarizer_example.py” in the Spark repo. PCAPCA是一个统计过程，它使用正交变换将一组可能相关的变量的观察值转换成一组称为主成分的线性不相关变量的值。一个PCA类使用PCA将向量映射到低维空间来训练一个模型。下面的例子显示了如何将五维特征向量投影到三维主成分中。 Examples 有关API的更多细节，请参阅PCA Python文档。12345678910111213141516from pyspark.ml.feature import PCAfrom pyspark.ml.linalg import Vectorsfrom pyspark.sql import SparkSession spark = SparkSession.builder.appName("PCA_Example").getOrCreate()data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),), (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),), (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]df = spark.createDataFrame(data, ["features"])pca = PCA(k=3, inputCol="features", outputCol="pcaFeatures")model = pca.fit(df)result = model.transform(df).select("pcaFeatures")result.show(truncate=False)spark.stop() output:12345678+-----------------------------------------------------------+|pcaFeatures |+-----------------------------------------------------------+|[1.6485728230883807,-4.013282700516296,-5.524543751369388] ||[-4.645104331781534,-1.1167972663619026,-5.524543751369387]||[-6.428880535676489,-5.337951427775355,-5.524543751369389] |+-----------------------------------------------------------+ Find full example code at “examples/src/main/python/ml/pca_example.py” in the Spark repo. PolynomialExpansionPolynomial expansion(多项式展开)是将特征扩展到一个多项式空间的过程，这个多项式空间是由原始维度的n-degree组合形成的。一个PolynomialExpansion类提供此功能。下面的例子展示了如何将特征扩展到一个三次多项式空间。 Examples 有关API的更多详细信息，请参阅PolynomialExpansion Python文档。1234567891011121314151617from pyspark.ml.feature import PolynomialExpansionfrom pyspark.ml.linalg import Vectorsfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("PolynormialExpansionExample").getOrCreate()df = spark.createDataFrame([ (Vectors.dense([2.0, 1.0]),), (Vectors.dense([0.0, 0.0]),), (Vectors.dense([3.0, -1.0]),)], ["features"])polyExpansion = PolynomialExpansion(degree=3, inputCol="features", outputCol="polyFeatures")polyDF = polyExpansion.transform(df)polyDF.show(truncate=False)spark.stop() output:12345678+----------+------------------------------------------+|features |polyFeatures |+----------+------------------------------------------+|[2.0,1.0] |[2.0,4.0,8.0,1.0,2.0,4.0,1.0,2.0,1.0] ||[0.0,0.0] |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0] ||[3.0,-1.0]|[3.0,9.0,27.0,-1.0,-3.0,-9.0,1.0,3.0,-1.0]|+----------+------------------------------------------+ Find full example code at “examples/src/main/python/ml/polynomial_expansion_example.py” in the Spark repo. Discrete Cosine Transform(DCT)Discrete Cosine Transform离散余弦变换将时域中的长度为N的实数序列转换为另一个频域中长度为N的实数序列。一个DCT类提供此功能，实现 DCT-II 和通过缩放结果1/sqrt(2)倍使得变换的表示矩阵是单一的。被应用于变换的序列是无偏移的（例如变换的序列的第0th个元素是 第0th 个DCT系数而不是N/2个）。 Examples 有关API的更多详细信息，请参阅DCT Python文档。12345678910111213141516from pyspark.ml.feature import DCTfrom pyspark.ml.linalg import Vectorsfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("DCT_Example").getOrCreate()df = spark.createDataFrame([ (Vectors.dense([0.0, 1.0, -2.0, 3.0]),), (Vectors.dense([-1.0, 2.0, 4.0, -7.0]),), (Vectors.dense([14.0, -2.0, -5.0, 1.0]),)], ["features"])dct = DCT(inverse=False, inputCol="features", outputCol="featuresDCT")dctDf = dct.transform(df)dctDf.select("featuresDCT").show(truncate=False)spark.stop() output:12345678+----------------------------------------------------------------+|featuresDCT |+----------------------------------------------------------------+|[1.0,-1.1480502970952693,2.0000000000000004,-2.7716385975338604]||[-1.0,3.378492794482933,-7.000000000000001,2.9301512653149677] ||[4.0,9.304453421915744,11.000000000000002,1.5579302036357163] |+----------------------------------------------------------------+ Find full example code at “examples/src/main/python/ml/dct_example.py” in the Spark repo. StringIndexerStringIndexer将一串字符串标签编码为标签索引。这些索引范围为[0, numLabels)按照标签频率排序，因此最频繁的标签获得索引0。对于unseen的标签如果用户选择保留它们，它们将被放在索引numLabels处。如果输入列是数字，我们将其转换为字符串值并将其索引。当下游管道组件（例如Estimator或 Transformer）使用此字符串索引标签时，必须将组件的输入列设置为此字符串索引列名称。在许多情况下，您可以使用setInputCol设置输入列 123456789101112from pyspark.ml.feature import StringIndexerfrom pyspark.sql import SparkSession spark = SparkSession.builder.appName("StringIndexerExample").getOrCreate()df = spark.createDataFrame( [(0, "a"), (1, "b"), (2, "c"), (3, "a"), (4, "a"), (5, "c")], ["id", "category"])indexer = StringIndexer(inputCol="category", outputCol="categoryIndex")indexed = indexer.fit(df).transform(df)indexed.show()spark.stop() output:1234567891011+---+--------+-------------+| id|category|categoryIndex|+---+--------+-------------+| 0| a| 0.0|| 1| b| 2.0|| 2| c| 1.0|| 3| a| 0.0|| 4| a| 0.0|| 5| c| 1.0|+---+--------+-------------+ 此外，StringIndexer处理看不见的标签还有三个策略： 抛出一个异常(默认的) 完全跳过unseen标签的行 把一个unseen的标签放在一个特殊的额外桶里，在索引numLabels处 让我们回到之前的例子：12345678 id | category----|---------- 0 | a 1 | b 2 | c 3 | d 4 | e 如果你没有设置如何StringIndexer处理看不见的标签或将其设置为“错误”，则会抛出异常。但是，如果您已经调用setHandleInvalid(“skip”)，则会生成以下数据集：123456 id | category | categoryIndex----|----------|--------------- 0 | a | 0.0 1 | b | 2.0 2 | c | 1.0 请注意，包含“d”或“e”的行不显示。 如果你调用setHandleInvalid(“keep”)，将生成以下数据集：123456789 id | category | categoryIndex----|----------|--------------- 0 | a | 0.0 1 | b | 2.0 2 | c | 1.0 3 | d | 3.0 4 | e | 3.0 # d,e 所在的被映射到索引“3.0” Find full example code at “examples/src/main/python/ml/string_indexer_example.py” in the Spark repo. IndexToString对应于StringIndexer，IndexToString将一列标签索引映射回包含作为字符串的原始标签的列。一个常见的用例是从StringIndexer标签生成索引，用这些索引对模型进行训练，并从预测IndexToString索引列中检索原始标签。然而，你也可以提供自己的标签。 Examples 构造tringIndexer例子，假设我们有一个如下的数据帧，其有id和categoryIndex列：123456789 id | categoryIndex----|--------------- 0 | 0.0 1 | 2.0 2 | 1.0 3 | 0.0 4 | 0.0 5 | 1.0 将categoryIndex作为输入列，应用IndexToString， originalCategory作为输出列，我们能够检索我们的原始标签（他们将从列的元数据推断）： 有关API的更多详细信息，请参阅IndexToString Python文档。1234567891011121314151617181920212223242526from pyspark.ml.feature import IndexToString, StringIndexerfrom pyspark.sql import SparkSession spark = SparkSession.builder.appName("IndexToStringExample").getOrCreate()df = spark.createDataFrame( [(0, "a"), (1, "b"), (2, "c"), (3, "a"), (4, "a"), (5, "c")], ["id", "category"])indexer = StringIndexer(inputCol="category", outputCol="categoryIndex")model = indexer.fit(df)indexed = model.transform(df)print("Transformed string column '%s' to indexed column '%s'" % (indexer.getInputCol(), indexer.getOutputCol()))indexed.show()print("StringIndexer will store labels in output column metadata\n")converter = IndexToString(inputCol="categoryIndex", outputCol="originalCategory")converted = converter.transform(indexed)print("Transformed indexed column '%s' back to original string column '%s' using " "labels in metadata" % (converter.getInputCol(), converter.getOutputCol()))converted.select("id", "categoryIndex", "originalCategory").show()spark.stop() output:1234567891011121314151617181920212223242526Transformed string column 'category' to indexed column 'categoryIndex'+---+--------+-------------+| id|category|categoryIndex|+---+--------+-------------+| 0| a| 0.0|| 1| b| 2.0|| 2| c| 1.0|| 3| a| 0.0|| 4| a| 0.0|| 5| c| 1.0|+---+--------+-------------+StringIndexer will store labels in output column metadataTransformed indexed column 'categoryIndex' back to original string column 'originalCategory' using labels in metadata+---+-------------+----------------+| id|categoryIndex|originalCategory|+---+-------------+----------------+| 0| 0.0| a|| 1| 2.0| b|| 2| 1.0| c|| 3| 0.0| a|| 4| 0.0| a|| 5| 1.0| c|+---+-------------+----------------+ Find full example code at “examples/src/main/python/ml/index_to_string_example.py” in the Spark repo. OneHotEncodingOne-hot encoding将一列标签索引映射到一列二进制向量，其中最多只有一个one-value。该编码允许那些期望使用连续特征的算法（例如Logistic回归）使用分类特征。 Examples 关于 API的更多细节请参考OneHotEncoder Python文档。123456789101112131415161718192021from pyspark.ml.feature import OneHotEncoder, StringIndexerfrom pyspark.sql import SparkSession spark = SparkSession.builder.appName("OneHotEncoderExample").getOrCreate()df = spark.createDataFrame([ (0, "a"), (1, "b"), (2, "c"), (3, "a"), (4, "a"), (5, "c")], ["id", "category"])stringIndexer = StringIndexer(inputCol="category", outputCol="categoryIndex")model = stringIndexer.fit(df)indexed = model.transform(df)encoder = OneHotEncoder(inputCol="categoryIndex", outputCol="categoryVec")encoded = encoder.transform(indexed)encoded.show()spark.stop() output:1234567891011+---+--------+-------------+-------------+| id|category|categoryIndex| categoryVec|+---+--------+-------------+-------------+| 0| a| 0.0|(2,[0],[1.0])|| 1| b| 2.0| (2,[],[])|| 2| c| 1.0|(2,[1],[1.0])|| 3| a| 0.0|(2,[0],[1.0])|| 4| a| 0.0|(2,[0],[1.0])|| 5| c| 1.0|(2,[1],[1.0])|+---+--------+-------------+-------------+ Find full example code at “examples/src/main/python/ml/onehot_encoder_example.py” in the Spark repo. VectorIndexerVectorIndexer有助于索引Vectors的数据集中的分类特征。它可以自动决定哪些特征是分类的，并将原始值转换为分类索引。具体来说，它做了以下几点： 取一个Vector类型的输入列和一个参数maxCategories。 根据不同值的数量确定哪些特征应该分类，这些特征最多被分为maxCategories类。 计算每个分类特征的分类索引(0-based)。 索引分类特征并将原始特征值转换为索引。 索引分类特征允许Decision Trees(决策树)和Tree Ensembles等算法适当地处理分类特征，提高性能。 Examples在下面的例子中，我们读入一个标记点​​的数据集，然后用VectorIndexer来决定哪些特征应该被视为分类特征。我们将分类特征值转换为它们的索引。这个转换的数据然后可以被传递给诸如DecisionTreeRegressor处理分类特征的算法。 请参阅VectorIndexer Python文档 以获取有关API的更多详细信息。123456789101112131415161718from pyspark.ml.feature import VectorIndexerfrom pyspark.sql import SparkSession spark = SparkSession.builder.appName("VectorIndexerExample").getOrCreate()data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")indexer = VectorIndexer(inputCol="features", outputCol="indexed", maxCategories=10)indexerModel = indexer.fit(data)categoricalFeatures = indexerModel.categoryMapsprint("Chose %d categorical features: %s" % (len(categoricalFeatures), ", ".join(str(k) for k in categoricalFeatures.keys())))# Create new column "indexed" with categorical values transformed to indicesindexedData = indexerModel.transform(data)indexedData.show()spark.stop() output:123456789101112131415161718192021222324252627Chose 351 categorical features: 645, 69, 365, 138, 101, 479, 333, 249, 0, 555, 666, 88, 170, 115, 276, 308, 5, 449, 120, 247, 614, 677, 202, 10, 56, 533, 142, 500, 340, 670, 174, 42, 417, 24, 37, 25, 257, 389, 52, 14, 504, 110, 587, 619, 196, 559, 638, 20, 421, 46, 93, 284, 228, 448, 57, 78, 29, 475, 164, 591, 646, 253, 106, 121, 84, 480, 147, 280, 61, 221, 396, 89, 133, 116, 1, 507, 312, 74, 307, 452, 6, 248, 60, 117, 678, 529, 85, 201, 220, 366, 534, 102, 334, 28, 38, 561, 392, 70, 424, 192, 21, 137, 165, 33, 92, 229, 252, 197, 361, 65, 97, 665, 583, 285, 224, 650, 615, 9, 53, 169, 593, 141, 610, 420, 109, 256, 225, 339, 77, 193, 669, 476, 642, 637, 590, 679, 96, 393, 647, 173, 13, 41, 503, 134, 73, 105, 2, 508, 311, 558, 674, 530, 586, 618, 166, 32, 34, 148, 45, 161, 279, 64, 689, 17, 149, 584, 562, 176, 423, 191, 22, 44, 59, 118, 281, 27, 641, 71, 391, 12, 445, 54, 313, 611, 144, 49, 335, 86, 672, 172, 113, 681, 219, 419, 81, 230, 362, 451, 76, 7, 39, 649, 98, 616, 477, 367, 535, 103, 140, 621, 91, 66, 251, 668, 198, 108, 278, 223, 394, 306, 135, 563, 226, 3, 505, 80, 167, 35, 473, 675, 589, 162, 531, 680, 255, 648, 112, 617, 194, 145, 48, 557, 690, 63, 640, 18, 282, 95, 310, 50, 67, 199, 673, 16, 585, 502, 338, 643, 31, 336, 613, 11, 72, 175, 446, 612, 143, 43, 250, 231, 450, 99, 363, 556, 87, 203, 671, 688, 104, 368, 588, 40, 304, 26, 258, 390, 55, 114, 171, 139, 418, 23, 8, 75, 119, 58, 667, 478, 536, 82, 620, 447, 36, 168, 146, 30, 51, 190, 19, 422, 564, 305, 107, 4, 136, 506, 79, 195, 474, 664, 532, 94, 283, 395, 332, 528, 644, 47, 15, 163, 200, 68, 62, 277, 691, 501, 90, 111, 254, 227, 337, 122, 83, 309, 560, 639, 676, 222, 592, 364, 100+-----+--------------------+--------------------+|label| features| indexed|+-----+--------------------+--------------------+| 0.0|(692,[127,128,129...|(692,[127,128,129...|| 1.0|(692,[158,159,160...|(692,[158,159,160...|| 1.0|(692,[124,125,126...|(692,[124,125,126...|| 1.0|(692,[152,153,154...|(692,[152,153,154...|| 1.0|(692,[151,152,153...|(692,[151,152,153...|| 0.0|(692,[129,130,131...|(692,[129,130,131...|| 1.0|(692,[158,159,160...|(692,[158,159,160...|| 1.0|(692,[99,100,101,...|(692,[99,100,101,...|| 0.0|(692,[154,155,156...|(692,[154,155,156...|| 0.0|(692,[127,128,129...|(692,[127,128,129...|| 1.0|(692,[154,155,156...|(692,[154,155,156...|| 0.0|(692,[153,154,155...|(692,[153,154,155...|| 0.0|(692,[151,152,153...|(692,[151,152,153...|| 1.0|(692,[129,130,131...|(692,[129,130,131...|| 0.0|(692,[154,155,156...|(692,[154,155,156...|| 1.0|(692,[150,151,152...|(692,[150,151,152...|| 0.0|(692,[124,125,126...|(692,[124,125,126...|| 0.0|(692,[152,153,154...|(692,[152,153,154...|| 1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|| 1.0|(692,[124,125,126...|(692,[124,125,126...|+-----+--------------------+--------------------+only showing top 20 rows Find full example code at “examples/src/main/python/ml/vector_indexer_example.py” in the Spark repo. InteractionInteraction是一个Transformer,采用向量或双值列的方法生成一个单一的向量列，其中包含每个输入列的一个值的所有组合的乘积。 例如，如果您有两个向量类型列，每个列都有三个维度作为输入列，那么您将获得一个9维向量作为输出列。 Examples 假设我们有以下DataFrame,有列“id1”，“vec1”和“vec2”：123456789id1|vec1 |vec2---|--------------|--------------1 |[1.0,2.0,3.0] |[8.0,4.0,5.0]2 |[4.0,3.0,8.0] |[7.0,9.0,8.0]3 |[6.0,1.0,9.0] |[2.0,3.0,6.0]4 |[10.0,8.0,6.0]|[9.0,4.0,5.0]5 |[9.0,2.0,7.0] |[10.0,7.0,3.0]6 |[1.0,1.0,4.0] |[2.0,8.0,4.0] 应用Interaction作用于这些输入列，然后interactedCol输出列包含：123456789id1|vec1 |vec2 |interactedCol---|--------------|--------------|------------------------------------------------------1 |[1.0,2.0,3.0] |[8.0,4.0,5.0] |[8.0,4.0,5.0,16.0,8.0,10.0,24.0,12.0,15.0]2 |[4.0,3.0,8.0] |[7.0,9.0,8.0] |[56.0,72.0,64.0,42.0,54.0,48.0,112.0,144.0,128.0]3 |[6.0,1.0,9.0] |[2.0,3.0,6.0] |[36.0,54.0,108.0,6.0,9.0,18.0,54.0,81.0,162.0]4 |[10.0,8.0,6.0]|[9.0,4.0,5.0] |[360.0,160.0,200.0,288.0,128.0,160.0,216.0,96.0,120.0]5 |[9.0,2.0,7.0] |[10.0,7.0,3.0]|[450.0,315.0,135.0,100.0,70.0,30.0,350.0,245.0,105.0]6 |[1.0,1.0,4.0] |[2.0,8.0,4.0] |[12.0,48.0,24.0,12.0,48.0,24.0,48.0,192.0,96.0] 注：该方法暂时并没有python的实现，有scala和Java的 NormalizerNormalizer是一个Transformer，它转换数据集的Vector行，规范化每个Vector为unit norm。它采用参数p来规范化，它指定用于规范化的p范数。（默认p = 2 ）。这种规范化可以帮助标准化您的输入数据，并改善学习算法的行为。 Examples 以下示例演示如何以libsvm格式加载数据集，然后将每行标准化为unit L^1 norm1和unitL^∞ norm。 有关API的更多详细信息，请参阅Normalizer Python文档。1234567891011121314151617181920212223from pyspark.ml.feature import Normalizerfrom pyspark.ml.linalg import Vectorsfrom pyspark.sql import SparkSession spark = SparkSession.builder.appName("NormalizerExample").getOrCreate()dataFrame = spark.createDataFrame([ (0, Vectors.dense([1.0, 0.5, -1.0]),), (1, Vectors.dense([2.0, 1.0, 1.0]),), (2, Vectors.dense([4.0, 10.0, 2.0]),)], ["id", "features"])# Normalize each Vector using $L^1$ norm.normalizer = Normalizer(inputCol="features", outputCol="normFeatures", p=1.0)l1NormData = normalizer.transform(dataFrame)print("Normalized using L^1 norm")l1NormData.show()# Normalize each Vector using $L^\infty$ norm.lInfNormData = normalizer.transform(dataFrame, &#123;normalizer.p: float("inf")&#125;)print("Normalized using L^inf norm")lInfNormData.show()spark.stop() output:123456789101112131415161718Normalized using L^1 norm+---+--------------+------------------+| id| features| normFeatures|+---+--------------+------------------+| 0|[1.0,0.5,-1.0]| [0.4,0.2,-0.4]|| 1| [2.0,1.0,1.0]| [0.5,0.25,0.25]|| 2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|+---+--------------+------------------+Normalized using L^inf norm+---+--------------+--------------+| id| features| normFeatures|+---+--------------+--------------+| 0|[1.0,0.5,-1.0]|[1.0,0.5,-1.0]|| 1| [2.0,1.0,1.0]| [1.0,0.5,0.5]|| 2|[4.0,10.0,2.0]| [0.4,1.0,0.2]|+---+--------------+--------------+ Find full example code at “examples/src/main/python/ml/normalizer_example.py” in the Spark repo. StandardScalerStandardScaler转换Vector行的数据集，将每个特征归一化为具有单位标准偏差和/或零均值。它需要参数： withStd：默认为true。将数据缩放到单位标准偏差。 withMean：默认为False。在缩放之前将数据集中在平均值上。它会建立一个密集的输出，所以在应用于稀疏输入时要小心。 StandardScaler是一个Estimator，可以fit在一个数据集上产生一个StandardScalerModel; 这相当于计算汇总统计。然后该模型可以转换Vector数据集中的列以具有单位标准偏差和/或零均值特征。 请注意，如果某个要素的标准偏差为零，则会在该特征的Vector中返回默认值0.0。 Example 以下示例演示如何加载数据集，然后将每个特征标准化为单位标准偏差。 有关API的更多详细信息，请参阅StandardScaler Python文档。1234567891011121314151617181920from pyspark.ml.feature import StandardScalerfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("StandardScalerExample").getOrCreate()dataFrame = spark.createDataFrame([ (0, Vectors.dense([1.0, 0.5, -1.0]),), (1, Vectors.dense([2.0, 1.0, 1.0]),), (2, Vectors.dense([4.0, 10.0, 2.0]),)], ["id", "features"])scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures", withStd=True, withMean=False)# Compute summary statistics by fitting the StandardScalerscalerModel = scaler.fit(dataFrame)# Normalize each feature to have unit standard deviation.scaledData = scalerModel.transform(dataFrame)scaledData.show(truncate=False)spark.stop() output:12345678+---+--------------+------------------------------------------------------------+|id |features |scaledFeatures |+---+--------------+------------------------------------------------------------+|0 |[1.0,0.5,-1.0]|[0.6546536707079772,0.09352195295828244,-0.6546536707079771]||1 |[2.0,1.0,1.0] |[1.3093073414159544,0.1870439059165649,0.6546536707079771] ||2 |[4.0,10.0,2.0]|[2.618614682831909,1.870439059165649,1.3093073414159542] |+---+--------------+------------------------------------------------------------+ Find full example code at “examples/src/main/python/ml/standard_scaler_example.py” in the Spark repo. MinMaxScalerMinMaxScaler转换Vector行数据集，将每个特征重新缩放到特定范围（通常为[0，1]）。它需要参数： min：默认为0.0。转换后的下界，被所有特征共享。max：默认为1.0。变换后的上界，被所有的特征共享。MinMaxScaler计算数据集的汇总统计并生成一个MinMaxScalerModel。然后模型可以单独转换每个特征，使其在给定的范围内。 特征E的重新缩放的值被计算为，Rescaled(ei) = (ei − Emin) / (Emax − Emin) ∗ (max − min) + min对于Emax==Emin的情况Rescaled(ei)=0.5∗(max+min) 请注意，由于零值可能会被转换为非零值，所以transofromer的输出将会是DenseVector，即使输入是稀疏输入。 Examples 有关API的更多详细信息，请参阅MinMaxScaler Python文档 和MinMaxScalerModel Python文档。123456789101112131415161718192021from pyspark.ml.feature import MinMaxScalerfrom pyspark.ml.linalg import Vectorsfrom pyspark.sql import SparkSession spark = SparkSession.builder.appName("MinMaxScalerExample").getOrCreate()dataFrame = spark.createDataFrame([ (0, Vectors.dense([1.0, 0.1, -1.0]),), (1, Vectors.dense([2.0, 1.1, 1.0]),), (2, Vectors.dense([3.0, 10.1, 3.0]),)], ["id", "features"])scaler = MinMaxScaler(inputCol="features", outputCol="scaledFeatures")# Compute summary statistics and generate MinMaxScalerModelscalerModel = scaler.fit(dataFrame)# rescale each feature to range [min, max].scaledData = scalerModel.transform(dataFrame)print("Features scaled to range: [%f, %f]" % (scaler.getMin(), scaler.getMax()))scaledData.select("features", "scaledFeatures").show()spark.stop() output:123456789Features scaled to range: [0.000000, 1.000000]+--------------+--------------+| features|scaledFeatures|+--------------+--------------+|[1.0,0.1,-1.0]| [0.0,0.0,0.0]|| [2.0,1.1,1.0]| [0.5,0.1,0.5]||[3.0,10.1,3.0]| [1.0,1.0,1.0]|+--------------+--------------+ Find full example code at “examples/src/main/python/ml/min_max_scaler_example.py” in the Spark repo. MaxAbsScalerMaxAbsScaler转换Vector行的数据集，通过分割每个特征的最大绝对值来重新缩放每个特征到范围[-1,1]。它不会移动/居中数据，因此不会破坏任何稀疏性。 MaxAbsScaler计算数据集的汇总统计并生成一个MaxAbsScalerModel。该模型可以将每个特征分别转换为范围[-1,1]。 Examples 有关API的更多详细信息，请参阅MaxAbsScaler Python文档 和MaxAbsScalerModel Python文档。12345678910111213141516171819202122from pyspark.ml.feature import MaxAbsScalerfrom pyspark.ml.linalg import Vectorsfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("MaxAbsScalerExample").getOrCreate()dataFrame = spark.createDataFrame([ (0, Vectors.dense([1.0, 0.1, -8.0]),), (1, Vectors.dense([2.0, 1.0, -4.0]),), (2, Vectors.dense([4.0, 10.0, 8.0]),)], ["id", "features"])scaler = MaxAbsScaler(inputCol="features", outputCol="scaledFeatures")# Compute summary statistics and generate MaxAbsScalerModelscalerModel = scaler.fit(dataFrame)# rescale each feature to range [-1, 1].scaledData = scalerModel.transform(dataFrame)scaledData.select("features", "scaledFeatures").show()spark.stop() output:12345678+--------------+----------------+| features| scaledFeatures|+--------------+----------------+|[1.0,0.1,-8.0]|[0.25,0.01,-1.0]||[2.0,1.0,-4.0]| [0.5,0.1,-0.5]||[4.0,10.0,8.0]| [1.0,1.0,1.0]|+--------------+----------------+ Find full example code at “examples/src/main/python/ml/max_abs_scaler_example.py” in the Spark repo. BucketizerBucketizer将一列连续的特征转换成特征桶列，其中桶由用户指定。它需要一个参数： splits：用于将连续特征映射到存储桶的参数。n个buckets有n+1个splits。由分割x，y定义的bucket值范围为[x,y)不包含y,而只有最后一个bucket包含y。splits应是严格增加的。必须明确提供inf的值以涵盖所有Double值; 否则，指定splits之外的值将被视为错误。两个splits的例子是Array(Double.NegativeInfinity, 0.0, 1.0, Double.PositiveInfinity)和Array(0.0, 1.0, 2.0)。 请注意，如果您不知道目标列的上限和下限，则应该添加Double.NegativeInfinity并Double.PositiveInfinity作为分割的界限，以防止出现Bucketizer界限异常。 还要注意，你提供的splits必须严格按照递增顺序，即s0 &lt; s1 &lt; s2 &lt; … &lt; sn。 更多细节可以在Bucketizer的API文档中找到。 Examples 以下示例演示了如何将一列Doubles转换为另一个索引表列123456789101112131415161718from pyspark.ml.feature import Bucketizerfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("BucketizerExample").getOrCreate()splits = [-float("inf"), -0.5, 0.0, 0.5, float("inf")]data = [(-999.9,), (-0.5,), (-0.3,), (0.0,), (0.2,), (999.9,)]dataFrame = spark.createDataFrame(data, ["features"])bucketizer = Bucketizer(splits=splits, inputCol="features", outputCol="bucketedFeatures")# Transform original data into its bucket index.bucketedData = bucketizer.transform(dataFrame)print("Bucketizer output with %d buckets" % (len(bucketizer.getSplits())-1))bucketedData.show()spark.stop() output:123456789101112Bucketizer output with 4 buckets+--------+----------------+|features|bucketedFeatures|+--------+----------------+| -999.9| 0.0|| -0.5| 1.0|| -0.3| 1.0|| 0.0| 2.0|| 0.2| 2.0|| 999.9| 3.0|+--------+----------------+ Find full example code at “examples/src/main/python/ml/bucketizer_example.py” in the Spark repo. ElementwiseProductElementwiseProduct将每个输入矢量使用元素乘法乘以一个提供的“权重”矢量。换句话说，它通过标量乘数来缩放数据集的每一列。这表示输入向量v和变换向量w之间的Hadamard product(哈达玛积)，得到结果向量。(v1…vN).T 。(w1…WN).T = (v1w1…vNwN).T Examples 下面的这个例子演示了如何使用变换向量值来变换向量。有关API的更多详细信息，请参阅ElementwiseProduct Python文档。1234567891011121314from pyspark.ml.feature import ElementwiseProductfrom pyspark.ml.linalg import Vectorsfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("ElementwiseProductExample").getOrCreate()# Create some vector data; also works for sparse vectorsdata = [(Vectors.dense([1.0, 2.0, 3.0]),), (Vectors.dense([4.0, 5.0, 6.0]),)]df = spark.createDataFrame(data, ["vector"])transformer = ElementwiseProduct(scalingVec=Vectors.dense([0.0, 1.0, 2.0]), inputCol="vector", outputCol="transformedVector")# Batch transform the vectors to create new column:transformer.transform(df).show()spark.stop() output:1234567+-------------+-----------------+| vector|transformedVector|+-------------+-----------------+|[1.0,2.0,3.0]| [0.0,2.0,6.0]||[4.0,5.0,6.0]| [0.0,5.0,12.0]|+-------------+-----------------+ Find full example code at “examples/src/main/python/ml/elementwise_product_example.py” in the Spark repo. SQLTransformerSQLTransformer实现由SQL语句定义的转换。目前我们只支持一下SQL语法：”SELECT … FROM THIS …” where， “THIS“代表输入数据集的基础表。select子句指定要在输出中显示的字段，常量和表达式，并且可以是Spark SQL支持的任何select子句。用户还可以使用Spark SQL内置函数和UDF对这些选定的列进行操作。例如，SQLTransformer支持像这样的语句： SELECT a, a + b AS a_b FROM THIS SELECT a, SQRT(b) AS b_sqrt FROM THIS where a &gt; 5 SELECT a, b, SUM(c) AS c_sum FROM THIS GROUP BY a, b Examples 有关该API的更多详细信息，请参阅SQLTransformer Python文档。12345678910111213from pyspark.ml.feature import SQLTransformerfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("SQLTransformerExample").getOrCreate()df = spark.createDataFrame([ (0, 1.0, 3.0), (2, 2.0, 5.0)], ["id", "v1", "v2"])sqlTrans = SQLTransformer( statement="SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__")sqlTrans.transform(df).show()spark.stop() output:1234567+---+---+---+---+----+| id| v1| v2| v3| v4|+---+---+---+---+----+| 0|1.0|3.0|4.0| 3.0|| 2|2.0|5.0|7.0|10.0|+---+---+---+---+----+ Find full example code at “examples/src/main/python/ml/sql_transformer.py” in the Spark repo. VectorAssemblerVectorAssembler是一个将给定的列列表组合成单个向量列的transoformer。对于将原始特征和由不同特征变换器生成的特征组合成一个特征向量，以便训练诸如逻辑回归和决策树等ML模型是有用的。 VectorAssembler接受以下输入列类型：所有数字类型，布尔类型和向量类型。在每一行中，输入列的值将按照指定的顺序连接成一个向量。 Examples 请参阅VectorAssembler Python文档 以获取有关API的更多详细信息。123456789101112131415161718from pyspark.ml.linalg import Vectorsfrom pyspark.ml.feature import VectorAssemblerfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("VectorAssemblerExample").getOrCreate()dataset = spark.createDataFrame( [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)], ["id", "hour", "mobile", "userFeatures", "clicked"])assembler = VectorAssembler( inputCols=["hour", "mobile", "userFeatures"], outputCol="features")output = assembler.transform(dataset)print("Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'")output.select("features", "clicked").show(truncate=False)spark.stop() output:1234567Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'+-----------------------+-------+|features |clicked|+-----------------------+-------+|[18.0,1.0,0.0,10.0,0.5]|1.0 |+-----------------------+-------+ Find full example code at “examples/src/main/python/ml/vector_assembler_example.py” in the Spark repo. QuantileDiscretizerQuantileDiscretizer将带有连续特征的列转换成具有分类分类特征的列。bins的数量通过numBuckets参数设置。如果输入的独特值不足以创建足够多的分位数，则所用桶的数量可能会小于此值。 NaN values：NaN值将在QuantileDiscretizer拟合过程中从列中移除。这将产生一个Bucketizer预测模型。在转换期间，当在数据集中发现NaN值时Bucketizer会引发错误，但是用户也可以通过设置handleInvalid来选择保留或删除数据集中的NaN值。如果用户选择保留NaN值，他们将被专门处理，并放入他们自己的bucket中，例如，如果使用4个bucket，那么非NaN数据将被放入bucket[0-3]，但是NaN将是算在一个特殊的bucket[4]里。 Algorithm：使用近似算法（有关详细说明，请参阅approxQuantile文档 ）来选择bin的范围。近似的精度可以用relativeError参数来控制 。设置为零时，计算确切的分位数（注意：精确计算分位数是一个耗费的操作）。下部和上部bin边界会是-Infinity和+Infinity以来涵盖所有实数值。 Examples 请参阅QuantileDiscretizer Python文档 以获取有关API的更多详细信息。12345678910111213from pyspark.ml.feature import QuantileDiscretizerfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("QuantileDiscretizerExample").getOrCreate()data = [(0, 18.0), (1, 19.0), (2, 8.0), (3, 5.0), (4, 2.2)]df = spark.createDataFrame(data, ["id", "hour"])discretizer = QuantileDiscretizer(numBuckets=3, inputCol="hour", outputCol="result")result = discretizer.fit(df).transform(df)result.show()spark.stop() output:12345678910+---+----+------+| id|hour|result|+---+----+------+| 0|18.0| 2.0|| 1|19.0| 2.0|| 2| 8.0| 1.0|| 3| 5.0| 1.0|| 4| 2.2| 0.0|+---+----+------+ Find full example code at “examples/src/main/python/ml/quantile_discretizer_example.py” in the Spark repo. ImputerImputer transformer使用平均值或位于列的中位数填充数据集中缺少的值。输入列应该是 DoubleType或FloatType。目前Imputer不支持分类特征，并可能为包含分类特征的列创建不正确的值。 注意：输入列中的所有null值都被视为缺失，所以也被归类。 Examples 有关API的更多详细信息，请参阅Imputer Python文档。123456789101112131415161718from pyspark.ml.feature import Imputerfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("ImputerExample").getOrCreate()df = spark.createDataFrame([ (1.0, float("nan")), (2.0, float("nan")), (float("nan"), 3.0), (4.0, 4.0), (5.0, 5.0)], ["a", "b"])imputer = Imputer(inputCols=["a", "b"], outputCols=["out_a", "out_b"])model = imputer.fit(df)model.transform(df).show()spark.stop() output:12345678910+---+---+-----+-----+| a| b|out_a|out_b|+---+---+-----+-----+|1.0|NaN| 1.0| 4.0||2.0|NaN| 2.0| 4.0||NaN|3.0| 3.0| 3.0||4.0|4.0| 4.0| 4.0||5.0|5.0| 5.0| 5.0|+---+---+-----+-----+ Find full example code at “examples/src/main/python/ml/imputer_example.py” in the Spark repo. Feature SelectorsVectorSlicerVectorSlicer是一个transformer，它将一个特征向量转换成一个新的具有原始特征的sub-array的特征向量。这对从向量列中提取特征很有用。 VectorSlicer接受一个具有指定索引的向量列，然后输出一个新的向量列，其值通过这些索引来选择。有两种类型的索引： setIndices()：代表向量中索引的整数索引。 setNames()：代表向量中特征名称的字符串索引。 这需要vector列有一个AttributeGroup，因为实现得匹配Attribute名称字段。 整数和字符串的规范都是可以接受的。而且，您可以同时使用整数索引和字符串名称。必须至少选择一个特征，不允许重复的特征，所以选择的索引和名称之间就没有重叠。请注意，如果选择了特征的名称，遇到空的输入属性时将会抛出异常。 输出向量将首先按照选定的索引（按给定的顺序）排序，然后是选定的名称（按给定的顺序）。 Examples 有关API的更多详细信息，请参阅VectorSlicer Python文档。1234567891011121314151617from pyspark.ml.feature import VectorSlicerfrom pyspark.ml.linalg import Vectorsfrom pyspark.sql.types import Rowfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("VectorSlicerExample").getOrCreate()df = spark.createDataFrame([ Row(userFeatures=Vectors.sparse(3, &#123;0: -2.0, 1: 2.3&#125;)), Row(userFeatures=Vectors.dense([-2.0, 2.3, 0.0]))])slicer = VectorSlicer(inputCol="userFeatures", outputCol="features", indices=[1])output = slicer.transform(df)output.select("userFeatures", "features").show()spark.stop() output:1234567+--------------------+-------------+| userFeatures| features|+--------------------+-------------+|(3,[0,1],[-2.0,2.3])|(1,[0],[2.3])|| [-2.0,2.3,0.0]| [2.3]|+--------------------+-------------+ Find full example code at “examples/src/main/python/ml/vector_slicer_example.py” in the Spark repo. RFormulaRFormula选择由Rmodel formula指定的列。目前我们支持R操作符的有限子集，包括’〜’，’。’，’：’，’+’和’ - ‘。其基本的操作符是： ‘~’: 分开 target和terms ‘+’: 连接terms，“+ 0”表示删除intercept ‘-‘: 删除一个term，“ - 1”表示删除intercept ‘:’: interaction（数值相乘或二元化分类值） ‘.’: 除target以外的所有列 假设a和都是b是double类型的列，我们使用以下简单的例子来说明RFormula的作用： y ~ a + b意味着模型y ~ w0 + w1 a + w2 b，其中w0是截距intercept，w1, w2是系数coefficients。\y ~ a + b + a:b - 1装置模型y ~ w1 a + w2 b + w3 a b，其中w1, w2, w3为系数。\RFormula产生特征的一个向量列的和一个double类型列或标签的字符串类型列。就像在R中使用公式进行线性回归时一样，字符串输入列将被进行one-hot编码，而数字列将被转换为doule类型。如果标签列是字符串类型的，它将首先被转换为StringIndexer的double类型。如果DataFrame中不存在标签列，则将使用公式中指定的结果变量创建输出标签列。 Examples 有关API的更多详细信息，请参阅RFormula Python文档。12345678910111213141516171819from pyspark.ml.feature import RFormulafrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("RFormulaExample").getOrCreate()dataset = spark.createDataFrame( [(7, "US", 18, 1.0), (8, "CA", 12, 0.0), (9, "NZ", 15, 0.0)], ["id", "country", "hour", "clicked"])formula = RFormula( formula="clicked ~ country + hour", featuresCol="features", labelCol="label")output = formula.fit(dataset).transform(dataset)output.select("features", "label").show()spark.stop() output:12345678+--------------+-----+| features|label|+--------------+-----+|[0.0,0.0,18.0]| 1.0||[1.0,0.0,12.0]| 0.0||[0.0,1.0,15.0]| 0.0|+--------------+-----+ Find full example code at “examples/src/main/python/ml/rformula_example.py” in the Spark repo. ChiSqSelectorChiSqSelector代表Chi-Squared特征选择。它作用于具有分类特征的已标记数据。ChiSqSelector使用Chi-Squared test of independence来决定选择哪些特征。它支持五种选择方法：numTopFeatures，percentile，fpr，fdr，fwe： numTopFeatures选择一个根据卡方检验得到的固定的数目前几个特征，这类似于产生具有最大预测能力的特征。 percentile类似于numTopFeatures，但只选择所有特征的一部分，而不是固定的数目。 fpr选择p值低于阈值的所有特征，从而控制选择的误报率。 fdr使用Benjamini-Hochbergprocedure选择false discovery rate低于阈值的所有特征。* fwe选择p值低于阈值的所有特征,阈值由1 / numFeatures缩放，从而控制选择的family-wise error rate。默认选择方法是numTopFeatures，top特征的默认数量设置为50.用户可以使用setSelectorType选择方法。 Examples 有关API的更多详细信息，请参阅ChiSqSelector Python文档。12345678910111213141516171819from pyspark.ml.feature import ChiSqSelectorfrom pyspark.ml.linalg import Vectorsfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("ChiSqSelectorExample").getOrCreate()df = spark.createDataFrame([ (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,), (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,), (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], ["id", "features", "clicked"])selector = ChiSqSelector(numTopFeatures=1, featuresCol="features", outputCol="selectedFeatures", labelCol="clicked")result = selector.fit(df).transform(df)print("ChiSqSelector output with top %d features selected" % selector.getNumTopFeatures())result.show()spark.stop() output:123456789ChiSqSelector output with top 1 features selected+---+------------------+-------+----------------+| id| features|clicked|selectedFeatures|+---+------------------+-------+----------------+| 7|[0.0,0.0,18.0,1.0]| 1.0| [18.0]|| 8|[0.0,1.0,12.0,0.0]| 0.0| [12.0]|| 9|[1.0,0.0,15.0,0.1]| 0.0| [15.0]|+---+------------------+-------+----------------+ Find full example code at “examples/src/main/python/ml/chisq_selector_example.py” in the Spark repo. Locality Sensitive HashingLocality Sensitive Hashing (LSH)是一类重要的散列技术，常用于聚类，近似最近邻搜索和大数据集异常值检测。 LSH的总体思路是使用一系列函数（“LSH families”）将数据点散列到桶中，使得彼此靠近的数据点高概率地出现在同一个桶中，而彼此相距很远的数据点很有可能在不同的桶里。LSH family正式定义如下。 在一个度量空间中(M, d)中，M一个集合，d是一个基于M的距离函数，LSH族是满足下列性质的函数族h：12345存在p,q属于M，d(p,q) &lt;= r1 =&gt; Pr(h(p)) = h(q) &gt;= p1d(p,1) &gt;= r2 =&gt; Pr(h(p)) = h(q) &lt;= p2 则这个LSH族被称为 (r1, r2, p1, p2)-敏感的。 在Spark中，不同的LSH families在不同的类中实现（例如MinHash），并且在每个类中提供用于feature transformation(特征变换)，approximate simility join(最近似连接)和approximate nearest neighbor最近邻的APIs。 在LSH中，我们将false positive定义为一对散列到同一个桶中远距输入特征(with d(p,q)≥r2)，并且将一个false negative定义为一对被散列到不同的桶中近距特征(with d(p,q)≤r1) LSH Operations我们描述了LSH可以应用的主要操作类型。一个拟合的LSH模型具有下列每个操作的方法。 Feature TransformationFeature Transformation是添加哈希值作为新列的基本功能。这对降维有用。用户可以通过设置inputCol和outputCol来指定输入和输出列名。 LSH还支持多个LSH哈希表。用户可以通过设置numHashTables来指定哈希表的数量。这也用于在approximate similarity join和approximate nearest neighbo的OR-amplification。增加哈希表的数量会提高精度，但同时也会增加通信成本和运行时间。 outputCol的类型是Seq[Vector]，其中数组的维度等于numHashTables，vectors的维度当前设置为1。在未来的版本中，我们将实现AND-amplification，使得用户可以指定这些vectors的维度。 Approximate Similarity JoinApproximate Similarity Join输入两个数据集，近似地返回数据集中那些距离小于用户定义的阈值的数行对。Approximate similarity join支持连接两个不同的数据集和self-joining(自连接)。自加入会产生一些重复的对。 Approximate similarity join接受转换和未转换的数据集作为输入。如果使用未转换的数据集，则会自动进行转换。在这种情况下，hash signture(哈希签名)将被创建为outputCol。 在连接的数据集中，可以在datasetA和datasetB中查询原始数据集。距离列将被添加到输出数据集，以显示返回的每对行之间的真实距离。 Approximate Nearest Neighbor SearchApproximate nearest neighbor search需要（拥有特征向量s的）数据集和一个关键字（单个特征向量），并且它近似地返回数据集中最接近这个向量的指定数量的行。 Approximate nearest neighbor search接受转换和未转换的数据集作为输入。如果使用未转换的数据集，则会自动进行转换。在这种情况下，哈希签名将被创建为outputCol。 距离列将被添加到输出数据集，以显示每个输出行和搜索键之间的真实距离。 注意：当散列桶中没有足够的候选项时，Approximate nearest neighbor search将返回少于l行。 LSH AlgorithmsBucketed Random Projection for Euclidean DistanceBucketed Random Projection是一个基于欧氏距离的LSH family。欧几里德距离定义如下：12d(x,y) = sqrt(sum((xi - yi)**2)) 其LSH族将特征向量投影到随机单位向量上，并将投影结果分成哈希桶： 12h(x) = [x·v/r] 其中r是用户定义的桶长度。桶长度可以用来控制散列桶的平均大小（从而控制桶的数量）。较大的桶长度（即，较少的桶）增加了特征被散列到相同桶的可能性（增加了true and false positives）。 Bucketed Random Projection接受任意向量作为输入特征，同时支持稀疏和密集向量。\有关API的更多详细信息，请参阅BucketedRandomProjectionLSH Python文档。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from pyspark.ml.feature import BucketedRandomProjectionLSHfrom pyspark.ml.linalg import Vectorsfrom pyspark.sql.functions import colfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("BucketedRandomProjectionLshExample").getOrCreate()dataA = [(0, Vectors.dense([1.0, 1.0]),), (1, Vectors.dense([1.0, -1.0]),), (2, Vectors.dense([-1.0, -1.0]),), (3, Vectors.dense([-1.0, 1.0]),)]dfA = spark.createDataFrame(dataA, ["id", "features"])dataB = [(4, Vectors.dense([1.0, 0.0]),), (5, Vectors.dense([-1.0, 0.0]),), (6, Vectors.dense([0.0, 1.0]),), (7, Vectors.dense([0.0, -1.0]),)]dfB = spark.createDataFrame(dataB, ["id", "features"])key = Vectors.dense([1.0, 0.0])brp = BucketedRandomProjectionLSH(inputCol="features", outputCol="hashes", bucketLength=2.0, numHashTables=3)model = brp.fit(dfA)# Feature Transformationprint("The hashed dataset where hashed values are stored in the column 'hashes':")model.transform(dfA).show(truncate=False)# Compute the locality sensitive hashes for the input rows, then perform approximate# similarity join.# We could avoid computing hashes by passing in the already-transformed dataset, e.g.# `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`print("Approximately joining dfA and dfB on Euclidean distance smaller than 1.5:")model.approxSimilarityJoin(dfA, dfB, 1.5, distCol="EuclideanDistance")\ .select(col("datasetA.id").alias("idA"), col("datasetB.id").alias("idB"), col("EuclideanDistance")).show(truncate=False)# Compute the locality sensitive hashes for the input rows, then perform approximate nearest# neighbor search.# We could avoid computing hashes by passing in the already-transformed dataset, e.g.# `model.approxNearestNeighbors(transformedA, key, 2)`print("Approximately searching dfA for 2 nearest neighbors of the key:")model.approxNearestNeighbors(dfA, key, 2).show(truncate=False)spark.stop() output:1234567891011121314151617181920212223242526272829303132The hashed dataset where hashed values are stored in the column 'hashes':+---+-----------+-----------------------+|id |features |hashes |+---+-----------+-----------------------+|0 |[1.0,1.0] |[[-1.0], [0.0], [0.0]] ||1 |[1.0,-1.0] |[[0.0], [-1.0], [0.0]] ||2 |[-1.0,-1.0]|[[0.0], [-1.0], [-1.0]]||3 |[-1.0,1.0] |[[-1.0], [0.0], [-1.0]]|+---+-----------+-----------------------+Approximately joining dfA and dfB on Euclidean distance smaller than 1.5:+---+---+-----------------+|idA|idB|EuclideanDistance|+---+---+-----------------+|0 |4 |1.0 ||2 |7 |1.0 ||1 |4 |1.0 ||0 |6 |1.0 ||3 |6 |1.0 ||3 |5 |1.0 ||1 |7 |1.0 ||2 |5 |1.0 |+---+---+-----------------+Approximately searching dfA for 2 nearest neighbors of the key:+---+----------+----------------------+-------+|id |features |hashes |distCol|+---+----------+----------------------+-------+|0 |[1.0,1.0] |[[-1.0], [0.0], [0.0]]|1.0 ||1 |[1.0,-1.0]|[[0.0], [-1.0], [0.0]]|1.0 |+---+----------+----------------------+-------+ Find full example code at “examples/src/main/python/ml/bucketed_random_projection_lsh_example.py” in the Spark repo. MinHash for Jaccard DistanceMinHash是用于计算Jaccard距离的LSH族，其中输入特征是自然数集合。两个集合的Jaccard距离由它们的交集和并集决定：12d(A,B) = 1 -|A ∩ B| / |A ∪ B| MinHash 对集合中的每个元素应用随机哈希函数g，并取所有哈希值的最小值：12h(A) = min(g(a)) ,a∈A MinHash的输入集表示为二元向量，其中向量索引表示元素本身，向量中的非零值表示集合中元素的存在。虽然支持密集和稀疏向量，但通常建议使用稀疏向量来提高效率。例如，Vectors.sparse(10, Array[(2, 1.0), (3, 1.0), (5, 1.0)])意味着空间中有10个元素。该集合包含元素2，元素3和元素5.所有非零值都被视为二进制“1”值。 注意：空集不能被MinHash转换，这意味着任何输入向量必须至少有一个非零的entry。 有关API的更多详细信息，请参阅MinHashLSH Python文档。123456789101112131415161718192021222324252627282930313233343536373839404142434445from pyspark.ml.feature import MinHashLSHfrom pyspark.ml.linalg import Vectorsfrom pyspark.sql.functions import colfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("MinHashLSHExample").getOrCreate()dataA = [(0, Vectors.sparse(6, [0, 1, 2], [1.0, 1.0, 1.0]),), (1, Vectors.sparse(6, [2, 3, 4], [1.0, 1.0, 1.0]),), (2, Vectors.sparse(6, [0, 2, 4], [1.0, 1.0, 1.0]),)]dfA = spark.createDataFrame(dataA, ["id", "features"])dataB = [(3, Vectors.sparse(6, [1, 3, 5], [1.0, 1.0, 1.0]),), (4, Vectors.sparse(6, [2, 3, 5], [1.0, 1.0, 1.0]),), (5, Vectors.sparse(6, [1, 2, 4], [1.0, 1.0, 1.0]),)]dfB = spark.createDataFrame(dataB, ["id", "features"])key = Vectors.sparse(6, [1, 3], [1.0, 1.0])mh = MinHashLSH(inputCol="features", outputCol="hashes", numHashTables=5)model = mh.fit(dfA)# Feature Transformationprint("The hashed dataset where hashed values are stored in the column 'hashes':")model.transform(dfA).show()# Compute the locality sensitive hashes for the input rows, then perform approximate# similarity join.# We could avoid computing hashes by passing in the already-transformed dataset, e.g.# `model.approxSimilarityJoin(transformedA, transformedB, 0.6)`print("Approximately joining dfA and dfB on distance smaller than 0.6:")model.approxSimilarityJoin(dfA, dfB, 0.6, distCol="JaccardDistance")\ .select(col("datasetA.id").alias("idA"), col("datasetB.id").alias("idB"), col("JaccardDistance")).show()# Compute the locality sensitive hashes for the input rows, then perform approximate nearest# neighbor search.# We could avoid computing hashes by passing in the already-transformed dataset, e.g.# `model.approxNearestNeighbors(transformedA, key, 2)`# It may return less than 2 rows when not enough approximate near-neighbor candidates are# found.print("Approximately searching dfA for 2 nearest neighbors of the key:")model.approxNearestNeighbors(dfA, key, 2).show()spark.stop() output:123456789101112131415161718192021222324252627The hashed dataset where hashed values are stored in the column 'hashes':+---+--------------------+--------------------+| id| features| hashes|+---+--------------------+--------------------+| 0|(6,[0,1,2],[1.0,1...|[[-8.91727E8], [-...|| 1|(6,[2,3,4],[1.0,1...|[[-1.81795643E9],...|| 2|(6,[0,2,4],[1.0,1...|[[-1.33587497E8],...|+---+--------------------+--------------------+Approximately joining dfA and dfB on distance smaller than 0.6:+---+---+---------------+|idA|idB|JaccardDistance|+---+---+---------------+| 1| 4| 0.5|| 1| 5| 0.5|| 2| 5| 0.5|| 0| 5| 0.5|+---+---+---------------+Approximately searching dfA for 2 nearest neighbors of the key:+---+--------------------+--------------------+-------+| id| features| hashes|distCol|+---+--------------------+--------------------+-------+| 0|(6,[0,1,2],[1.0,1...|[[-8.91727E8], [-...| 0.75|| 1|(6,[2,3,4],[1.0,1...|[[-1.81795643E9],...| 0.75|+---+--------------------+--------------------+-------+ Find full example code at “examples/src/main/python/ml/min_hash_lsh_example.py” in the Spark repo. 结束]]></content>
      <categories>
        <category>Spark</category>
        <category>MLlib</category>
      </categories>
      <tags>
        <tag>TF-IDF</tag>
        <tag>Word2Vec</tag>
        <tag>OneHotEncoding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkMLlib-Pipeline]]></title>
    <url>%2F2018%2F01%2F19%2Fsparkmllib-pipeline%2F</url>
    <content type="text"><![CDATA[Spark中的管道pipeline ML Pipelines 管道中的主要概念MLlib对机器学习算法的API进行了标准化，使得将多种算法合并成一个流水线或工作流变得更加容易。本部分涵盖了Pipelines API引入的关键概念，其中流水线概念主要受scikit-learn项目的启发。 DataFrame：这个ML API使用Spark SQL中的DataFrame作为一个ML数据集，它可以容纳各种数据类型。例如，一个DataFrame可以具有存储文本，特征向量，真实标签和预测的不同列。 Transformer：一个Transformer是可以将一个DataFrame变换成成另一个DataFrame的算法。例如，一个ML模型是一个Transformer将一个DataFrame特征转化为一个DataFrame预测的模型。 Estimator：一个 Estimator是一个可以被应用在DataFrame上来产生一个Transformer的算法。例如，一个学习算法是一种Estimator，它可以在DataFrame上训练并生成模型。 Pipeline：Pipeline将多个Transformers和Estimators连接起来以指定ML工作流程。 Parameter：所有Transformers和Estimators现在对于指定参数共享通用API。 DataFrame(数据帧)机器学习可以应用于各种数据类型，如向量，文本，图像和结构化数据。这个API采用DataFrameSpark SQL来支持各种数据类型。 DataFrame支持许多基本和结构化的类型; 请参阅Spark SQL数据类型参考以获取受支持类型的列表。除了Spark SQL指南中列出的类型以外，DataFrame还可以使用ML Vector类型。 A DataFrame可以隐式地或显式地从常规创建RDD。有关示例，请参阅下面的代码示例和Spark SQL编程指南。 a DataFrame中的列被命名。下面的代码示例使用“text”，“feature”和“label”等名称。 Pipeline components(管道组件)TransformersA Transformer是包含特征变换器和学习模型的抽象。从技术上来说，a Transformer实现了一种方法(transform()),将一个DataFrame转换为另一个的方法，通常通过附加一列或多列。例如： 特征转换器选取一个DataFrame，读取列（例如文本），将其映射到新的列（例如特征向量），并且输出具有附加映射列的新DataFrame 学习模型可以选取一个DataFrame，读取包含特征向量的列，预测每个特征向量的标签，并输出带有预测标签的新列的DataFrame。 Estimators一个Estimator是在数据集上训练的学习算法的抽象概念。从技术上讲，一个Estimator实现了一个方法 fit()，它接受DataFrame并生成一个 Model，这是一个Transformer。例如，一个学习算法，如LogisticRegression(它是一个Estimator)，调用 fit() 函数来训练一个LogisticRegressionModel模型，它是一个Model也是一个Transformer。 Properties o pipeline componentsTransformer.transform()s和Estimator.fit()s都是无状态的。将来，有状态算法可以通过替代概念来支持。每个Transformer或者Estimator的实例具有唯一的ID，这在指定参数（在下面讨论）中是有用的。 Pipeline在机器学习中，通常运行一系列算法来处理和学习数据。例如，简单的文本文档处理工作流程可能包括几个阶段： 将每个文档的文本分词。 将每个文档的单词转换为数字特征向量。 使用特征向量和标签来学习预测模型。 MLlib表示这样一个工作流程Pipeline，它由一系列 PipelineStages（Transformers和Estimators）组成，并以特定顺序运行。我们将使用这个简单的工作流程作为本节中的一个运行示例。 How it worksA Pipeline是一个阶段序列，每个阶段是一个Transformer或一个Estimator。这些阶段是按顺序运行的，输入的DataFrame在每个阶段都经过转换。对于Transformer阶段，transform() 方法被调用作用于DataFrame上。对于Estimator阶段，fit()方法被调用，以产生Transformer（它成为PipelineModel或合适的Pipeline的一部分），以及Transformer的transform()方法也被调用作用于DataFrame。 我们用简单的文本文档工作流来说明这一点。下图是a 。训练时间的使用情况Pipeline。 在上面，最上面一行代表一个Pipeline有三个阶段。前两个（Tokenizer和HashingTF）是Transformers（蓝色），第三个（LogisticRegression）是Estimator（红色）。最下面一行代表流经管道的数据，其中圆柱表示DataFrames。这个Pipeline.fit()方法在原始DataFrame文档和标签上被调用。Tokenizer.transform()方法将原始文本文档分词，分词后的words作为一个新列添加到DataFrame中。HashingTF.transform()方法将单词列转换为特征向量，并向这些向量作为一个新列添加到DataFrame中。现在，既然LogisticRegression是一个Estimator，Pipeline首先调用LogisticRegression.fit()方法就生成一个LogisticRegressionModel。如果Pipeline有更多Estimators，它就会在DataFrame传送到下个阶段之前调用LogisticRegressionModel的transform() 方法。 一个Pipeline是一个Estimator。因此，在Pipeline的fit()方法运行后，它产生一个PipelineModel，这是一个 Transformer。这PipelineModel是在测试时使用 ; 下图说明了这种用法。 在上面的图中，PipelineModel具有和原始的Pipeline相同数量的阶段，但所有EstimatorS在原始Pipeline中已变成TransformerS。当PipelineModel的transform()方法在测试数据集被调用，数据在管道上按序传递。每个阶段的transform()方法都会更新数据集并将其传递到下一个阶段。 Pipelines和PipelineModels有助于确保训练和测试数据经过相同的特征处理步骤。 DetailsDAG Pipelines：A Pipeline的阶段被指定为一个有序数组。这里给出的例子都是线性Pipeline的，即Pipeline每个阶段使用前一阶段产生的数据。Pipeline只要数据流图形成有向无环图（DAG），就可以创建非线性的PipelineS。该图当前是基于每个阶段的输入和输出列名（通常指定为参数）隐含指定的。如果Pipeline形式为DAG，那么阶段必须按拓扑顺序指定。 Runtime checking：由于Pipelines可以在不同类型的DataFrames上运行，所以不能使用compile-time类型检查。 Pipelines和PipelineModels，而是在实际运行Pipeline之前进行runtime checking检查。这种类型的检查是通过使用DataFrame schema来完成的，schema是对DataFrame的列的数据类型的描述。 Unique Pipeline stages：A Pipeline的阶段应该是独一无二的实例。例如，同一个实例 myHashingTF不应该插入Pipeline两次，因为Pipeline阶段必须有唯一的ID。然而，不同的实例myHashingTF1和myHashingTF2（两个类型HashingTF）可以放在一起，Pipeline因为创建不同的实例使用不同的ID。 ParametersMLlib Estimators和Transformers使用统一的API来指定参数。 A Param是一个带有自包含文档的命名参数。A ParamMap是一组（参数，值）对。 将参数传递给算法有两种主要方法： 为实例设置参数。例如，如果lr是的一个实例LogisticRegression，它可以调用lr.setMaxIter(10)让lr.fit()至多10次迭代使用。这个API类似于spark.mllib包中使用的API 。 传递ParamMap给fit()或transform()。任何在ParamMap中额参数将覆盖以前通过setter方法指定的参数。 参数属于Estimators和Transformers的特定实例。例如，如果我们有两个LogisticRegression实例lr1和lr2，然后我们可以建立一个ParamMap与两个maxIter指定的参数：ParamMap(lr1.maxIter -&gt; 10, lr2.maxIter -&gt; 20)。如果一个Pipeline里有两个包含maxIter参数的算法，那么这很有用。 Saving and LoadingPipelines通常情况下，会将模型或管道保存到磁盘供以后使用。在Spark 1.6中，模型导入/导出功能被添加到管道API中。大多数基本的Transformers都和一些更加基本的ML模型一样被支持。请参阅算法的API文档以查看是否支持保存和加载。 Code examples本节给出了说明上述功能的代码示例。有关更多信息，请参阅API文档（Scala， Java和Python）。 Example: Estimator, Transformer, and Param这个例子涉及的概念Estimator，Transformer和Param。请参阅EstimatorPython文档，TransformerPython文档和ParamsPython文档以获取有关API的更多详细信息。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162from pyspark.ml.linalg import Vectorsfrom pyspark.ml.classification import LogisticRegressionfrom pyspark.sql import SparkSession spark = SparkSession.builder.appName("ParamsExample").getOrCreate()# Prepare training data from a list of (label, features) tuples.training = spark.createDataFrame([ (1.0, Vectors.dense([0.0, 1.1, 0.1])), (0.0, Vectors.dense([2.0, 1.0, -1.0])), (0.0, Vectors.dense([2.0, 1.3, 1.0])), (1.0, Vectors.dense([0.0, 1.2, -0.5]))], ["label", "features"])# Create a LogisticRegression instance. This instance is an Estimator.lr = LogisticRegression(maxIter=10, regParam=0.01)# Print out the parameters, documentation, and any default values.print("LogisticRegression parameters:\n" + lr.explainParams() + "\n")# Learn a LogisticRegression model. This uses the parameters stored in lr.model1 = lr.fit(training)# Since model1 is a Model (i.e., a transformer produced by an Estimator),# we can view the parameters it used during fit().# This prints the parameter (name: value) pairs, where names are unique IDs for this# LogisticRegression instance.print("Model 1 was fit using parameters: ")print(model1.extractParamMap())# We may alternatively specify parameters using a Python dictionary as a paramMapparamMap = &#123;lr.maxIter: 20&#125;paramMap[lr.maxIter] = 30 # Specify 1 Param, overwriting the original maxIter.paramMap.update(&#123;lr.regParam: 0.1, lr.threshold: 0.55&#125;) # Specify multiple Params.# You can combine paramMaps, which are python dictionaries.paramMap2 = &#123;lr.probabilityCol: "myProbability"&#125; # Change output column nameparamMapCombined = paramMap.copy()paramMapCombined.update(paramMap2)# Now learn a new model using the paramMapCombined parameters.# paramMapCombined overrides all parameters set earlier via lr.set* methods.model2 = lr.fit(training, paramMapCombined)print("Model 2 was fit using parameters: ")print(model2.extractParamMap())# Prepare test datatest = spark.createDataFrame([ (1.0, Vectors.dense([-1.0, 1.5, 1.3])), (0.0, Vectors.dense([3.0, 2.0, -0.1])), (1.0, Vectors.dense([0.0, 2.2, -1.5]))], ["label", "features"])# Make predictions on test data using the Transformer.transform() method.# LogisticRegression.transform will only use the 'features' column.# Note that model2.transform() outputs a "myProbability" column instead of the usual# 'probability' column since we renamed the lr.probabilityCol parameter previously.prediction = model2.transform(test)result = prediction.select("features", "label", "myProbability", "prediction") \ .collect()for row in result: print("features=%s, label=%s -&gt; prob=%s, prediction=%s" % (row.features, row.label, row.myProbability, row.prediction))spark.stop() Find full example code at “examples/src/main/python/ml/estimator_transformer_param_example.py” in the Spark repo. Example: Pipeline本示例遵循Pipeline上图中所示的简单文本文档。有关APi的更多详细信息，请参阅PipelinePython文档 123456789101112131415161718192021222324252627282930313233343536373839from pyspark.ml import Pipelinefrom pyspark.ml.classification import LogisticRegressionfrom pyspark.ml.feature import HashingTF, Tokenizerfrom pyspark.sql import SparkSession spark = SparkSession.builder.appName(""PipeLineExample"").getOrCreate()# Prepare training documents from a list of (id, text, label) tuples.training = spark.createDataFrame([ (0, "a b c d e spark", 1.0), (1, "b d", 0.0), (2, "spark f g h", 1.0), (3, "hadoop mapreduce", 0.0)], ["id", "text", "label"])# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.tokenizer = Tokenizer(inputCol="text", outputCol="words")hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")lr = LogisticRegression(maxIter=10, regParam=0.001)pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])# Fit the pipeline to training documents.model = pipeline.fit(training)# Prepare test documents, which are unlabeled (id, text) tuples.test = spark.createDataFrame([ (4, "spark i j k"), (5, "l m n"), (6, "spark hadoop spark"), (7, "apache hadoop")], ["id", "text"])# Make predictions on test documents and print columns of interest.prediction = model.transform(test)selected = prediction.select("id", "text", "probability", "prediction")for row in selected.collect(): rid, text, prob, prediction = row print("(%d, %s) --&gt; prob=%s, prediction=%f" % (rid, text, str(prob), prediction))spark.stop() Find full example code at “examples/src/main/python/ml/pipeline_example.py” in the Spark repo. Model selection (hyperparameter tuning)- 模型选择(超参数调整)使用ML管道的一大好处是超参数优化。有关自动模型选择的更多信息，请参阅ML调整指南。 结束]]></content>
      <categories>
        <category>Spark</category>
        <category>MLlib</category>
      </categories>
      <tags>
        <tag>pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkMLlib-Basic]]></title>
    <url>%2F2018%2F01%2F19%2Fsparkmllib-basic%2F</url>
    <content type="text"><![CDATA[机器学习裤(MLlib)指南 MLlib是Spark的机器学习库，可让实际的机器学习容易和可扩展，它提供了如下工具： ML算法：通用学习算法，如分类，回归，聚类和协同过滤 特征提取，特征提取，转换，降维和选择 管道：用于构建，评估和调整ML管道的工具 持久性：保存和加载算法，模型和管道 实用程序：线性代数，统计，数据处理等 公告：基于DataFrame的API是主要的APIMLlib基于RDD的API现在处于维护模式。 从Spark 2.0开始，包中的基于RDD的API spark.mllib已进入维护模式。Spark的主要机器学习API现在是包中的基于DataFrame的API spark.ml。 有什么影响？ MLlib将仍然支持基于RDD的API spark.mllib并修复错误。 MLlib不会将新功能添加到基于RDD的API。 在Spark 2.x版本中，MLlib将为基于DataFrame的API添加功能，以便与基于RDD的API达成功能奇偶校验。 达到功能奇偶校验（大致估计为Spark 2.3）后，基于RDD的API将被弃用。 基于RDD的API预计将在Spark 3.0中被删除。 为什么MLlib切换到基于DataFrame的API？ DataFrames提供比RDD更友好的API。DataFrame的许多优点包括Spark数据源，SQL / DataFrame查询，Tungsten和Catalyst优化以及跨语言的统一API。 MLlib的基于DataFrame的API提供跨ML算法和跨多种语言的统一API。 数据框便于实际的ML管线，特别是功能转换。有关详细信息，请参阅管道指南。 什么是“Spark ML”？ “Spark ML”不是一个正式的名字，偶尔用于指代基于MLlib DataFrame的API。这主要是由于org.apache.spark.ml基于DataFrame的API所使用的Scala包名以及我们最初用来强调管道概念的“Spark ML Pipelines”术语。 MLlib是否被弃用？ 编号MLlib包括基于RDD的API和基于DataFrame的API。基于RDD的API现在处于维护模式。但是这两个API都没过时，MLlib也是。 依赖MLlib使用线性代数包Breeze，它依赖于 netlib-java进行优化的数值处理。如果本机库1在运行时不可用，您将看到一条警告消息，而将使用纯JVM实现。 由于运行时专有二进制文件的授权问题，netlib-java默认情况下，我们不包含本地代理。要配置netlib-java/ Breeze以使用系统优化的二进制文件，请包括 com.github.fommil.netlib:all:1.1.2（或者构建Spark -Pnetlib-lgpl）作为项目的依赖项，并阅读netlib-java文档以获取平台的其他安装说明。 要在Python中使用MLlib，您将需要NumPy 1.4或更高版本。 2.2中的亮点下面的列表突出了在2.2 Spark发行版中添加到MLlib中的一些新功能和增强功能： ALS为所有用户或项目提供top-k建议的方法，与mllib （SPARK-19535）中的功能相匹配。性能也得到了改善两者ml和mllib （SPARK-11968和 SPARK-20587） Correlation和 ChiSquareTest统计功能DataFrames （SPARK-19636和 SPARK-19635） FPGrowth频繁模式挖掘算法（SPARK-14503） GLM现在支持Tweedie全家（SPARK-18929） Imputer特征变换器来估算数据集中的缺失值（SPARK-13​​568） LinearSVC 对于线性支持向量机分类（SPARK-14709） 逻辑回归现在支持训练期间系数的限制（SPARK-20047） 迁移指南MLlib正在积极开发中。未来发行版中标记为Experimental/ 的API DeveloperApi可能会更改，下面的迁移指南将解释发行版之间的所有更改。 Basic StatisticsCorrellation(相关性)计算两组数据之间的相关性是统计学中的一个常见操作。在spark.ml 我们提供的灵活性来计算多个系列之间的成对相关性。支持的相关方法目前是皮尔逊和斯皮尔曼相关性。 Correlation 使用指定的方法计算输入矢量数据集的相关矩阵。输出将是一个DataFrame，它包含向量列的相关矩阵。 1234567891011121314151617from pyspark.ml.linalg import Vectorsfrom pyspark.ml.stat import Correlationfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("CorrelationExample").getOrCreate()data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),), (Vectors.dense([4.0, 5.0, 0.0, 3.0]),), (Vectors.dense([6.0, 7.0, 0.0, 8.0]),), (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]df = spark.createDataFrame(data, ["features"])r1 = Correlation.corr(df, "features").head()print("Pearson correlation matrix:\n" + str(r1[0]))r2 = Correlation.corr(df, "features", "spearman").head()print("Spearman correlation matrix:\n" + str(r2[0]))spark.stop() Find full example code at “examples/src/main/python/ml/correlation_example.py” in the Spark repo. Hypothesis testing(假设检验)假设检验是统计学中一个强大的工具，用来确定一个结果是否具有统计显著性，这个结果是否偶然发生。spark.ml目前支持皮尔逊的卡方（χ2χ2）测试独立性。 ChiSquareTest针对标签的每个特征进行皮尔森独立测试。对于每个特征，（特征，标签）对被转换为计算卡方统计量的可能性矩阵。所有标签和特征值必须是分类的。 123456789101112131415161718from pyspark.ml.linalg import Vectorsfrom pyspark.ml.stat import ChiSquareTestfrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName("HypothesisTestExample").getOrCreate()data = [(0.0, Vectors.dense(0.5, 10.0)), (0.0, Vectors.dense(1.5, 20.0)), (1.0, Vectors.dense(1.5, 30.0)), (0.0, Vectors.dense(3.5, 30.0)), (0.0, Vectors.dense(3.5, 40.0)), (1.0, Vectors.dense(3.5, 40.0))]df = spark.createDataFrame(data, ["label", "features"])r = ChiSquareTest.test(df, "features", "label").head()print("pValues: " + str(r.pValues))print("degreesOfFreedom: " + str(r.degreesOfFreedom))print("statistics: " + str(r.statistics))spark.stop() Find full example code at “examples/src/main/python/ml/chi_square_test_example.py” in the Spark repo. Pipeline接着往下阅读Pipeline 结束]]></content>
      <categories>
        <category>Spark</category>
        <category>MLlib</category>
      </categories>
      <tags>
        <tag>DataFrame</tag>
        <tag>RDD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo-music]]></title>
    <url>%2F2018%2F01%2F19%2Fhexo-music%2F</url>
    <content type="text"><![CDATA[hexo 添加音乐，iframe 、 hexo-tag-aplayer iframe方式生成音乐外链找到网易云的生成外链页面后(注找有版权保护的音乐不行，请更换没有版权保护的) 点击iframe插件，根据自己的喜好设置后复制iframe代码 1234&lt;div id="music163player"&gt; &lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=280 height=86 src="//music.163.com/outchain/player?type=2&amp;id=38358214&amp;auto=0&amp;height=66"&gt; &lt;/iframe&gt;&lt;/div&gt; 添加到sidebar.swig放置到hexo-theme/next/layout/_macro/sidebar.swig文件下 123456789 ... &lt;pre&gt; &lt;!--insert here--&gt; &lt;div id="music163player"&gt; &lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=280 height=86 src="//music.163.com/outchain/player?type=2&amp;id=38358214&amp;auto=0&amp;height=66"&gt; &lt;/iframe&gt; &lt;/div&gt; &lt;/aside&gt;&#123;% endmacro %&#125; 效果展示 hexo-tag-aplayer安装1npm install hexo-tag-aplayer@2.0.1 添加到文章中在media目录和images目录放置相应音频和图片文件，当然你也可以填写网络上的相关地址 1&#123;% aplayer "Lucid dream" "Owl City" "/media/Lucid-Dream.mp3" "/images/lucid-dream.jpg" %&#125; 运行hexo g,hexo s -p 3000可在本地查看效果或运行hexo g -d部署，进入自己的网站查看 效果展示 new APlayer({ element: document.getElementById("aplayer0"), narrow: false, autoplay: false, showlrc: 0, music: { title: "Lucid dream", author: "Owl City", url: "/media/Lucid-Dream.mp3", pic: "/images/lucid-dream.jpg", } }); 结束]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>music</tag>
        <tag>hexo-tag-aplayer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python进程线程 - threading]]></title>
    <url>%2F2018%2F01%2F18%2Fthreading%2F</url>
    <content type="text"><![CDATA[Python线程模块threading, 生产者消费者模型等 python线程简介可以用线程来执行阻塞式I/O,但不要用它做平行计算 标准的Python实现叫做CPython。Cpython分两步来运行Python程序： 首先，把文本形式的源代码解析并编译成字节码 然后，用一种基于栈的解释器来运行这份字节码 执行Python程序时，字节码解释器必须保持协调一致的状态。Python采用GIL(global inerpreter lock,全局解释器锁)机制来确保这种协调性(coherence)。 GIL实际上就是一把互斥锁(mutual-exclusion-lock,又称为mutex)，用以防止CPython受到占先式多线程切换(preemptive multithreaing)操作的干扰。 GIL有一种非常显著额负面影响。用C++或者Java等语言写程序时，可以同时执行多条线程，以充分利用计算机所配备的多个CPU核心。Python程序尽管也支持多线程，但由于受到GIL保护，所以同一时刻，只有一条线程可以向前执行。这就意味着，如果我们想利用多线程做平行计算(parallel computation)， 并希望借此为Python程序提速，那么结果会非常令人失望。 既然如此，Python为什么还要支持多线程呢？ 首先，多线程使得到程序看上去好像能够在同一时间做许多事情。如果要自己实现这种效果，并手工管理任务之间的切换，那就显得比较困难 其次，在处理阻塞式I/O时很有用。读写文件、在网络间通信，以及与显示器等设备相交互等，都属于阻塞式的I/O操作。为了响应这种阻塞式的请求，操作系统必须花一些时间，而开发者可以借助多线程，把python程序与这些耗时的I/O操作隔离开。(python在执行系统调用的时候会释放GIL)。当然除了线程，还有一些其他的方，也能处理阻塞式的I/O操作，例如内置的asyncio模块等。相对于这些模块，使用多线程来实现会比较简单一些。 使用线程threading.ThreadThread 是threading模块中最重要的类之一，可以使用它来创建线程。有两种方式来创建线程： 一种是创建一个threading.Thread对象，在它的初始化函数（init）中将可调用对象作为参数传入。 另一种是通过继承Thread类，重写它的run方法； 下面分别举例说明：开启4个线程，每个线程进行10次+1操作 先来看看通过创建继承threading.Thread对象来创建线程的例子： 123456789101112131415161718192021222324252627import timeimport randomimport threadingdef func1(loop): """创建threading.Thread对象的方式创建线程""" global func1_count, func1_lock thread_name = threading.currentThread().getName() # 获取线程名 for _ in range(loop): with func1_lock: func1_count += 1 print(thread_name, func1_count) time.sleep(1)def func1_main(num): global func1_count, func1_lock threads = [] func1_count = 0 func1_lock = threading.Lock() # 线程中使用Lock防止数据竞争 for i in range(num): t = threading.Thread(target=func1,args=(10, )) threads.append(t) for t in threads: t.start() # 启动所有线程 for t in threads: t.join() # 主线程中等待所有子线程退出 继承Thread类: 1234567891011121314151617181920212223242526class Counter(threading.Thread): my_count = 0 # 类变量 my_lock = threading.Lock() def __init__(self, loop=10): super().__init__() self._loop = loop # self._count = init_count # self._lock = threading.Lock() def run(self): thread_name = threading.currentThread().getName() for _ in range(self._loop): with Counter.my_lock: Counter.my_count += 1 print(thread_name, Counter.my_count) time.sleep(1)def func2_main(num): threads = [] for _ in range(num): t = Counter() # 默认loop为10，init_count为0 t.start() threads.append(t) for t in threads: t.join() 相对于方法一的修改，不使用global而是使用一个自定义的counter类 12345678910111213141516171819202122232425class LockingCounter(object): def __init__(self, init_count): self._lock = threading.Lock() self._count = init_count def increase(self, offset=1): with self._lock: self._count += 1def worker(index, loop, counter): thread_name = threading.currentThread().getName() for _ in range(loop): counter.increase(1) print(thread_name, counter._count) time.sleep(1)def func3_main(num, func, loop, counter): threads = [] for i in range(num): args = (i, loop, counter) t = threading.Thread(target=func, args=args) threads.append(t) t.start() for t in threads: t.join() 运行： 123456789if __name__ == '__main__': print('-----method1-----:') thread_num = 4 func1_main(thread_num) print('-----method2-----:') func2_main(thread_num) print('-----method3-----:') counter = LockingCounter(0) func3_main(thread_num, worker, 10, counter) 使用Queue来协调各线程之间的工作管线(Pipeline)是一种优秀的任务处理方式，它可以把处理流程分为若干阶段，并使用多条Python线程来同时执行这些任务 构建并发式的管线时，要注意许多问题，其中包括：如何防止某个阶段陷入持续等待的状态之中、如何停止工作线程，以及如何防止内存膨胀等。 Queue类所提供的的机制，可以彻底解决上述问题，它具备阻塞式的队列操作、能够制定缓冲区尺寸，而且还支持join方法，这使得开发者可以构建出健壮的管线。 示例：生产者消费者模型 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import timeimport randomimport threadingfrom queue import Queue # 队列模块from itertools import chainq = Queue()sentinel = object() # 结束标记def Producer(nums): """生产者函数 nums:product起始编号元组,例如(1,10)""" thread_name = threading.currentThread().getName() for item in range(*nums): q.put(item) print('[+] %s 生产 item%s' % (thread_name,item)) time.sleep(random.randrange(2)) # 控制生产速度def Consumer(): """消费者函数""" thread_name = threading.currentThread().getName() while True: data = q.get() if data is sentinel: print('[x] %s 退出' % thread_name) break print('[-] %s 消费 item%s' % (thread_name, data)) time.sleep(1)def run(): """主函数""" pnum = 2 cnum = 3 # 生产者线程，每个线程生产10个，1号线程生产1，10，2号生产11-20...... pthreads = [ threading.Thread(target=Producer, args=((i * 10 + 1, (i + 1) * 10 + 1),), name="生产者%d号" % (i + 1)) for i in range(pnum)] # 消费者线程 cthreads = [ threading.Thread(target=Consumer, name="消费者%d号" % (i + 1)) for i in range(cnum)] for thread in chain(pthreads, cthreads): thread.start() for pt in pthreads: pt.join() # 生产线程阻塞 for _ in range(cnum): q.put(sentinel) # put结束标记 for ct in cthreads: ct.join() print("all done")if __name__ == '__main__': run() output: 123456789101112......[+] 生产者1号 生产 item8[-] 消费者2号 消费 item8[+] 生产者1号 生产 item9[-] 消费者3号 消费 item9[+] 生产者1号 生产 item10[-] 消费者2号 消费 item10[x] 消费者1号 退出[x] 消费者3号 退出[x] 消费者2号 退出all done 管线 我们构建一个有三个阶段的管线：下载图片–&gt;&gt;调整大小–&gt;&gt;重新上传 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495# -*- coding: utf-8 -*-"""用threading模块和Queue实现管线"""import timeimport threadingfrom queue import Queueclass ClosableQueue(Queue): """带有终止信号的Queue close时put终止信号""" SENTINEL = object() # 终止信号 def close(self): self.put(self.SENTINEL) def __iter__(self): while True: item = self.get() try: if item is self.SENTINEL: return # 致使线程退出 yield item finally: self.task_done()class StopableWorker(threading.Thread): """queue遇到终止信号的线程退出""" def __init__(self, func, in_queue, out_queue): super().__init__() self.func = func self.in_queue = in_queue self.out_queue = out_queue def run(self): for item in self.in_queue: result = self.func(item) if result is not None: self.out_queue.put(result)def download(item): """下载""" print('download item ', item) time.sleep(0.1) return item def resize(item): """调整""" print('resize item ', item) time.sleep(0.1) return itemdef upload(item): """上传""" print('upload item ', item) return item def main(): """主程序""" # 各阶段队列 download_queue = ClosableQueue() resize_queue = ClosableQueue() upload_queue = ClosableQueue() out_queue = Queue() # 线程 threads = [ StopableWorker(download, download_queue, resize_queue), StopableWorker(resize, resize_queue, upload_queue), StopableWorker(upload, upload_queue, out_queue) ] for thread in threads: thread.start() for i in range(1, 101): download_queue.put(i) download_queue.close() download_queue.join() resize_queue.close() resize_queue.join() upload_queue.close() upload_queue.join() print(out_queue.qsize(), 'pictures finished') # while not out_queue.empty(): # print(out_queue.get())if __name__ == '__main__': main() output: 1234567891011121314......upload item 96resize item 97upload item 97resize item 98download item 99download item 100resize item 99upload item 98upload item 99resize item 100upload item 100100 pictures finished 结束]]></content>
      <categories>
        <category>Python</category>
        <category>进程线程协程</category>
      </categories>
      <tags>
        <tag>Thread</tag>
        <tag>Queue</tag>
        <tag>Producer_Consumer</tag>
        <tag>Pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python进程线程 - subprocess]]></title>
    <url>%2F2018%2F01%2F18%2Fsubprocess%2F</url>
    <content type="text"><![CDATA[python子进程模块subprocess 前言 subprocess模块是python从2.4版本开始引入的模块。主要用来取代 一些旧的模块方法，如os.system、os.spawn、os.popen、commands等。subprocess通过子进程来执行外部指令，并通过input/output/error管道，获取子进程的执行的返回信息。 常用方法subprocess.call()执行命令，并返回执行状态，其中shell参数为False时，命令需要通过列表的方式传入，当shell为True时，可直接传入命令 1234567891011121314151617181920&gt;&gt;&gt; import subprocess&gt;&gt;&gt; child = subprocess.call(['df', '-h'], shell=False)Filesystem Size Used Avail Use% Mounted onudev 475M 0 475M 0% /devtmpfs 99M 2.9M 97M 3% /run/dev/vda1 40G 9.5G 28G 26% /tmpfs 495M 4.0K 495M 1% /dev/shmtmpfs 5.0M 4.0K 5.0M 1% /run/locktmpfs 495M 0 495M 0% /sys/fs/cgrouptmpfs 99M 0 99M 0% /run/user/1000&gt;&gt;&gt; child2 = subprocess.call('df -h', shell=True)Filesystem Size Used Avail Use% Mounted onudev 475M 0 475M 0% /devtmpfs 99M 2.9M 97M 3% /run/dev/vda1 40G 9.5G 28G 26% /tmpfs 495M 4.0K 495M 1% /dev/shmtmpfs 5.0M 4.0K 5.0M 1% /run/locktmpfs 495M 0 495M 0% /sys/fs/cgrouptmpfs 99M 0 99M 0% /run/user/1000 subprocess.check_call()用法与subprocess.call()类似，区别是，当返回值不为0时，直接抛出异常 123456789101112131415161718&gt;&gt;&gt; child3 = subprocess.check_call('df -h', shell=True)Filesystem Size Used Avail Use% Mounted onudev 475M 0 475M 0% /devtmpfs 99M 2.9M 97M 3% /run/dev/vda1 40G 9.5G 28G 26% /tmpfs 495M 4.0K 495M 1% /dev/shmtmpfs 5.0M 4.0K 5.0M 1% /run/locktmpfs 495M 0 495M 0% /sys/fs/cgrouptmpfs 99M 0 99M 0% /run/user/1000&gt;&gt;&gt; print(child3)0&gt;&gt;&gt; child4 = subprocess.check_call('df-h', shell=True)/bin/sh: 1: df-h: not foundTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "/home/cg/anaconda3/lib/python3.6/subprocess.py", line 291, in check_call raise CalledProcessError(retcode, cmd)subprocess.CalledProcessError: Command 'df-h' returned non-zero exit status 127. subprocess.check_output()用法与上面两个方法类似，区别是，如果当返回值为0时，不直接输出结果，如果返回值不为0，直接抛出异常。需要说明的是，该方法在python3.x和python2.7.x中才有。 123&gt;&gt;&gt; child5 = subprocess.check_output('df -h', shell=True)&gt;&gt;&gt; child5b'Filesystem Size Used Avail Use% Mounted on\nudev 475M 0 475M 0% /dev\ntmpfs 99M 2.9M 97M 3% /run\n/dev/vda1 40G 9.5G 28G 26% /\ntmpfs 495M 4.0K 495M 1% /dev/shm\ntmpfs 5.0M 4.0K 5.0M 1% /run/lock\ntmpfs 495M 0 495M 0% /sys/fs/cgroup\ntmpfs 99M 0 99M 0% /run/user/1000\n' subprocess.Popen()在一些复杂场景中，我们需要将一个进程的执行输出作为另一个进程的输入。在另一些场景中，我们需要先进入到某个输入环境，然后再执行一系列的指令等。这个时候我们就需要使用到suprocess的Popen()方法。该方法有以下参数： args：shell命令，可以是字符串，或者序列类型，如list,tuple。 bufsize：缓冲区大小，可不用关心 stdin,stdout,stderr：分别表示程序的标准输入，标准输出及标准错误 shell：与上面方法中用法相同 cwd：用于设置子进程的当前目录 env：用于指定子进程的环境变量。如果env=None，则默认从父进程继承环境变量 universal_newlines：不同系统的的换行符不同，当该参数设定为true时，则表示使用\n作为换行符 示例1：在~/test下创建一个suprocesstest的目录， 以及删除： 1234&gt;&gt;&gt; child6 = subprocess.Popen('mkdir subprocesstest',shell=True,cwd='/home/cg/test')# 查看目录，已经创建该文件夹&gt;&gt;&gt; child7 = subprocess.Popen('rmdir subprocesstest',shell=True,cwd='/home/cg/test')# 查看目录，已经删除该文件夹 示例2: 使用python执行几个命令 12345678910111213141516import subprocessproc = subprocess.Popen(["python"], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)proc.stdin.write('print(1)\n'.encode('utf-8'))proc.stdin.write('print(2)\n'.encode('utf-8'))proc.stdin.write('print(3)\n'.encode('utf-8'))proc.stdin.close()cmd_out = proc.stdout.read()proc.stdout.close()cmd_error = proc.stderr.read()proc.stderr.close()print(cmd_out)print(cmd_error) output: 12b'1\n2\n3\n'b'' 或者使用communicate()方法： 12345678910import subprocessproc = subprocess.Popen(["python"], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)proc.stdin.write('print(1)\n'.encode('utf-8'))proc.stdin.write('print(2)\n'.encode('utf-8'))proc.stdin.write('print(3)\n'.encode('utf-8'))out_err_list = proc.communicate()print(out_err_list) output: 1(b'1\n2\n3\n', b'') #(out,err)元组 示例3: 将一个子进程的输出，作为另一个子进程的输入 12345# 类似于shell的cat /etc/passwd | grep 0:0import subprocesschild1 = subprocess.Popen(["cat","/etc/passwd"], stdout=subprocess.PIPE)child2 = subprocess.Popen(["grep","0:0"],stdin=child1.stdout, stdout=subprocess.PIPE)out = child2.communicate() 其他方法： 123456import subprocesschild = subprocess.Popen('sleep 60',shell=True,stdout=subprocess.PIPE)child.poll() #检查子进程状态child.kill() #终止子进程child.send_signal() #向子进程发送信号child.terminate() #终止子进程 结束]]></content>
      <categories>
        <category>Python</category>
        <category>进程线程协程</category>
      </categories>
      <tags>
        <tag>subprocess</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github + hexo + Next 搭建免费个人博客]]></title>
    <url>%2F2018%2F01%2F12%2Fdeploy-hexo-next%2F</url>
    <content type="text"><![CDATA[that’s have fun with hexo]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>github</tag>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F01%2F10%2Fhello-world%2F</url>
    <content type="text"><![CDATA[这就是一个简介 这里更多的内容 Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
</search>
