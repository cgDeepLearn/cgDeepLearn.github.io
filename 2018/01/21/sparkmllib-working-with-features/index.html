<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">

<script>
    (function(){
        if(''){
            if (prompt('请输入文章密码') !== ''){
                alert('密码错误！');
                history.back();
            }
        }
    })();
</script>


  
  
    
      
    
    
      
    
  <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
  <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-flash.min.css" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="D5-qRqseoFKaMoLLpwgLLeyygQcyL2YH5spNiQ4g9gE" />
















  
  
    
  
  <link href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  

  
    
      
    

    
  

  
    
    
    <link href="http://fonts.useso.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Roboto Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






  

<link href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.css" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="TF-IDF,Word2Vec,OneHotEncoding," />





  <link rel="alternate" href="/atom.xml" title="cgDeepLearn" type="application/atom+xml" />






<meta name="description" content="Extracting, Transforming and Selecting features(特征的提取、转换、选择)   本节介绍用于处理特征的算法，大致分为以下几组：  Extraction(提取)：从“原始”数据中提取特征 Transformation(转换)：缩放，转换或修改特征 Selection(选择)：从一大组特征集中选择一个子集 Locality Sensitive Hashin">
<meta name="keywords" content="TF-IDF,Word2Vec,OneHotEncoding">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkMLlib-Working-with-Features">
<meta property="og:url" content="https://blog.writeathink.cn/2018/01/21/sparkmllib-working-with-features/index.html">
<meta property="og:site_name" content="cgDeepLearn">
<meta property="og:description" content="Extracting, Transforming and Selecting features(特征的提取、转换、选择)   本节介绍用于处理特征的算法，大致分为以下几组：  Extraction(提取)：从“原始”数据中提取特征 Transformation(转换)：缩放，转换或修改特征 Selection(选择)：从一大组特征集中选择一个子集 Locality Sensitive Hashin">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-03-01T09:37:32.859Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SparkMLlib-Working-with-Features">
<meta name="twitter:description" content="Extracting, Transforming and Selecting features(特征的提取、转换、选择)   本节介绍用于处理特征的算法，大致分为以下几组：  Extraction(提取)：从“原始”数据中提取特征 Transformation(转换)：缩放，转换或修改特征 Selection(选择)：从一大组特征集中选择一个子集 Locality Sensitive Hashin">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":true,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '5IB1N2USIP',
      apiKey: '2acaab00a79e34bd9030a923f014cdcb',
      indexName: 'my_github.io_search',
      hits: {"per_page":10},
      labels: {"input_placeholder":"输入关键字","hits_empty":"没有找到与 ${query} 相关的内容","hits_stats":"${hits} ， 共耗时 ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://blog.writeathink.cn/2018/01/21/sparkmllib-working-with-features/"/>





  <title>SparkMLlib-Working-with-Features | cgDeepLearn</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-11315781-1', 'auto');
  ga('send', 'pageview');
</script>





</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    <!--forkmeon github
    <a href="https://github.com/cgDeepLearn"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/652c5b9acfaddf3a9c326fa6bde407b87f7be0f4/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6f72616e67655f6666373630302e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_orange_ff7600.png"></a>
    -->
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">cgDeepLearn</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">More than code</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-top">
          <a href="/top/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-signal"></i> <br />
            
            最热
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://blog.writeathink.cn/2018/01/21/sparkmllib-working-with-features/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="cgDeepLearn">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="cgDeepLearn">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">SparkMLlib-Working-with-Features</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-21T17:17:09+08:00">
                2018-01-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
                  >
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark/MLlib/" itemprop="url" rel="index">
                    <span itemprop="name">MLlib</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/01/21/sparkmllib-working-with-features/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2018/01/21/sparkmllib-working-with-features/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/01/21/sparkmllib-working-with-features/" class="leancloud_visitors" data-flag-title="SparkMLlib-Working-with-Features">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">热度&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
                 <span>℃</span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  13,773
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <script src="/assets/js/APlayer.min.js"> </script><p class="description">Extracting, Transforming and Selecting features(特征的提取、转换、选择)<br></p>

<p><img src="" alt="" style="width:100%"></p>
<p>本节介绍用于处理特征的算法，大致分为以下几组：</p>
<ul>
<li><strong>Extraction(提取)</strong>：从“原始”数据中提取特征</li>
<li><strong>Transformation(转换)</strong>：缩放，转换或修改特征</li>
<li><strong>Selection(选择)</strong>：从一大组特征集中选择一个子集</li>
<li><strong>Locality Sensitive Hashing局部敏感散列（LSH）</strong>：这类算法将特征变换的各个方面与其他算法相结合。</li>
</ul>
<a id="more"></a>
<h2 id="Feature-Extractors"><a href="#Feature-Extractors" class="headerlink" title="Feature Extractors"></a><strong>Feature Extractors</strong></h2><h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a><strong>TF-IDF</strong></h3><p><strong>词频-逆向文档频率（<code>TF-IDF</code>）</strong> 是一种在文本挖掘中广泛使用的特征矢量化方法，用于反映在预料中词语在文档中的重要性。<em>t</em>表示一个词，<em>d</em>表示文档和<em>D</em>表示语料。词频<em>TF(t,d)</em>是词语<em>t</em>在文档<em>d</em>中出现的次数，而文档频率<em>DF(t, D)</em>是语料中文档包含词语<em>t</em>的的数量。如果我们只用词频来衡量重要性，那么很容易过分强调出现频率很高但是却只承载文档极少的信息的词语，例如“a”，“the”和“of”。如果一个词语经常在整个语料库中出现，这意味着对于特定的文档它并没有承载特殊的信息。<strong>逆文档频率是一个词语提供多少信息的数字度量</strong>： </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IDF(t,D) = log(|D|+1)/(DF(t, D)+1)</span><br></pre></td></tr></table></figure>
<p>其中|D|是语料的文档总数。由于使用对数，所以如果一个词语在所有文档中出现，则其IDF值为0.请注意，smoothing term用来避免除以零对于那些在语料外的词语。TF-IDF度量是TF和IDF的乘积：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TFIDF(t,d,D)=TF(t,d)⋅IDF(t,D)</span><br></pre></td></tr></table></figure>
<p>词频和文档频率的定义有几个变体。在MLlib中，我们将TF和IDF分开，使它们更灵活。</p>
<p><code>TF</code>：HashingTF与CountVectorizer都可用于生成词频向量。</p>
<p>HashingTF是一个Transformer，其选取词语的集合并将这些集合转换成固定长度的特征向量。在文本处理中，“set of term”可能是一堆文字。 HashingTF利用哈希技巧(<a href="http://en.wikipedia.org/wiki/Feature_hashing" target="_blank" rel="noopener">hashing trick</a>)。通过应用散列函数(hash function)将原始特征映射成一个索引（term）。这里使用的哈希函数是<a href="https://en.wikipedia.org/wiki/MurmurHash" target="_blank" rel="noopener">MurmurHash 3</a> 。然后根据映射后的索引计算词频。这种方法避免了计算全局term-to-index的映射，而这些计算对于大型语料库来说可能是耗费的，但是它具有潜在的散列冲突，其中不同的原始特征可能在散列之后变成相同的term。为了减少碰撞的几率，我们可以增加目标特征维数，即散列表的桶数(buckets)。<strong>由于使用简单的模来将散列函数转换为列索引，所以建议使用2的幂作为特征维度，否则特征将不会均匀地映射到列</strong>。默认的特征维度是2^18=262144。一个可选的二进制切换参数控制词频计数。当设置为真时，所有非零频率计数都被设置为1.这对于模拟二进制计数而不是整数计数的离散概率模型特别有用。</p>
<p>CountVectorizer将文本文档转换为词条计数的向量。有关更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/ml-features.html#countvectorizer" target="_blank" rel="noopener">CountVectorizer</a> 。</p>
<p><code>IDF</code>：IDF是一个Estimator,被应用于一个数据集并产生一个IDFModel。IDFModel选取特征向量（通常从HashingTF或CountVectorizer创建）并缩放每一列。直观地，它减少了频繁出现在语料库中的列的权重。</p>
<p>Note： spark.ml不提供用于文本分割的工具。我们推荐用户参考<a href="http://nlp.stanford.edu/" target="_blank" rel="noopener">Stanford NLP Grou</a> 和 <a href="https://github.com/scalanlp/chalk" target="_blank" rel="noopener">scalanlp/chalk</a>。</p>
<ul>
<li>Examples</li>
</ul>
<p>在下面的代码段中，我们从一组句子开始。我们使用Tokenizer每个句子分成单词。对于每个句子（词包），我们使用HashingTF把语句散列成一个特征向量。我们用IDF来重新调整特征向量; 使用文本作为特征时，这通常会提高性能。我们的特征向量可以传递给学习算法。</p>
<p>有关API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.HashingTF" target="_blank" rel="noopener">HashingTF Python文档</a>和<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.IDF" target="_blank" rel="noopener">IDF Python文档</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> HashingTF, IDF, Tokenizer</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"TF_IDfExample"</span>).getOrCreate()</span><br><span class="line">sentenceData = spark.createDataFrame([</span><br><span class="line">    (<span class="number">0.0</span>, <span class="string">"Hi I heard about Spark"</span>),</span><br><span class="line">    (<span class="number">0.0</span>, <span class="string">"I wish Java could use case classes"</span>),</span><br><span class="line">    (<span class="number">1.0</span>, <span class="string">"Logistic regression models are neat"</span>)</span><br><span class="line">], [<span class="string">"label"</span>, <span class="string">"sentence"</span>])</span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(inputCol=<span class="string">"sentence"</span>, outputCol=<span class="string">"words"</span>)</span><br><span class="line">wordsData = tokenizer.transform(sentenceData)</span><br><span class="line"></span><br><span class="line">hashingTF = HashingTF(inputCol=<span class="string">"words"</span>, outputCol=<span class="string">"rawFeatures"</span>, numFeatures=<span class="number">20</span>)</span><br><span class="line">featurizedData = hashingTF.transform(wordsData)</span><br><span class="line"><span class="comment"># alternatively, CountVectorizer can also be used to get term frequency vectors</span></span><br><span class="line"></span><br><span class="line">idf = IDF(inputCol=<span class="string">"rawFeatures"</span>, outputCol=<span class="string">"features"</span>)</span><br><span class="line">idfModel = idf.fit(featurizedData)</span><br><span class="line">rescaledData = idfModel.transform(featurizedData)</span><br><span class="line"></span><br><span class="line">rescaledData.select(<span class="string">"label"</span>, <span class="string">"features"</span>).show()</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>
<p>output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">|label|            features|</span><br><span class="line">|-----|--------------------|</span><br><span class="line">|  <span class="number">0.0</span>|(<span class="number">20</span>,[<span class="number">0</span>,<span class="number">5</span>,<span class="number">9</span>,<span class="number">17</span>],[<span class="number">0.</span>..|</span><br><span class="line">|  <span class="number">0.0</span>|(<span class="number">20</span>,[<span class="number">2</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">13</span>,<span class="number">15</span>]...|</span><br><span class="line">|  <span class="number">1.0</span>|(<span class="number">20</span>,[<span class="number">4</span>,<span class="number">6</span>,<span class="number">13</span>,<span class="number">15</span>,<span class="number">18.</span>..|</span><br></pre></td></tr></table></figure>
<p>Find full example code at “examples/src/main/python/ml/tf_idf_example.py” in the Spark repo.</p>
<h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a><strong>Word2Vec</strong></h3><p><code>Word2Vec</code>是一个<code>Estimator</code>,它选取表征文件的单词序列(句子)来训练一个Word2VecModel。模型将每个单词映射到一个唯一的固定大小的向量vector。Word2VecModel 用所有单词在文档中的平均值将每个文档转换为一个向量vector; 然后这个vector可以用作预测，文档相似度计算等功能。请参阅<a href="https://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec" target="_blank" rel="noopener">Word2Vec MLlib用户指南</a>了解更多详细信息。</p>
<ul>
<li>Examples</li>
</ul>
<p>在下面的代码段中，我们从一组文档开始，每个文档都被表示为一个单词序列。对于每个文档，我们把它转换成一个特征向量。这个特征向量可以传递给一个学习算法。</p>
<p>有关API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Word2Vec" target="_blank" rel="noopener">Word2Vec Python文档</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> Word2Vec</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"Word2VecExample"</span>).getOrCreate()</span><br><span class="line"><span class="comment"># Input data: Each row is a bag of words from a sentence or document.</span></span><br><span class="line">documentDF = spark.createDataFrame([</span><br><span class="line">    (<span class="string">"Hi I heard about Spark"</span>.split(<span class="string">" "</span>), ),</span><br><span class="line">    (<span class="string">"I wish Java could use case classes"</span>.split(<span class="string">" "</span>), ),</span><br><span class="line">    (<span class="string">"Logistic regression models are neat"</span>.split(<span class="string">" "</span>), )</span><br><span class="line">], [<span class="string">"text"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Learn a mapping from words to Vectors.</span></span><br><span class="line">word2Vec = Word2Vec(vectorSize=<span class="number">3</span>, minCount=<span class="number">0</span>, inputCol=<span class="string">"text"</span>, outputCol=<span class="string">"result"</span>)</span><br><span class="line">model = word2Vec.fit(documentDF)</span><br><span class="line"></span><br><span class="line">result = model.transform(documentDF)</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> result.collect():</span><br><span class="line">    text, vector = row</span><br><span class="line">    print(<span class="string">"Text: [%s] =&gt; \nVector: %s\n"</span> % (<span class="string">", "</span>.join(text), str(vector)))</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>
<p>output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Text: [Hi, I, heard, about, Spark] =&gt; </span><br><span class="line">Vector: [<span class="number">0.0334007266909</span>,<span class="number">0.00213784053922</span>,<span class="number">-0.00239131785929</span>]</span><br><span class="line"></span><br><span class="line">Text: [I, wish, Java, could, use, case, classes] =&gt; </span><br><span class="line">Vector: [<span class="number">0.0464252099129</span>,<span class="number">0.0357359477452</span>,<span class="number">-0.000244158453175</span>]</span><br><span class="line"></span><br><span class="line">Text: [Logistic, regression, models, are, neat] =&gt; </span><br><span class="line">Vector: [<span class="number">-0.00983053222299</span>,<span class="number">0.0668786892667</span>,<span class="number">-0.0307074898912</span>]</span><br><span class="line">---</span><br></pre></td></tr></table></figure>
<p>Find full example code at “examples/src/main/python/ml/word2vec_example.py” in the Spark repo.</p>
<h3 id="CountVectorizer"><a href="#CountVectorizer" class="headerlink" title="CountVectorizer"></a><strong>CountVectorizer</strong></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在拟合过程中，CountVectorizer将选择整个语料库中词频排在前面vocabSize个的词汇。一个可选参数minDF也会影响拟合过程，方法是指定词汇必须出现的文档的最小数量（或小于1.0）。另一个可选的二进制切换参数控制输出向量。如果设置为true，则所有非零计数都设置为1.这对于模拟二进制计数而不是整数计数的离散概率模型特别有用。</span><br><span class="line"></span><br><span class="line">- Examples</span><br><span class="line"></span><br><span class="line">假设我们有列如下数据帧，有id和texts列：</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"></span><br><span class="line"> id | texts</span><br><span class="line">----|----------</span><br><span class="line"> 0  | Array(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)</span><br><span class="line"> 1  | Array(&quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;)</span><br></pre></td></tr></table></figure>
<p>texts列中的每一行都是一个Array [String]类型的文档。调用CountVectorizer产生CountVectorizerModel与词汇（a，b，c）。然后转换后的输出列“向量”包含：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"> id | texts                           | vector</span><br><span class="line">----|---------------------------------|---------------</span><br><span class="line"> <span class="number">0</span>  | Array(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>)            | (<span class="number">3</span>,[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>,<span class="number">1.0</span>])</span><br><span class="line"> <span class="number">1</span>  | Array(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"a"</span>)  | (<span class="number">3</span>,[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2.0</span>,<span class="number">2.0</span>,<span class="number">1.0</span>])</span><br></pre></td></tr></table></figure>
<p>每个向量表示文档在词汇表上的标记计数。</p>
<p>有关API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.CountVectorizer" target="_blank" rel="noopener">CountVectorizer Python文档</a> 和<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.CountVectorizerModel" target="_blank" rel="noopener">CountVectorizerModel Python文档</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"CountVectorizerExample"</span>).getOrCreate()</span><br><span class="line"><span class="comment"># Input data: Each row is a bag of words with a ID.</span></span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">0</span>, <span class="string">"a b c"</span>.split(<span class="string">" "</span>)),</span><br><span class="line">    (<span class="number">1</span>, <span class="string">"a b b c a"</span>.split(<span class="string">" "</span>))</span><br><span class="line">], [<span class="string">"id"</span>, <span class="string">"words"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># fit a CountVectorizerModel from the corpus.</span></span><br><span class="line">cv = CountVectorizer(inputCol=<span class="string">"words"</span>, outputCol=<span class="string">"features"</span>, vocabSize=<span class="number">3</span>, minDF=<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line">model = cv.fit(df)</span><br><span class="line"></span><br><span class="line">result = model.transform(df)</span><br><span class="line">result.show(truncate=<span class="keyword">False</span>)</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>
<p>output:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+---+---------------+-------------------------+</span><br><span class="line">|id |words          |features                 |</span><br><span class="line">+---+---------------+-------------------------+</span><br><span class="line">|<span class="number">0</span>  |[a, b, c]      |(<span class="number">3</span>,[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>,<span class="number">1.0</span>])|</span><br><span class="line">|<span class="number">1</span>  |[a, b, b, c, a]|(<span class="number">3</span>,[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2.0</span>,<span class="number">2.0</span>,<span class="number">1.0</span>])|</span><br><span class="line">+---+---------------+-------------------------+</span><br></pre></td></tr></table></figure>
<p>Find full example code at “examples/src/main/python/ml/count_vectorizer_example.py” in the Spark repo.</p>
<h2 id="Feature-Transformers"><a href="#Feature-Transformers" class="headerlink" title="Feature Transformers"></a><strong>Feature Transformers</strong></h2><h3 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a><strong>Tokenizer</strong></h3><p><a href="http://en.wikipedia.org/wiki/Lexical_analysis#Tokenization" target="_blank" rel="noopener">Tokenization</a>(分词)是将文本（如句子）分解成单个词（通常是单词）的过程。一个简单的<a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Tokenizer" target="_blank" rel="noopener">Tokenizer</a>类提供了这个功能。下面的例子展示了如何将句子拆分成单词序列。</p>
<p><a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.RegexTokenizer" target="_blank" rel="noopener">RegexTokenizer</a>允许更高级的基于正则表达式（正则表达式）匹配的分词。默认情况下，使用参数“pattern”（正则表达式，默认：”\s+”）作为分隔符来分割输入文本。或者，用户可以将参数“gaps”设置为false，指示正则表达式“pattern”而不是分割间隙来表示“tokens”，并查找所有匹配事件作为分词结果。</p>
<ul>
<li>Examples</li>
</ul>
<p>有关API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Tokenizer" target="_blank" rel="noopener">Tokenizer Python文档</a>和<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.RegexTokenizer" target="_blank" rel="noopener">RegexTokenizer Python文档</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> Tokenizer, RegexTokenizer</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> col, udf</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> IntegerType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"TokenizerExample"</span>).getOrCreate()</span><br><span class="line">sentenceDataFrame = spark.createDataFrame([</span><br><span class="line">    (<span class="number">0</span>, <span class="string">"Hi I heard about Spark"</span>),</span><br><span class="line">    (<span class="number">1</span>, <span class="string">"I wish Java could use case classes"</span>),</span><br><span class="line">    (<span class="number">2</span>, <span class="string">"Logistic,regression,models,are,neat"</span>)</span><br><span class="line">], [<span class="string">"id"</span>, <span class="string">"sentence"</span>])</span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(inputCol=<span class="string">"sentence"</span>, outputCol=<span class="string">"words"</span>)</span><br><span class="line"></span><br><span class="line">regexTokenizer = RegexTokenizer(inputCol=<span class="string">"sentence"</span>, outputCol=<span class="string">"words"</span>, pattern=<span class="string">"\\W"</span>)</span><br><span class="line"><span class="comment"># alternatively, pattern="\\w+", gaps(False)</span></span><br><span class="line"></span><br><span class="line">countTokens = udf(<span class="keyword">lambda</span> words: len(words), IntegerType())</span><br><span class="line"></span><br><span class="line">tokenized = tokenizer.transform(sentenceDataFrame)</span><br><span class="line">tokenized.select(<span class="string">"sentence"</span>, <span class="string">"words"</span>)\</span><br><span class="line">    .withColumn(<span class="string">"tokens"</span>, countTokens(col(<span class="string">"words"</span>))).show(truncate=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">regexTokenized = regexTokenizer.transform(sentenceDataFrame)</span><br><span class="line">regexTokenized.select(<span class="string">"sentence"</span>, <span class="string">"words"</span>) \</span><br><span class="line">    .withColumn(<span class="string">"tokens"</span>, countTokens(col(<span class="string">"words"</span>))).show(truncate=<span class="keyword">False</span>)</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+-----------------------------------+------------------------------------------+------+</span><br><span class="line">|sentence                           |words                                     |tokens|</span><br><span class="line">+-----------------------------------+------------------------------------------+------+</span><br><span class="line">|Hi I heard about Spark             |[hi, i, heard, about, spark]              |<span class="number">5</span>     |</span><br><span class="line">|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|<span class="number">7</span>     |</span><br><span class="line">|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |<span class="number">1</span>     |</span><br><span class="line">+-----------------------------------+------------------------------------------+------+</span><br><span class="line"></span><br><span class="line">+-----------------------------------+------------------------------------------+------+</span><br><span class="line">|sentence                           |words                                     |tokens|</span><br><span class="line">+-----------------------------------+------------------------------------------+------+</span><br><span class="line">|Hi I heard about Spark             |[hi, i, heard, about, spark]              |<span class="number">5</span>     |</span><br><span class="line">|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|<span class="number">7</span>     |</span><br><span class="line">|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |<span class="number">5</span>     |</span><br><span class="line">+-----------------------------------+------------------------------------------+------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/tokenizer_example.py” in the Spark repo.</p>
<h3 id="StopWordsRemover"><a href="#StopWordsRemover" class="headerlink" title="StopWordsRemover"></a><strong>StopWordsRemover</strong></h3><p><a href="https://en.wikipedia.org/wiki/Stop_words" target="_blank" rel="noopener">Stop words</a>(停止词)是应该从输入中排除的词，通常是因为这些词经常出现而又不具有如此多的含义。</p>
<p>StopWordsRemover将一串字符串（例如一个Tokenizer的输出）作为输入，并从输入序列中删除所有的停止词。停用词表由stopWords参数指定。某些语言的默认停用词可通过调用访问StopWordsRemover.loadDefaultStopWords(language)，可用的选项有“danish”, “dutch”, “english”, “finnish”, “french”, “german”, “hungarian”, “italian”, “norwegian”, “portuguese”, “russian”, “spanish”, “swedish” and “turkish”。布尔参数caseSensitive指示匹配是否区分大小写（默认为false）。</p>
<ul>
<li>Examples</li>
</ul>
<p>假设我们有列如下数据帧,拥有列id和raw：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"> id | raw</span><br><span class="line">----|----------</span><br><span class="line"> <span class="number">0</span>  | [I, saw, the, red, baloon]</span><br><span class="line"> <span class="number">1</span>  | [Mary, had, a, little, lamb]</span><br></pre></td></tr></table></figure></p>
<p>应用StopWordsRemover与raw作为输入列，filtered作为输出列，我们应该得到以下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"> id | raw                         | filtered</span><br><span class="line">----|-----------------------------|--------------------</span><br><span class="line"> <span class="number">0</span>  | [I, saw, the, red, baloon]  |  [saw, red, baloon]</span><br><span class="line"> <span class="number">1</span>  | [Mary, had, a, little, lamb]|[Mary, little, lamb]</span><br></pre></td></tr></table></figure></p>
<p>在这里filtered，“I”，“the”，“had”和“a”这些停用词语已被滤除。\<br>有关API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover" target="_blank" rel="noopener">StopWordsRemover Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> StopWordsRemover</span><br><span class="line"></span><br><span class="line">sentenceData = spark.createDataFrame([</span><br><span class="line">    (<span class="number">0</span>, [<span class="string">"I"</span>, <span class="string">"saw"</span>, <span class="string">"the"</span>, <span class="string">"red"</span>, <span class="string">"balloon"</span>]),</span><br><span class="line">    (<span class="number">1</span>, [<span class="string">"Mary"</span>, <span class="string">"had"</span>, <span class="string">"a"</span>, <span class="string">"little"</span>, <span class="string">"lamb"</span>])</span><br><span class="line">], [<span class="string">"id"</span>, <span class="string">"raw"</span>])</span><br><span class="line"></span><br><span class="line">remover = StopWordsRemover(inputCol=<span class="string">"raw"</span>, outputCol=<span class="string">"filtered"</span>)</span><br><span class="line">remover.transform(sentenceData).show(truncate=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/stopwords_remover_example.py” in the Spark repo.</p>
<h3 id="n-gram"><a href="#n-gram" class="headerlink" title="n-gram"></a><strong>n-gram</strong></h3><p>一个<a href="https://en.wikipedia.org/wiki/N-gram" target="_blank" rel="noopener">n-gram</a>是一个包含整数n个tokens（通常是单词）的序列。NGram类可用于输入特征转变成n-grams。</p>
<p>NGram将一串字符串（例如一个Tokenizer的输出）作为输入。参数n用于确定每个n-gram中的terms的数量。输出将由n-grams的序列组成，每个n-gram由空格分隔的n个连续的words的字符串表示。如果输入序列少于n，则没有输出。</p>
<ul>
<li>Examples</li>
</ul>
<p>有关API的更多细节，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.NGram" target="_blank" rel="noopener">NGram Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> NGram</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"n_gramExample"</span>).getOrCreate()</span><br><span class="line">wordDataFrame = spark.createDataFrame([</span><br><span class="line">    (<span class="number">0</span>, [<span class="string">"Hi"</span>, <span class="string">"I"</span>, <span class="string">"heard"</span>, <span class="string">"about"</span>, <span class="string">"Spark"</span>]),</span><br><span class="line">    (<span class="number">1</span>, [<span class="string">"I"</span>, <span class="string">"wish"</span>, <span class="string">"Java"</span>, <span class="string">"could"</span>, <span class="string">"use"</span>, <span class="string">"case"</span>, <span class="string">"classes"</span>]),</span><br><span class="line">    (<span class="number">2</span>, [<span class="string">"Logistic"</span>, <span class="string">"regression"</span>, <span class="string">"models"</span>, <span class="string">"are"</span>, <span class="string">"neat"</span>])</span><br><span class="line">], [<span class="string">"id"</span>, <span class="string">"words"</span>])</span><br><span class="line"></span><br><span class="line">ngram = NGram(n=<span class="number">2</span>, inputCol=<span class="string">"words"</span>, outputCol=<span class="string">"ngrams"</span>)</span><br><span class="line"></span><br><span class="line">ngramDataFrame = ngram.transform(wordDataFrame)</span><br><span class="line">ngramDataFrame.select(<span class="string">"ngrams"</span>).show(truncate=<span class="keyword">False</span>)</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+------------------------------------------------------------------+</span><br><span class="line">|ngrams                                                            |</span><br><span class="line">+------------------------------------------------------------------+</span><br><span class="line">|[Hi I, I heard, heard about, about Spark]                         |</span><br><span class="line">|[I wish, wish Java, Java could, could use, use case, case classes]|</span><br><span class="line">|[Logistic regression, regression models, models are, are neat]    |</span><br><span class="line">+------------------------------------------------------------------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/n_gram_example.py” in the Spark repo.</p>
<h3 id="Binarizer"><a href="#Binarizer" class="headerlink" title="Binarizer"></a><strong>Binarizer</strong></h3><p><code>Binarization</code>(二值化)是将数字特征阈值化为二进制（0/1）特征的过程。</p>
<p>Binarizer需传入参数inputCol和outputCol，以及所述threshold参数来进行二值化。大于阈值的特征值被二进制化为1.0; 等于或小于阈值的值被二值化为0.0。inputCol支持Vector和Double类型。</p>
<ul>
<li>Examples</li>
</ul>
<p>有关API的更多细节，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Binarizer" target="_blank" rel="noopener">Binarizer Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> Binarizer</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession  </span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"BinarizerExample"</span>).getOrCreate()</span><br><span class="line">continuousDataFrame = spark.createDataFrame([</span><br><span class="line">    (<span class="number">0</span>, <span class="number">0.1</span>),</span><br><span class="line">    (<span class="number">1</span>, <span class="number">0.8</span>),</span><br><span class="line">    (<span class="number">2</span>, <span class="number">0.2</span>)</span><br><span class="line">], [<span class="string">"id"</span>, <span class="string">"feature"</span>])</span><br><span class="line"></span><br><span class="line">binarizer = Binarizer(threshold=<span class="number">0.5</span>, inputCol=<span class="string">"feature"</span>, outputCol=<span class="string">"binarized_feature"</span>)</span><br><span class="line"></span><br><span class="line">binarizedDataFrame = binarizer.transform(continuousDataFrame)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Binarizer output with Threshold = %f"</span> % binarizer.getThreshold())</span><br><span class="line">binarizedDataFrame.show()</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Binarizer output <span class="keyword">with</span> Threshold = <span class="number">0.500000</span></span><br><span class="line">+---+-------+-----------------+</span><br><span class="line">| id|feature|binarized_feature|</span><br><span class="line">+---+-------+-----------------+</span><br><span class="line">|  <span class="number">0</span>|    <span class="number">0.1</span>|              <span class="number">0.0</span>|</span><br><span class="line">|  <span class="number">1</span>|    <span class="number">0.8</span>|              <span class="number">1.0</span>|</span><br><span class="line">|  <span class="number">2</span>|    <span class="number">0.2</span>|              <span class="number">0.0</span>|</span><br><span class="line">+---+-------+-----------------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/binarizer_example.py” in the Spark repo.</p>
<h3 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a><strong>PCA</strong></h3><p><a href="http://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank" rel="noopener">PCA</a>是一个统计过程，它使用正交变换将一组可能相关的变量的观察值转换成一组称为主成分的线性不相关变量的值。一个PCA类使用PCA将向量映射到低维空间来训练一个模型。下面的例子显示了如何将五维特征向量投影到三维主成分中。</p>
<ul>
<li>Examples</li>
</ul>
<p>有关API的更多细节，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.PCA" target="_blank" rel="noopener">PCA Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession  </span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"PCA_Example"</span>).getOrCreate()</span><br><span class="line">data = [(Vectors.sparse(<span class="number">5</span>, [(<span class="number">1</span>, <span class="number">1.0</span>), (<span class="number">3</span>, <span class="number">7.0</span>)]),),</span><br><span class="line">        (Vectors.dense([<span class="number">2.0</span>, <span class="number">0.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>]),),</span><br><span class="line">        (Vectors.dense([<span class="number">4.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">6.0</span>, <span class="number">7.0</span>]),)]</span><br><span class="line">df = spark.createDataFrame(data, [<span class="string">"features"</span>])</span><br><span class="line"></span><br><span class="line">pca = PCA(k=<span class="number">3</span>, inputCol=<span class="string">"features"</span>, outputCol=<span class="string">"pcaFeatures"</span>)</span><br><span class="line">model = pca.fit(df)</span><br><span class="line"></span><br><span class="line">result = model.transform(df).select(<span class="string">"pcaFeatures"</span>)</span><br><span class="line">result.show(truncate=<span class="keyword">False</span>)</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+-----------------------------------------------------------+</span><br><span class="line">|pcaFeatures                                                |</span><br><span class="line">+-----------------------------------------------------------+</span><br><span class="line">|[<span class="number">1.6485728230883807</span>,<span class="number">-4.013282700516296</span>,<span class="number">-5.524543751369388</span>] |</span><br><span class="line">|[<span class="number">-4.645104331781534</span>,<span class="number">-1.1167972663619026</span>,<span class="number">-5.524543751369387</span>]|</span><br><span class="line">|[<span class="number">-6.428880535676489</span>,<span class="number">-5.337951427775355</span>,<span class="number">-5.524543751369389</span>] |</span><br><span class="line">+-----------------------------------------------------------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/pca_example.py” in the Spark repo.</p>
<h3 id="PolynomialExpansion"><a href="#PolynomialExpansion" class="headerlink" title="PolynomialExpansion"></a><strong>PolynomialExpansion</strong></h3><p><a href="http://en.wikipedia.org/wiki/Polynomial_expansion" target="_blank" rel="noopener">Polynomial expansion</a>(多项式展开)是将特征扩展到一个多项式空间的过程，这个多项式空间是由原始维度的n-degree组合形成的。一个<a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.PolynomialExpansion" target="_blank" rel="noopener">PolynomialExpansion</a>类提供此功能。下面的例子展示了如何将特征扩展到一个三次多项式空间。</p>
<ul>
<li>Examples</li>
</ul>
<p>有关API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.PolynomialExpansion" target="_blank" rel="noopener">PolynomialExpansion Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> PolynomialExpansion</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"PolynormialExpansionExample"</span>).getOrCreate()</span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (Vectors.dense([<span class="number">2.0</span>, <span class="number">1.0</span>]),),</span><br><span class="line">    (Vectors.dense([<span class="number">0.0</span>, <span class="number">0.0</span>]),),</span><br><span class="line">    (Vectors.dense([<span class="number">3.0</span>, <span class="number">-1.0</span>]),)</span><br><span class="line">], [<span class="string">"features"</span>])</span><br><span class="line"></span><br><span class="line">polyExpansion = PolynomialExpansion(degree=<span class="number">3</span>, inputCol=<span class="string">"features"</span>, outputCol=<span class="string">"polyFeatures"</span>)</span><br><span class="line">polyDF = polyExpansion.transform(df)</span><br><span class="line"></span><br><span class="line">polyDF.show(truncate=<span class="keyword">False</span>)</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+----------+------------------------------------------+</span><br><span class="line">|features  |polyFeatures                              |</span><br><span class="line">+----------+------------------------------------------+</span><br><span class="line">|[<span class="number">2.0</span>,<span class="number">1.0</span>] |[<span class="number">2.0</span>,<span class="number">4.0</span>,<span class="number">8.0</span>,<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">4.0</span>,<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">1.0</span>]     |</span><br><span class="line">|[<span class="number">0.0</span>,<span class="number">0.0</span>] |[<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.0</span>]     |</span><br><span class="line">|[<span class="number">3.0</span>,<span class="number">-1.0</span>]|[<span class="number">3.0</span>,<span class="number">9.0</span>,<span class="number">27.0</span>,<span class="number">-1.0</span>,<span class="number">-3.0</span>,<span class="number">-9.0</span>,<span class="number">1.0</span>,<span class="number">3.0</span>,<span class="number">-1.0</span>]|</span><br><span class="line">+----------+------------------------------------------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/polynomial_expansion_example.py” in the Spark repo.</p>
<h3 id="Discrete-Cosine-Transform-DCT"><a href="#Discrete-Cosine-Transform-DCT" class="headerlink" title="Discrete Cosine Transform(DCT)"></a><strong>Discrete Cosine Transform(DCT)</strong></h3><p><a href="https://en.wikipedia.org/wiki/Discrete_cosine_transform" target="_blank" rel="noopener">Discrete Cosine Transform</a>离散余弦变换将时域中的长度为N的实数序列转换为另一个频域中长度为N的实数序列。一个<a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.DCT" target="_blank" rel="noopener">DCT</a>类提供此功能，实现 <a href="https://en.wikipedia.org/wiki/Discrete_cosine_transform#DCT-II" target="_blank" rel="noopener">DCT-II</a> 和通过缩放结果1/sqrt(2)倍使得变换的表示矩阵是单一的。被应用于变换的序列是无偏移的（例如变换的序列的第0th个元素是 第0th 个DCT系数而不是N/2个）。</p>
<ul>
<li>Examples</li>
</ul>
<p>有关API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.DCT" target="_blank" rel="noopener">DCT Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> DCT</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"DCT_Example"</span>).getOrCreate()</span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (Vectors.dense([<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">-2.0</span>, <span class="number">3.0</span>]),),</span><br><span class="line">    (Vectors.dense([<span class="number">-1.0</span>, <span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">-7.0</span>]),),</span><br><span class="line">    (Vectors.dense([<span class="number">14.0</span>, <span class="number">-2.0</span>, <span class="number">-5.0</span>, <span class="number">1.0</span>]),)], [<span class="string">"features"</span>])</span><br><span class="line"></span><br><span class="line">dct = DCT(inverse=<span class="keyword">False</span>, inputCol=<span class="string">"features"</span>, outputCol=<span class="string">"featuresDCT"</span>)</span><br><span class="line"></span><br><span class="line">dctDf = dct.transform(df)</span><br><span class="line"></span><br><span class="line">dctDf.select(<span class="string">"featuresDCT"</span>).show(truncate=<span class="keyword">False</span>)</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+----------------------------------------------------------------+</span><br><span class="line">|featuresDCT                                                     |</span><br><span class="line">+----------------------------------------------------------------+</span><br><span class="line">|[<span class="number">1.0</span>,<span class="number">-1.1480502970952693</span>,<span class="number">2.0000000000000004</span>,<span class="number">-2.7716385975338604</span>]|</span><br><span class="line">|[<span class="number">-1.0</span>,<span class="number">3.378492794482933</span>,<span class="number">-7.000000000000001</span>,<span class="number">2.9301512653149677</span>]  |</span><br><span class="line">|[<span class="number">4.0</span>,<span class="number">9.304453421915744</span>,<span class="number">11.000000000000002</span>,<span class="number">1.5579302036357163</span>]   |</span><br><span class="line">+----------------------------------------------------------------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/dct_example.py” in the Spark repo.</p>
<h3 id="StringIndexer"><a href="#StringIndexer" class="headerlink" title="StringIndexer"></a><strong>StringIndexer</strong></h3><p><code>StringIndexer</code>将一串字符串标签编码为标签索引。这些索引范围为[0, numLabels)按照标签频率排序，因此最频繁的标签获得索引0。对于unseen的标签如果用户选择保留它们，它们将被放在索引numLabels处。如果输入列是数字，我们将其转换为字符串值并将其索引。当下游管道组件（例如Estimator或 Transformer）使用此字符串索引标签时，必须将组件的输入列设置为此字符串索引列名称。在许多情况下，您可以使用setInputCol设置输入列</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> StringIndexer</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession  </span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"StringIndexerExample"</span>).getOrCreate()</span><br><span class="line">df = spark.createDataFrame(</span><br><span class="line">    [(<span class="number">0</span>, <span class="string">"a"</span>), (<span class="number">1</span>, <span class="string">"b"</span>), (<span class="number">2</span>, <span class="string">"c"</span>), (<span class="number">3</span>, <span class="string">"a"</span>), (<span class="number">4</span>, <span class="string">"a"</span>), (<span class="number">5</span>, <span class="string">"c"</span>)],</span><br><span class="line">    [<span class="string">"id"</span>, <span class="string">"category"</span>])</span><br><span class="line"></span><br><span class="line">indexer = StringIndexer(inputCol=<span class="string">"category"</span>, outputCol=<span class="string">"categoryIndex"</span>)</span><br><span class="line">indexed = indexer.fit(df).transform(df)</span><br><span class="line">indexed.show()</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+---+--------+-------------+</span><br><span class="line">| id|category|categoryIndex|</span><br><span class="line">+---+--------+-------------+</span><br><span class="line">|  <span class="number">0</span>|       a|          <span class="number">0.0</span>|</span><br><span class="line">|  <span class="number">1</span>|       b|          <span class="number">2.0</span>|</span><br><span class="line">|  <span class="number">2</span>|       c|          <span class="number">1.0</span>|</span><br><span class="line">|  <span class="number">3</span>|       a|          <span class="number">0.0</span>|</span><br><span class="line">|  <span class="number">4</span>|       a|          <span class="number">0.0</span>|</span><br><span class="line">|  <span class="number">5</span>|       c|          <span class="number">1.0</span>|</span><br><span class="line">+---+--------+-------------+</span><br></pre></td></tr></table></figure></p>
<p>此外，StringIndexer处理看不见的标签还有三个策略：</p>
<ol>
<li>抛出一个异常(默认的)</li>
<li>完全跳过unseen标签的行</li>
<li>把一个unseen的标签放在一个特殊的额外桶里，在索引numLabels处</li>
</ol>
<p>让我们回到之前的例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"> id | category</span><br><span class="line">----|----------</span><br><span class="line"> <span class="number">0</span>  | a</span><br><span class="line"> <span class="number">1</span>  | b</span><br><span class="line"> <span class="number">2</span>  | c</span><br><span class="line"> <span class="number">3</span>  | d</span><br><span class="line"> <span class="number">4</span>  | e</span><br></pre></td></tr></table></figure></p>
<p>如果你没有设置如何StringIndexer处理看不见的标签或将其设置为“错误”，则会抛出异常。但是，如果您已经调用setHandleInvalid(“skip”)，则会生成以下数据集：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"> id | category | categoryIndex</span><br><span class="line">----|----------|---------------</span><br><span class="line"> <span class="number">0</span>  | a        | <span class="number">0.0</span></span><br><span class="line"> <span class="number">1</span>  | b        | <span class="number">2.0</span></span><br><span class="line"> <span class="number">2</span>  | c        | <span class="number">1.0</span></span><br></pre></td></tr></table></figure></p>
<p>请注意，包含“d”或“e”的行不显示。</p>
<p>如果你调用setHandleInvalid(“keep”)，将生成以下数据集：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"> id | category | categoryIndex</span><br><span class="line">----|----------|---------------</span><br><span class="line"> <span class="number">0</span>  | a        | <span class="number">0.0</span></span><br><span class="line"> <span class="number">1</span>  | b        | <span class="number">2.0</span></span><br><span class="line"> <span class="number">2</span>  | c        | <span class="number">1.0</span></span><br><span class="line"> <span class="number">3</span>  | d        | <span class="number">3.0</span></span><br><span class="line"> <span class="number">4</span>  | e        | <span class="number">3.0</span></span><br><span class="line"> <span class="comment"># d,e 所在的被映射到索引“3.0”</span></span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/string_indexer_example.py” in the Spark repo.</p>
<h3 id="IndexToString"><a href="#IndexToString" class="headerlink" title="IndexToString"></a><strong>IndexToString</strong></h3><p>对应于StringIndexer，IndexToString将一列标签索引映射回包含作为字符串的原始标签的列。一个常见的用例是从StringIndexer标签生成索引，用这些索引对模型进行训练，并从预测IndexToString索引列中检索原始标签。然而，你也可以提供自己的标签。</p>
<ul>
<li>Examples</li>
</ul>
<p>构造tringIndexer例子，假设我们有一个如下的数据帧，其有id和categoryIndex列：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"> id | categoryIndex</span><br><span class="line">----|---------------</span><br><span class="line"> <span class="number">0</span>  | <span class="number">0.0</span></span><br><span class="line"> <span class="number">1</span>  | <span class="number">2.0</span></span><br><span class="line"> <span class="number">2</span>  | <span class="number">1.0</span></span><br><span class="line"> <span class="number">3</span>  | <span class="number">0.0</span></span><br><span class="line"> <span class="number">4</span>  | <span class="number">0.0</span></span><br><span class="line"> <span class="number">5</span>  | <span class="number">1.0</span></span><br></pre></td></tr></table></figure></p>
<p>将categoryIndex作为输入列，应用IndexToString， originalCategory作为输出列，我们能够检索我们的原始标签（他们将从列的元数据推断）：</p>
<p>有关API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.IndexToString" target="_blank" rel="noopener">IndexToString Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> IndexToString, StringIndexer</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession  </span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"IndexToStringExample"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">df = spark.createDataFrame(</span><br><span class="line">    [(<span class="number">0</span>, <span class="string">"a"</span>), (<span class="number">1</span>, <span class="string">"b"</span>), (<span class="number">2</span>, <span class="string">"c"</span>), (<span class="number">3</span>, <span class="string">"a"</span>), (<span class="number">4</span>, <span class="string">"a"</span>), (<span class="number">5</span>, <span class="string">"c"</span>)],</span><br><span class="line">    [<span class="string">"id"</span>, <span class="string">"category"</span>])</span><br><span class="line"></span><br><span class="line">indexer = StringIndexer(inputCol=<span class="string">"category"</span>, outputCol=<span class="string">"categoryIndex"</span>)</span><br><span class="line">model = indexer.fit(df)</span><br><span class="line">indexed = model.transform(df)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Transformed string column '%s' to indexed column '%s'"</span></span><br><span class="line">      % (indexer.getInputCol(), indexer.getOutputCol()))</span><br><span class="line">indexed.show()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"StringIndexer will store labels in output column metadata\n"</span>)</span><br><span class="line"></span><br><span class="line">converter = IndexToString(inputCol=<span class="string">"categoryIndex"</span>, outputCol=<span class="string">"originalCategory"</span>)</span><br><span class="line">converted = converter.transform(indexed)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Transformed indexed column '%s' back to original string column '%s' using "</span></span><br><span class="line">      <span class="string">"labels in metadata"</span> % (converter.getInputCol(), converter.getOutputCol()))</span><br><span class="line">converted.select(<span class="string">"id"</span>, <span class="string">"categoryIndex"</span>, <span class="string">"originalCategory"</span>).show()</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Transformed string column <span class="string">'category'</span> to indexed column <span class="string">'categoryIndex'</span></span><br><span class="line">+---+--------+-------------+</span><br><span class="line">| id|category|categoryIndex|</span><br><span class="line">+---+--------+-------------+</span><br><span class="line">|  <span class="number">0</span>|       a|          <span class="number">0.0</span>|</span><br><span class="line">|  <span class="number">1</span>|       b|          <span class="number">2.0</span>|</span><br><span class="line">|  <span class="number">2</span>|       c|          <span class="number">1.0</span>|</span><br><span class="line">|  <span class="number">3</span>|       a|          <span class="number">0.0</span>|</span><br><span class="line">|  <span class="number">4</span>|       a|          <span class="number">0.0</span>|</span><br><span class="line">|  <span class="number">5</span>|       c|          <span class="number">1.0</span>|</span><br><span class="line">+---+--------+-------------+</span><br><span class="line"></span><br><span class="line">StringIndexer will store labels <span class="keyword">in</span> output column metadata</span><br><span class="line"></span><br><span class="line">Transformed indexed column <span class="string">'categoryIndex'</span> back to original string column <span class="string">'originalCategory'</span> using labels <span class="keyword">in</span> metadata</span><br><span class="line">+---+-------------+----------------+</span><br><span class="line">| id|categoryIndex|originalCategory|</span><br><span class="line">+---+-------------+----------------+</span><br><span class="line">|  <span class="number">0</span>|          <span class="number">0.0</span>|               a|</span><br><span class="line">|  <span class="number">1</span>|          <span class="number">2.0</span>|               b|</span><br><span class="line">|  <span class="number">2</span>|          <span class="number">1.0</span>|               c|</span><br><span class="line">|  <span class="number">3</span>|          <span class="number">0.0</span>|               a|</span><br><span class="line">|  <span class="number">4</span>|          <span class="number">0.0</span>|               a|</span><br><span class="line">|  <span class="number">5</span>|          <span class="number">1.0</span>|               c|</span><br><span class="line">+---+-------------+----------------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/index_to_string_example.py” in the Spark repo.</p>
<h3 id="OneHotEncoding"><a href="#OneHotEncoding" class="headerlink" title="OneHotEncoding"></a><strong>OneHotEncoding</strong></h3><p><a href="http://en.wikipedia.org/wiki/One-hot" target="_blank" rel="noopener">One-hot encoding</a>将一列标签索引映射到一列二进制向量，其中最多只有一个one-value。该编码允许那些期望使用连续特征的算法（例如Logistic回归）使用分类特征。</p>
<ul>
<li>Examples</li>
</ul>
<p>关于 API的更多细节请参考<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.OneHotEncoder" target="_blank" rel="noopener">OneHotEncoder Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> OneHotEncoder, StringIndexer</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession  </span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"OneHotEncoderExample"</span>).getOrCreate()</span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">0</span>, <span class="string">"a"</span>),</span><br><span class="line">    (<span class="number">1</span>, <span class="string">"b"</span>),</span><br><span class="line">    (<span class="number">2</span>, <span class="string">"c"</span>),</span><br><span class="line">    (<span class="number">3</span>, <span class="string">"a"</span>),</span><br><span class="line">    (<span class="number">4</span>, <span class="string">"a"</span>),</span><br><span class="line">    (<span class="number">5</span>, <span class="string">"c"</span>)</span><br><span class="line">], [<span class="string">"id"</span>, <span class="string">"category"</span>])</span><br><span class="line"></span><br><span class="line">stringIndexer = StringIndexer(inputCol=<span class="string">"category"</span>, outputCol=<span class="string">"categoryIndex"</span>)</span><br><span class="line">model = stringIndexer.fit(df)</span><br><span class="line">indexed = model.transform(df)</span><br><span class="line"></span><br><span class="line">encoder = OneHotEncoder(inputCol=<span class="string">"categoryIndex"</span>, outputCol=<span class="string">"categoryVec"</span>)</span><br><span class="line">encoded = encoder.transform(indexed)</span><br><span class="line">encoded.show()</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+---+--------+-------------+-------------+</span><br><span class="line">| id|category|categoryIndex|  categoryVec|</span><br><span class="line">+---+--------+-------------+-------------+</span><br><span class="line">|  <span class="number">0</span>|       a|          <span class="number">0.0</span>|(<span class="number">2</span>,[<span class="number">0</span>],[<span class="number">1.0</span>])|</span><br><span class="line">|  <span class="number">1</span>|       b|          <span class="number">2.0</span>|    (<span class="number">2</span>,[],[])|</span><br><span class="line">|  <span class="number">2</span>|       c|          <span class="number">1.0</span>|(<span class="number">2</span>,[<span class="number">1</span>],[<span class="number">1.0</span>])|</span><br><span class="line">|  <span class="number">3</span>|       a|          <span class="number">0.0</span>|(<span class="number">2</span>,[<span class="number">0</span>],[<span class="number">1.0</span>])|</span><br><span class="line">|  <span class="number">4</span>|       a|          <span class="number">0.0</span>|(<span class="number">2</span>,[<span class="number">0</span>],[<span class="number">1.0</span>])|</span><br><span class="line">|  <span class="number">5</span>|       c|          <span class="number">1.0</span>|(<span class="number">2</span>,[<span class="number">1</span>],[<span class="number">1.0</span>])|</span><br><span class="line">+---+--------+-------------+-------------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/onehot_encoder_example.py” in the Spark repo.</p>
<h3 id="VectorIndexer"><a href="#VectorIndexer" class="headerlink" title="VectorIndexer"></a><strong>VectorIndexer</strong></h3><p>VectorIndexer有助于索引Vectors的数据集中的分类特征。它可以自动决定哪些特征是分类的，并将原始值转换为分类索引。具体来说，它做了以下几点：</p>
<ol>
<li>取一个<a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.Vector" target="_blank" rel="noopener">Vector</a>类型的输入列和一个参数maxCategories。</li>
<li>根据不同值的数量确定哪些特征应该分类，这些特征最多被分为maxCategories类。</li>
<li>计算每个分类特征的分类索引(0-based)。</li>
<li>索引分类特征并将原始特征值转换为索引。</li>
</ol>
<p>索引分类特征允许Decision Trees(决策树)和Tree Ensembles等算法适当地处理分类特征，提高性能。</p>
<ul>
<li>Examples<br>在下面的例子中，我们读入一个标记点​​的数据集，然后用VectorIndexer来决定哪些特征应该被视为分类特征。我们将分类特征值转换为它们的索引。这个转换的数据然后可以被传递给诸如DecisionTreeRegressor处理分类特征的算法。</li>
</ul>
<p>请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorIndexer" target="_blank" rel="noopener">VectorIndexer Python文档</a> 以获取有关API的更多详细信息。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorIndexer</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession  </span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"VectorIndexerExample"</span>).getOrCreate()</span><br><span class="line">data = spark.read.format(<span class="string">"libsvm"</span>).load(<span class="string">"data/mllib/sample_libsvm_data.txt"</span>)</span><br><span class="line"></span><br><span class="line">indexer = VectorIndexer(inputCol=<span class="string">"features"</span>, outputCol=<span class="string">"indexed"</span>, maxCategories=<span class="number">10</span>)</span><br><span class="line">indexerModel = indexer.fit(data)</span><br><span class="line"></span><br><span class="line">categoricalFeatures = indexerModel.categoryMaps</span><br><span class="line">print(<span class="string">"Chose %d categorical features: %s"</span> %</span><br><span class="line">      (len(categoricalFeatures), <span class="string">", "</span>.join(str(k) <span class="keyword">for</span> k <span class="keyword">in</span> categoricalFeatures.keys())))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create new column "indexed" with categorical values transformed to indices</span></span><br><span class="line">indexedData = indexerModel.transform(data)</span><br><span class="line">indexedData.show()</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Chose <span class="number">351</span> categorical features: <span class="number">645</span>, <span class="number">69</span>, <span class="number">365</span>, <span class="number">138</span>, <span class="number">101</span>, <span class="number">479</span>, <span class="number">333</span>, <span class="number">249</span>, <span class="number">0</span>, <span class="number">555</span>, <span class="number">666</span>, <span class="number">88</span>, <span class="number">170</span>, <span class="number">115</span>, <span class="number">276</span>, <span class="number">308</span>, <span class="number">5</span>, <span class="number">449</span>, <span class="number">120</span>, <span class="number">247</span>, <span class="number">614</span>, <span class="number">677</span>, <span class="number">202</span>, <span class="number">10</span>, <span class="number">56</span>, <span class="number">533</span>, <span class="number">142</span>, <span class="number">500</span>, <span class="number">340</span>, <span class="number">670</span>, <span class="number">174</span>, <span class="number">42</span>, <span class="number">417</span>, <span class="number">24</span>, <span class="number">37</span>, <span class="number">25</span>, <span class="number">257</span>, <span class="number">389</span>, <span class="number">52</span>, <span class="number">14</span>, <span class="number">504</span>, <span class="number">110</span>, <span class="number">587</span>, <span class="number">619</span>, <span class="number">196</span>, <span class="number">559</span>, <span class="number">638</span>, <span class="number">20</span>, <span class="number">421</span>, <span class="number">46</span>, <span class="number">93</span>, <span class="number">284</span>, <span class="number">228</span>, <span class="number">448</span>, <span class="number">57</span>, <span class="number">78</span>, <span class="number">29</span>, <span class="number">475</span>, <span class="number">164</span>, <span class="number">591</span>, <span class="number">646</span>, <span class="number">253</span>, <span class="number">106</span>, <span class="number">121</span>, <span class="number">84</span>, <span class="number">480</span>, <span class="number">147</span>, <span class="number">280</span>, <span class="number">61</span>, <span class="number">221</span>, <span class="number">396</span>, <span class="number">89</span>, <span class="number">133</span>, <span class="number">116</span>, <span class="number">1</span>, <span class="number">507</span>, <span class="number">312</span>, <span class="number">74</span>, <span class="number">307</span>, <span class="number">452</span>, <span class="number">6</span>, <span class="number">248</span>, <span class="number">60</span>, <span class="number">117</span>, <span class="number">678</span>, <span class="number">529</span>, <span class="number">85</span>, <span class="number">201</span>, <span class="number">220</span>, <span class="number">366</span>, <span class="number">534</span>, <span class="number">102</span>, <span class="number">334</span>, <span class="number">28</span>, <span class="number">38</span>, <span class="number">561</span>, <span class="number">392</span>, <span class="number">70</span>, <span class="number">424</span>, <span class="number">192</span>, <span class="number">21</span>, <span class="number">137</span>, <span class="number">165</span>, <span class="number">33</span>, <span class="number">92</span>, <span class="number">229</span>, <span class="number">252</span>, <span class="number">197</span>, <span class="number">361</span>, <span class="number">65</span>, <span class="number">97</span>, <span class="number">665</span>, <span class="number">583</span>, <span class="number">285</span>, <span class="number">224</span>, <span class="number">650</span>, <span class="number">615</span>, <span class="number">9</span>, <span class="number">53</span>, <span class="number">169</span>, <span class="number">593</span>, <span class="number">141</span>, <span class="number">610</span>, <span class="number">420</span>, <span class="number">109</span>, <span class="number">256</span>, <span class="number">225</span>, <span class="number">339</span>, <span class="number">77</span>, <span class="number">193</span>, <span class="number">669</span>, <span class="number">476</span>, <span class="number">642</span>, <span class="number">637</span>, <span class="number">590</span>, <span class="number">679</span>, <span class="number">96</span>, <span class="number">393</span>, <span class="number">647</span>, <span class="number">173</span>, <span class="number">13</span>, <span class="number">41</span>, <span class="number">503</span>, <span class="number">134</span>, <span class="number">73</span>, <span class="number">105</span>, <span class="number">2</span>, <span class="number">508</span>, <span class="number">311</span>, <span class="number">558</span>, <span class="number">674</span>, <span class="number">530</span>, <span class="number">586</span>, <span class="number">618</span>, <span class="number">166</span>, <span class="number">32</span>, <span class="number">34</span>, <span class="number">148</span>, <span class="number">45</span>, <span class="number">161</span>, <span class="number">279</span>, <span class="number">64</span>, <span class="number">689</span>, <span class="number">17</span>, <span class="number">149</span>, <span class="number">584</span>, <span class="number">562</span>, <span class="number">176</span>, <span class="number">423</span>, <span class="number">191</span>, <span class="number">22</span>, <span class="number">44</span>, <span class="number">59</span>, <span class="number">118</span>, <span class="number">281</span>, <span class="number">27</span>, <span class="number">641</span>, <span class="number">71</span>, <span class="number">391</span>, <span class="number">12</span>, <span class="number">445</span>, <span class="number">54</span>, <span class="number">313</span>, <span class="number">611</span>, <span class="number">144</span>, <span class="number">49</span>, <span class="number">335</span>, <span class="number">86</span>, <span class="number">672</span>, <span class="number">172</span>, <span class="number">113</span>, <span class="number">681</span>, <span class="number">219</span>, <span class="number">419</span>, <span class="number">81</span>, <span class="number">230</span>, <span class="number">362</span>, <span class="number">451</span>, <span class="number">76</span>, <span class="number">7</span>, <span class="number">39</span>, <span class="number">649</span>, <span class="number">98</span>, <span class="number">616</span>, <span class="number">477</span>, <span class="number">367</span>, <span class="number">535</span>, <span class="number">103</span>, <span class="number">140</span>, <span class="number">621</span>, <span class="number">91</span>, <span class="number">66</span>, <span class="number">251</span>, <span class="number">668</span>, <span class="number">198</span>, <span class="number">108</span>, <span class="number">278</span>, <span class="number">223</span>, <span class="number">394</span>, <span class="number">306</span>, <span class="number">135</span>, <span class="number">563</span>, <span class="number">226</span>, <span class="number">3</span>, <span class="number">505</span>, <span class="number">80</span>, <span class="number">167</span>, <span class="number">35</span>, <span class="number">473</span>, <span class="number">675</span>, <span class="number">589</span>, <span class="number">162</span>, <span class="number">531</span>, <span class="number">680</span>, <span class="number">255</span>, <span class="number">648</span>, <span class="number">112</span>, <span class="number">617</span>, <span class="number">194</span>, <span class="number">145</span>, <span class="number">48</span>, <span class="number">557</span>, <span class="number">690</span>, <span class="number">63</span>, <span class="number">640</span>, <span class="number">18</span>, <span class="number">282</span>, <span class="number">95</span>, <span class="number">310</span>, <span class="number">50</span>, <span class="number">67</span>, <span class="number">199</span>, <span class="number">673</span>, <span class="number">16</span>, <span class="number">585</span>, <span class="number">502</span>, <span class="number">338</span>, <span class="number">643</span>, <span class="number">31</span>, <span class="number">336</span>, <span class="number">613</span>, <span class="number">11</span>, <span class="number">72</span>, <span class="number">175</span>, <span class="number">446</span>, <span class="number">612</span>, <span class="number">143</span>, <span class="number">43</span>, <span class="number">250</span>, <span class="number">231</span>, <span class="number">450</span>, <span class="number">99</span>, <span class="number">363</span>, <span class="number">556</span>, <span class="number">87</span>, <span class="number">203</span>, <span class="number">671</span>, <span class="number">688</span>, <span class="number">104</span>, <span class="number">368</span>, <span class="number">588</span>, <span class="number">40</span>, <span class="number">304</span>, <span class="number">26</span>, <span class="number">258</span>, <span class="number">390</span>, <span class="number">55</span>, <span class="number">114</span>, <span class="number">171</span>, <span class="number">139</span>, <span class="number">418</span>, <span class="number">23</span>, <span class="number">8</span>, <span class="number">75</span>, <span class="number">119</span>, <span class="number">58</span>, <span class="number">667</span>, <span class="number">478</span>, <span class="number">536</span>, <span class="number">82</span>, <span class="number">620</span>, <span class="number">447</span>, <span class="number">36</span>, <span class="number">168</span>, <span class="number">146</span>, <span class="number">30</span>, <span class="number">51</span>, <span class="number">190</span>, <span class="number">19</span>, <span class="number">422</span>, <span class="number">564</span>, <span class="number">305</span>, <span class="number">107</span>, <span class="number">4</span>, <span class="number">136</span>, <span class="number">506</span>, <span class="number">79</span>, <span class="number">195</span>, <span class="number">474</span>, <span class="number">664</span>, <span class="number">532</span>, <span class="number">94</span>, <span class="number">283</span>, <span class="number">395</span>, <span class="number">332</span>, <span class="number">528</span>, <span class="number">644</span>, <span class="number">47</span>, <span class="number">15</span>, <span class="number">163</span>, <span class="number">200</span>, <span class="number">68</span>, <span class="number">62</span>, <span class="number">277</span>, <span class="number">691</span>, <span class="number">501</span>, <span class="number">90</span>, <span class="number">111</span>, <span class="number">254</span>, <span class="number">227</span>, <span class="number">337</span>, <span class="number">122</span>, <span class="number">83</span>, <span class="number">309</span>, <span class="number">560</span>, <span class="number">639</span>, <span class="number">676</span>, <span class="number">222</span>, <span class="number">592</span>, <span class="number">364</span>, <span class="number">100</span></span><br><span class="line">+-----+--------------------+--------------------+</span><br><span class="line">|label|            features|             indexed|</span><br><span class="line">+-----+--------------------+--------------------+</span><br><span class="line">|  <span class="number">0.0</span>|(<span class="number">692</span>,[<span class="number">127</span>,<span class="number">128</span>,<span class="number">129.</span>..|(<span class="number">692</span>,[<span class="number">127</span>,<span class="number">128</span>,<span class="number">129.</span>..|</span><br><span class="line">|  <span class="number">1.0</span>|(<span class="number">692</span>,[<span class="number">158</span>,<span class="number">159</span>,<span class="number">160.</span>..|(<span class="number">692</span>,[<span class="number">158</span>,<span class="number">159</span>,<span class="number">160.</span>..|</span><br><span class="line">|  <span class="number">1.0</span>|(<span class="number">692</span>,[<span class="number">124</span>,<span class="number">125</span>,<span class="number">126.</span>..|(<span class="number">692</span>,[<span class="number">124</span>,<span class="number">125</span>,<span class="number">126.</span>..|</span><br><span class="line">|  <span class="number">1.0</span>|(<span class="number">692</span>,[<span class="number">152</span>,<span class="number">153</span>,<span class="number">154.</span>..|(<span class="number">692</span>,[<span class="number">152</span>,<span class="number">153</span>,<span class="number">154.</span>..|</span><br><span class="line">|  <span class="number">1.0</span>|(<span class="number">692</span>,[<span class="number">151</span>,<span class="number">152</span>,<span class="number">153.</span>..|(<span class="number">692</span>,[<span class="number">151</span>,<span class="number">152</span>,<span class="number">153.</span>..|</span><br><span class="line">|  <span class="number">0.0</span>|(<span class="number">692</span>,[<span class="number">129</span>,<span class="number">130</span>,<span class="number">131.</span>..|(<span class="number">692</span>,[<span class="number">129</span>,<span class="number">130</span>,<span class="number">131.</span>..|</span><br><span class="line">|  <span class="number">1.0</span>|(<span class="number">692</span>,[<span class="number">158</span>,<span class="number">159</span>,<span class="number">160.</span>..|(<span class="number">692</span>,[<span class="number">158</span>,<span class="number">159</span>,<span class="number">160.</span>..|</span><br><span class="line">|  <span class="number">1.0</span>|(<span class="number">692</span>,[<span class="number">99</span>,<span class="number">100</span>,<span class="number">101</span>,...|(<span class="number">692</span>,[<span class="number">99</span>,<span class="number">100</span>,<span class="number">101</span>,...|</span><br><span class="line">|  <span class="number">0.0</span>|(<span class="number">692</span>,[<span class="number">154</span>,<span class="number">155</span>,<span class="number">156.</span>..|(<span class="number">692</span>,[<span class="number">154</span>,<span class="number">155</span>,<span class="number">156.</span>..|</span><br><span class="line">|  <span class="number">0.0</span>|(<span class="number">692</span>,[<span class="number">127</span>,<span class="number">128</span>,<span class="number">129.</span>..|(<span class="number">692</span>,[<span class="number">127</span>,<span class="number">128</span>,<span class="number">129.</span>..|</span><br><span class="line">|  <span class="number">1.0</span>|(<span class="number">692</span>,[<span class="number">154</span>,<span class="number">155</span>,<span class="number">156.</span>..|(<span class="number">692</span>,[<span class="number">154</span>,<span class="number">155</span>,<span class="number">156.</span>..|</span><br><span class="line">|  <span class="number">0.0</span>|(<span class="number">692</span>,[<span class="number">153</span>,<span class="number">154</span>,<span class="number">155.</span>..|(<span class="number">692</span>,[<span class="number">153</span>,<span class="number">154</span>,<span class="number">155.</span>..|</span><br><span class="line">|  <span class="number">0.0</span>|(<span class="number">692</span>,[<span class="number">151</span>,<span class="number">152</span>,<span class="number">153.</span>..|(<span class="number">692</span>,[<span class="number">151</span>,<span class="number">152</span>,<span class="number">153.</span>..|</span><br><span class="line">|  <span class="number">1.0</span>|(<span class="number">692</span>,[<span class="number">129</span>,<span class="number">130</span>,<span class="number">131.</span>..|(<span class="number">692</span>,[<span class="number">129</span>,<span class="number">130</span>,<span class="number">131.</span>..|</span><br><span class="line">|  <span class="number">0.0</span>|(<span class="number">692</span>,[<span class="number">154</span>,<span class="number">155</span>,<span class="number">156.</span>..|(<span class="number">692</span>,[<span class="number">154</span>,<span class="number">155</span>,<span class="number">156.</span>..|</span><br><span class="line">|  <span class="number">1.0</span>|(<span class="number">692</span>,[<span class="number">150</span>,<span class="number">151</span>,<span class="number">152.</span>..|(<span class="number">692</span>,[<span class="number">150</span>,<span class="number">151</span>,<span class="number">152.</span>..|</span><br><span class="line">|  <span class="number">0.0</span>|(<span class="number">692</span>,[<span class="number">124</span>,<span class="number">125</span>,<span class="number">126.</span>..|(<span class="number">692</span>,[<span class="number">124</span>,<span class="number">125</span>,<span class="number">126.</span>..|</span><br><span class="line">|  <span class="number">0.0</span>|(<span class="number">692</span>,[<span class="number">152</span>,<span class="number">153</span>,<span class="number">154.</span>..|(<span class="number">692</span>,[<span class="number">152</span>,<span class="number">153</span>,<span class="number">154.</span>..|</span><br><span class="line">|  <span class="number">1.0</span>|(<span class="number">692</span>,[<span class="number">97</span>,<span class="number">98</span>,<span class="number">99</span>,<span class="number">12.</span>..|(<span class="number">692</span>,[<span class="number">97</span>,<span class="number">98</span>,<span class="number">99</span>,<span class="number">12.</span>..|</span><br><span class="line">|  <span class="number">1.0</span>|(<span class="number">692</span>,[<span class="number">124</span>,<span class="number">125</span>,<span class="number">126.</span>..|(<span class="number">692</span>,[<span class="number">124</span>,<span class="number">125</span>,<span class="number">126.</span>..|</span><br><span class="line">+-----+--------------------+--------------------+</span><br><span class="line">only showing top <span class="number">20</span> rows</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/vector_indexer_example.py” in the Spark repo.</p>
<h3 id="Interaction"><a href="#Interaction" class="headerlink" title="Interaction"></a><strong>Interaction</strong></h3><p><code>Interaction</code>是一个Transformer,采用向量或双值列的方法生成一个单一的向量列，其中包含每个输入列的一个值的所有组合的乘积。</p>
<p>例如，如果您有两个向量类型列，每个列都有三个维度作为输入列，那么您将获得一个9维向量作为输出列。</p>
<ul>
<li>Examples</li>
</ul>
<p>假设我们有以下DataFrame,有列“id1”，“vec1”和“vec2”：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">id1|vec1          |vec2</span><br><span class="line">---|--------------|--------------</span><br><span class="line"><span class="number">1</span>  |[<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>] |[<span class="number">8.0</span>,<span class="number">4.0</span>,<span class="number">5.0</span>]</span><br><span class="line"><span class="number">2</span>  |[<span class="number">4.0</span>,<span class="number">3.0</span>,<span class="number">8.0</span>] |[<span class="number">7.0</span>,<span class="number">9.0</span>,<span class="number">8.0</span>]</span><br><span class="line"><span class="number">3</span>  |[<span class="number">6.0</span>,<span class="number">1.0</span>,<span class="number">9.0</span>] |[<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">6.0</span>]</span><br><span class="line"><span class="number">4</span>  |[<span class="number">10.0</span>,<span class="number">8.0</span>,<span class="number">6.0</span>]|[<span class="number">9.0</span>,<span class="number">4.0</span>,<span class="number">5.0</span>]</span><br><span class="line"><span class="number">5</span>  |[<span class="number">9.0</span>,<span class="number">2.0</span>,<span class="number">7.0</span>] |[<span class="number">10.0</span>,<span class="number">7.0</span>,<span class="number">3.0</span>]</span><br><span class="line"><span class="number">6</span>  |[<span class="number">1.0</span>,<span class="number">1.0</span>,<span class="number">4.0</span>] |[<span class="number">2.0</span>,<span class="number">8.0</span>,<span class="number">4.0</span>]</span><br></pre></td></tr></table></figure></p>
<p>应用Interaction作用于这些输入列，然后interactedCol输出列包含：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">id1|vec1          |vec2          |interactedCol</span><br><span class="line">---|--------------|--------------|------------------------------------------------------</span><br><span class="line"><span class="number">1</span>  |[<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>] |[<span class="number">8.0</span>,<span class="number">4.0</span>,<span class="number">5.0</span>] |[<span class="number">8.0</span>,<span class="number">4.0</span>,<span class="number">5.0</span>,<span class="number">16.0</span>,<span class="number">8.0</span>,<span class="number">10.0</span>,<span class="number">24.0</span>,<span class="number">12.0</span>,<span class="number">15.0</span>]</span><br><span class="line"><span class="number">2</span>  |[<span class="number">4.0</span>,<span class="number">3.0</span>,<span class="number">8.0</span>] |[<span class="number">7.0</span>,<span class="number">9.0</span>,<span class="number">8.0</span>] |[<span class="number">56.0</span>,<span class="number">72.0</span>,<span class="number">64.0</span>,<span class="number">42.0</span>,<span class="number">54.0</span>,<span class="number">48.0</span>,<span class="number">112.0</span>,<span class="number">144.0</span>,<span class="number">128.0</span>]</span><br><span class="line"><span class="number">3</span>  |[<span class="number">6.0</span>,<span class="number">1.0</span>,<span class="number">9.0</span>] |[<span class="number">2.0</span>,<span class="number">3.0</span>,<span class="number">6.0</span>] |[<span class="number">36.0</span>,<span class="number">54.0</span>,<span class="number">108.0</span>,<span class="number">6.0</span>,<span class="number">9.0</span>,<span class="number">18.0</span>,<span class="number">54.0</span>,<span class="number">81.0</span>,<span class="number">162.0</span>]</span><br><span class="line"><span class="number">4</span>  |[<span class="number">10.0</span>,<span class="number">8.0</span>,<span class="number">6.0</span>]|[<span class="number">9.0</span>,<span class="number">4.0</span>,<span class="number">5.0</span>] |[<span class="number">360.0</span>,<span class="number">160.0</span>,<span class="number">200.0</span>,<span class="number">288.0</span>,<span class="number">128.0</span>,<span class="number">160.0</span>,<span class="number">216.0</span>,<span class="number">96.0</span>,<span class="number">120.0</span>]</span><br><span class="line"><span class="number">5</span>  |[<span class="number">9.0</span>,<span class="number">2.0</span>,<span class="number">7.0</span>] |[<span class="number">10.0</span>,<span class="number">7.0</span>,<span class="number">3.0</span>]|[<span class="number">450.0</span>,<span class="number">315.0</span>,<span class="number">135.0</span>,<span class="number">100.0</span>,<span class="number">70.0</span>,<span class="number">30.0</span>,<span class="number">350.0</span>,<span class="number">245.0</span>,<span class="number">105.0</span>]</span><br><span class="line"><span class="number">6</span>  |[<span class="number">1.0</span>,<span class="number">1.0</span>,<span class="number">4.0</span>] |[<span class="number">2.0</span>,<span class="number">8.0</span>,<span class="number">4.0</span>] |[<span class="number">12.0</span>,<span class="number">48.0</span>,<span class="number">24.0</span>,<span class="number">12.0</span>,<span class="number">48.0</span>,<span class="number">24.0</span>,<span class="number">48.0</span>,<span class="number">192.0</span>,<span class="number">96.0</span>]</span><br></pre></td></tr></table></figure></p>
<p>注：该方法暂时并没有python的实现，有scala和Java的</p>
<h3 id="Normalizer"><a href="#Normalizer" class="headerlink" title="Normalizer"></a><strong>Normalizer</strong></h3><p><code>Normalizer</code>是一个Transformer，它转换数据集的Vector行，规范化每个Vector为unit norm。它采用参数p来规范化，它指定用于规范化的p范数。（默认p = 2 ）。这种规范化可以帮助标准化您的输入数据，并改善学习算法的行为。</p>
<ul>
<li>Examples</li>
</ul>
<p>以下示例演示如何以libsvm格式加载数据集，然后将每行标准化为unit L^1 norm1和unitL^∞ norm。</p>
<p>有关API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Normalizer" target="_blank" rel="noopener">Normalizer Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> Normalizer</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession  </span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"NormalizerExample"</span>).getOrCreate()</span><br><span class="line">dataFrame = spark.createDataFrame([</span><br><span class="line">    (<span class="number">0</span>, Vectors.dense([<span class="number">1.0</span>, <span class="number">0.5</span>, <span class="number">-1.0</span>]),),</span><br><span class="line">    (<span class="number">1</span>, Vectors.dense([<span class="number">2.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]),),</span><br><span class="line">    (<span class="number">2</span>, Vectors.dense([<span class="number">4.0</span>, <span class="number">10.0</span>, <span class="number">2.0</span>]),)</span><br><span class="line">], [<span class="string">"id"</span>, <span class="string">"features"</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize each Vector using $L^1$ norm.</span></span><br><span class="line">normalizer = Normalizer(inputCol=<span class="string">"features"</span>, outputCol=<span class="string">"normFeatures"</span>, p=<span class="number">1.0</span>)</span><br><span class="line">l1NormData = normalizer.transform(dataFrame)</span><br><span class="line">print(<span class="string">"Normalized using L^1 norm"</span>)</span><br><span class="line">l1NormData.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize each Vector using $L^\infty$ norm.</span></span><br><span class="line">lInfNormData = normalizer.transform(dataFrame, &#123;normalizer.p: float(<span class="string">"inf"</span>)&#125;)</span><br><span class="line">print(<span class="string">"Normalized using L^inf norm"</span>)</span><br><span class="line">lInfNormData.show()</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Normalized using L^<span class="number">1</span> norm</span><br><span class="line">+---+--------------+------------------+</span><br><span class="line">| id|      features|      normFeatures|</span><br><span class="line">+---+--------------+------------------+</span><br><span class="line">|  <span class="number">0</span>|[<span class="number">1.0</span>,<span class="number">0.5</span>,<span class="number">-1.0</span>]|    [<span class="number">0.4</span>,<span class="number">0.2</span>,<span class="number">-0.4</span>]|</span><br><span class="line">|  <span class="number">1</span>| [<span class="number">2.0</span>,<span class="number">1.0</span>,<span class="number">1.0</span>]|   [<span class="number">0.5</span>,<span class="number">0.25</span>,<span class="number">0.25</span>]|</span><br><span class="line">|  <span class="number">2</span>|[<span class="number">4.0</span>,<span class="number">10.0</span>,<span class="number">2.0</span>]|[<span class="number">0.25</span>,<span class="number">0.625</span>,<span class="number">0.125</span>]|</span><br><span class="line">+---+--------------+------------------+</span><br><span class="line"></span><br><span class="line">Normalized using L^inf norm</span><br><span class="line">+---+--------------+--------------+</span><br><span class="line">| id|      features|  normFeatures|</span><br><span class="line">+---+--------------+--------------+</span><br><span class="line">|  <span class="number">0</span>|[<span class="number">1.0</span>,<span class="number">0.5</span>,<span class="number">-1.0</span>]|[<span class="number">1.0</span>,<span class="number">0.5</span>,<span class="number">-1.0</span>]|</span><br><span class="line">|  <span class="number">1</span>| [<span class="number">2.0</span>,<span class="number">1.0</span>,<span class="number">1.0</span>]| [<span class="number">1.0</span>,<span class="number">0.5</span>,<span class="number">0.5</span>]|</span><br><span class="line">|  <span class="number">2</span>|[<span class="number">4.0</span>,<span class="number">10.0</span>,<span class="number">2.0</span>]| [<span class="number">0.4</span>,<span class="number">1.0</span>,<span class="number">0.2</span>]|</span><br><span class="line">+---+--------------+--------------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/normalizer_example.py” in the Spark repo.</p>
<h3 id="StandardScaler"><a href="#StandardScaler" class="headerlink" title="StandardScaler"></a><strong>StandardScaler</strong></h3><p><code>StandardScaler</code>转换Vector行的数据集，将每个特征归一化为具有单位标准偏差和/或零均值。它需要参数：</p>
<ul>
<li>withStd：默认为true。将数据缩放到单位标准偏差。</li>
<li>withMean：默认为False。在缩放之前将数据集中在平均值上。它会建立一个密集的输出，所以在应用于稀疏输入时要小心。</li>
</ul>
<p>StandardScaler是一个Estimator，可以fit在一个数据集上产生一个StandardScalerModel; 这相当于计算汇总统计。然后该模型可以转换Vector数据集中的列以具有单位标准偏差和/或零均值特征。</p>
<p>请注意，如果某个要素的标准偏差为零，则会在该特征的Vector中返回默认值0.0。</p>
<ul>
<li>Example</li>
</ul>
<p>以下示例演示如何加载数据集，然后将每个特征标准化为单位标准偏差。</p>
<p>有关API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.StandardScaler" target="_blank" rel="noopener">StandardScaler Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"StandardScalerExample"</span>).getOrCreate()</span><br><span class="line">dataFrame = spark.createDataFrame([</span><br><span class="line">    (<span class="number">0</span>, Vectors.dense([<span class="number">1.0</span>, <span class="number">0.5</span>, <span class="number">-1.0</span>]),),</span><br><span class="line">    (<span class="number">1</span>, Vectors.dense([<span class="number">2.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]),),</span><br><span class="line">    (<span class="number">2</span>, Vectors.dense([<span class="number">4.0</span>, <span class="number">10.0</span>, <span class="number">2.0</span>]),)</span><br><span class="line">], [<span class="string">"id"</span>, <span class="string">"features"</span>])</span><br><span class="line">scaler = StandardScaler(inputCol=<span class="string">"features"</span>, outputCol=<span class="string">"scaledFeatures"</span>,</span><br><span class="line">                        withStd=<span class="keyword">True</span>, withMean=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute summary statistics by fitting the StandardScaler</span></span><br><span class="line">scalerModel = scaler.fit(dataFrame)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize each feature to have unit standard deviation.</span></span><br><span class="line">scaledData = scalerModel.transform(dataFrame)</span><br><span class="line">scaledData.show(truncate=<span class="keyword">False</span>)</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+---+--------------+------------------------------------------------------------+</span><br><span class="line">|id |features      |scaledFeatures                                              |</span><br><span class="line">+---+--------------+------------------------------------------------------------+</span><br><span class="line">|<span class="number">0</span>  |[<span class="number">1.0</span>,<span class="number">0.5</span>,<span class="number">-1.0</span>]|[<span class="number">0.6546536707079772</span>,<span class="number">0.09352195295828244</span>,<span class="number">-0.6546536707079771</span>]|</span><br><span class="line">|<span class="number">1</span>  |[<span class="number">2.0</span>,<span class="number">1.0</span>,<span class="number">1.0</span>] |[<span class="number">1.3093073414159544</span>,<span class="number">0.1870439059165649</span>,<span class="number">0.6546536707079771</span>]  |</span><br><span class="line">|<span class="number">2</span>  |[<span class="number">4.0</span>,<span class="number">10.0</span>,<span class="number">2.0</span>]|[<span class="number">2.618614682831909</span>,<span class="number">1.870439059165649</span>,<span class="number">1.3093073414159542</span>]    |</span><br><span class="line">+---+--------------+------------------------------------------------------------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/standard_scaler_example.py” in the Spark repo.</p>
<h3 id="MinMaxScaler"><a href="#MinMaxScaler" class="headerlink" title="MinMaxScaler"></a><strong>MinMaxScaler</strong></h3><p><code>MinMaxScaler</code>转换Vector行数据集，将每个特征重新缩放到特定范围（通常为[0，1]）。它需要参数：</p>
<p>min：默认为0.0。转换后的下界，被所有特征共享。<br>max：默认为1.0。变换后的上界，被所有的特征共享。<br>MinMaxScaler计算数据集的汇总统计并生成一个MinMaxScalerModel。然后模型可以单独转换每个特征，使其在给定的范围内。</p>
<p>特征E的重新缩放的值被计算为，<br>Rescaled(ei) = (ei − Emin) / (Emax − Emin) ∗ (max − min) + min<br>对于Emax==Emin的情况Rescaled(ei)=0.5∗(max+min)</p>
<p>请注意，由于零值可能会被转换为非零值，所以transofromer的输出将会是DenseVector，即使输入是稀疏输入。</p>
<ul>
<li>Examples</li>
</ul>
<p>有关API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.MinMaxScaler" target="_blank" rel="noopener">MinMaxScaler Python文档</a> 和<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.MinMaxScalerModel" target="_blank" rel="noopener">MinMaxScalerModel Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession  </span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"MinMaxScalerExample"</span>).getOrCreate()</span><br><span class="line">dataFrame = spark.createDataFrame([</span><br><span class="line">    (<span class="number">0</span>, Vectors.dense([<span class="number">1.0</span>, <span class="number">0.1</span>, <span class="number">-1.0</span>]),),</span><br><span class="line">    (<span class="number">1</span>, Vectors.dense([<span class="number">2.0</span>, <span class="number">1.1</span>, <span class="number">1.0</span>]),),</span><br><span class="line">    (<span class="number">2</span>, Vectors.dense([<span class="number">3.0</span>, <span class="number">10.1</span>, <span class="number">3.0</span>]),)</span><br><span class="line">], [<span class="string">"id"</span>, <span class="string">"features"</span>])</span><br><span class="line"></span><br><span class="line">scaler = MinMaxScaler(inputCol=<span class="string">"features"</span>, outputCol=<span class="string">"scaledFeatures"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute summary statistics and generate MinMaxScalerModel</span></span><br><span class="line">scalerModel = scaler.fit(dataFrame)</span><br><span class="line"></span><br><span class="line"><span class="comment"># rescale each feature to range [min, max].</span></span><br><span class="line">scaledData = scalerModel.transform(dataFrame)</span><br><span class="line">print(<span class="string">"Features scaled to range: [%f, %f]"</span> % (scaler.getMin(), scaler.getMax()))</span><br><span class="line">scaledData.select(<span class="string">"features"</span>, <span class="string">"scaledFeatures"</span>).show()</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Features scaled to range: [<span class="number">0.000000</span>, <span class="number">1.000000</span>]</span><br><span class="line">+--------------+--------------+</span><br><span class="line">|      features|scaledFeatures|</span><br><span class="line">+--------------+--------------+</span><br><span class="line">|[<span class="number">1.0</span>,<span class="number">0.1</span>,<span class="number">-1.0</span>]| [<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.0</span>]|</span><br><span class="line">| [<span class="number">2.0</span>,<span class="number">1.1</span>,<span class="number">1.0</span>]| [<span class="number">0.5</span>,<span class="number">0.1</span>,<span class="number">0.5</span>]|</span><br><span class="line">|[<span class="number">3.0</span>,<span class="number">10.1</span>,<span class="number">3.0</span>]| [<span class="number">1.0</span>,<span class="number">1.0</span>,<span class="number">1.0</span>]|</span><br><span class="line">+--------------+--------------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/min_max_scaler_example.py” in the Spark repo.</p>
<h3 id="MaxAbsScaler"><a href="#MaxAbsScaler" class="headerlink" title="MaxAbsScaler"></a><strong>MaxAbsScaler</strong></h3><p><code>MaxAbsScaler</code>转换Vector行的数据集，通过分割每个特征的最大绝对值来重新缩放每个特征到范围[-1,1]。它不会移动/居中数据，因此不会破坏任何稀疏性。</p>
<p>MaxAbsScaler计算数据集的汇总统计并生成一个MaxAbsScalerModel。该模型可以将每个特征分别转换为范围[-1,1]。</p>
<ul>
<li>Examples</li>
</ul>
<p>有关API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.MaxAbsScaler" target="_blank" rel="noopener">MaxAbsScaler Python文档</a> 和<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.MaxAbsScalerModel" target="_blank" rel="noopener">MaxAbsScalerModel Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> MaxAbsScaler</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"MaxAbsScalerExample"</span>).getOrCreate()</span><br><span class="line">dataFrame = spark.createDataFrame([</span><br><span class="line">    (<span class="number">0</span>, Vectors.dense([<span class="number">1.0</span>, <span class="number">0.1</span>, <span class="number">-8.0</span>]),),</span><br><span class="line">    (<span class="number">1</span>, Vectors.dense([<span class="number">2.0</span>, <span class="number">1.0</span>, <span class="number">-4.0</span>]),),</span><br><span class="line">    (<span class="number">2</span>, Vectors.dense([<span class="number">4.0</span>, <span class="number">10.0</span>, <span class="number">8.0</span>]),)</span><br><span class="line">], [<span class="string">"id"</span>, <span class="string">"features"</span>])</span><br><span class="line"></span><br><span class="line">scaler = MaxAbsScaler(inputCol=<span class="string">"features"</span>, outputCol=<span class="string">"scaledFeatures"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute summary statistics and generate MaxAbsScalerModel</span></span><br><span class="line">scalerModel = scaler.fit(dataFrame)</span><br><span class="line"></span><br><span class="line"><span class="comment"># rescale each feature to range [-1, 1].</span></span><br><span class="line">scaledData = scalerModel.transform(dataFrame)</span><br><span class="line"></span><br><span class="line">scaledData.select(<span class="string">"features"</span>, <span class="string">"scaledFeatures"</span>).show()</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+--------------+----------------+</span><br><span class="line">|      features|  scaledFeatures|</span><br><span class="line">+--------------+----------------+</span><br><span class="line">|[<span class="number">1.0</span>,<span class="number">0.1</span>,<span class="number">-8.0</span>]|[<span class="number">0.25</span>,<span class="number">0.01</span>,<span class="number">-1.0</span>]|</span><br><span class="line">|[<span class="number">2.0</span>,<span class="number">1.0</span>,<span class="number">-4.0</span>]|  [<span class="number">0.5</span>,<span class="number">0.1</span>,<span class="number">-0.5</span>]|</span><br><span class="line">|[<span class="number">4.0</span>,<span class="number">10.0</span>,<span class="number">8.0</span>]|   [<span class="number">1.0</span>,<span class="number">1.0</span>,<span class="number">1.0</span>]|</span><br><span class="line">+--------------+----------------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/max_abs_scaler_example.py” in the Spark repo.</p>
<h3 id="Bucketizer"><a href="#Bucketizer" class="headerlink" title="Bucketizer"></a><strong>Bucketizer</strong></h3><p><code>Bucketizer</code>将一列连续的特征转换成特征桶列，其中桶由用户指定。它需要一个参数：</p>
<ul>
<li>splits：用于将连续特征映射到存储桶的参数。n个buckets有n+1个splits。由分割x，y定义的bucket值范围为[x,y)不包含y,而只有最后一个bucket包含y。splits应是严格增加的。必须明确提供inf的值以涵盖所有Double值; 否则，指定splits之外的值将被视为错误。两个splits的例子是Array(Double.NegativeInfinity, 0.0, 1.0, Double.PositiveInfinity)和Array(0.0, 1.0, 2.0)。</li>
</ul>
<p>请注意，如果您不知道目标列的上限和下限，则应该添加Double.NegativeInfinity并Double.PositiveInfinity作为分割的界限，以防止出现Bucketizer界限异常。</p>
<p>还要注意，你提供的splits必须严格按照递增顺序，即s0 &lt; s1 &lt; s2 &lt; … &lt; sn。</p>
<p>更多细节可以在<a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Bucketizer" target="_blank" rel="noopener">Bucketizer</a>的API文档中找到。</p>
<ul>
<li>Examples</li>
</ul>
<p>以下示例演示了如何将一列Doubles转换为另一个索引表列<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> Bucketizer</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"BucketizerExample"</span>).getOrCreate()</span><br><span class="line">splits = [-float(<span class="string">"inf"</span>), <span class="number">-0.5</span>, <span class="number">0.0</span>, <span class="number">0.5</span>, float(<span class="string">"inf"</span>)]</span><br><span class="line"></span><br><span class="line">data = [(<span class="number">-999.9</span>,), (<span class="number">-0.5</span>,), (<span class="number">-0.3</span>,), (<span class="number">0.0</span>,), (<span class="number">0.2</span>,), (<span class="number">999.9</span>,)]</span><br><span class="line">dataFrame = spark.createDataFrame(data, [<span class="string">"features"</span>])</span><br><span class="line"></span><br><span class="line">bucketizer = Bucketizer(splits=splits, inputCol=<span class="string">"features"</span>, outputCol=<span class="string">"bucketedFeatures"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Transform original data into its bucket index.</span></span><br><span class="line">bucketedData = bucketizer.transform(dataFrame)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Bucketizer output with %d buckets"</span> % (len(bucketizer.getSplits())<span class="number">-1</span>))</span><br><span class="line">bucketedData.show()</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Bucketizer output <span class="keyword">with</span> <span class="number">4</span> buckets</span><br><span class="line">+--------+----------------+</span><br><span class="line">|features|bucketedFeatures|</span><br><span class="line">+--------+----------------+</span><br><span class="line">|  <span class="number">-999.9</span>|             <span class="number">0.0</span>|</span><br><span class="line">|    <span class="number">-0.5</span>|             <span class="number">1.0</span>|</span><br><span class="line">|    <span class="number">-0.3</span>|             <span class="number">1.0</span>|</span><br><span class="line">|     <span class="number">0.0</span>|             <span class="number">2.0</span>|</span><br><span class="line">|     <span class="number">0.2</span>|             <span class="number">2.0</span>|</span><br><span class="line">|   <span class="number">999.9</span>|             <span class="number">3.0</span>|</span><br><span class="line">+--------+----------------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/bucketizer_example.py” in the Spark repo.</p>
<h3 id="ElementwiseProduct"><a href="#ElementwiseProduct" class="headerlink" title="ElementwiseProduct"></a><strong>ElementwiseProduct</strong></h3><p><code>ElementwiseProduct</code>将每个输入矢量使用元素乘法乘以一个提供的“权重”矢量。换句话说，它通过标量乘数来缩放数据集的每一列。这表示输入向量v和变换向量w之间的<a href="https://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29" target="_blank" rel="noopener">Hadamard product</a>(哈达玛积)，得到结果向量。<br>(v1…vN).T 。(w1…WN).T = (v1w1…vNwN).T</p>
<ul>
<li>Examples</li>
</ul>
<p>下面的这个例子演示了如何使用变换向量值来变换向量。有关API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.ElementwiseProduct" target="_blank" rel="noopener">ElementwiseProduct Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> ElementwiseProduct</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"ElementwiseProductExample"</span>).getOrCreate()</span><br><span class="line"><span class="comment"># Create some vector data; also works for sparse vectors</span></span><br><span class="line">data = [(Vectors.dense([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]),), (Vectors.dense([<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]),)]</span><br><span class="line">df = spark.createDataFrame(data, [<span class="string">"vector"</span>])</span><br><span class="line">transformer = ElementwiseProduct(scalingVec=Vectors.dense([<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>]),</span><br><span class="line">                                 inputCol=<span class="string">"vector"</span>, outputCol=<span class="string">"transformedVector"</span>)</span><br><span class="line"><span class="comment"># Batch transform the vectors to create new column:</span></span><br><span class="line">transformer.transform(df).show()</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+-------------+-----------------+</span><br><span class="line">|       vector|transformedVector|</span><br><span class="line">+-------------+-----------------+</span><br><span class="line">|[<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>]|    [<span class="number">0.0</span>,<span class="number">2.0</span>,<span class="number">6.0</span>]|</span><br><span class="line">|[<span class="number">4.0</span>,<span class="number">5.0</span>,<span class="number">6.0</span>]|   [<span class="number">0.0</span>,<span class="number">5.0</span>,<span class="number">12.0</span>]|</span><br><span class="line">+-------------+-----------------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/elementwise_product_example.py” in the Spark repo.</p>
<h3 id="SQLTransformer"><a href="#SQLTransformer" class="headerlink" title="SQLTransformer"></a><strong>SQLTransformer</strong></h3><p><code>SQLTransformer</code>实现由SQL语句定义的转换。目前我们只支持一下SQL语法：”SELECT … FROM <strong>THIS</strong> …” where， “<strong>THIS</strong>“代表输入数据集的基础表。select子句指定要在输出中显示的字段，常量和表达式，并且可以是Spark SQL支持的任何select子句。用户还可以使用Spark SQL内置函数和UDF对这些选定的列进行操作。例如，SQLTransformer支持像这样的语句：</p>
<ul>
<li>SELECT a, a + b AS a_b FROM <strong>THIS</strong></li>
<li>SELECT a, SQRT(b) AS b_sqrt FROM <strong>THIS</strong> where a &gt; 5</li>
<li><p>SELECT a, b, SUM(c) AS c_sum FROM <strong>THIS</strong> GROUP BY a, b</p>
</li>
<li><p>Examples</p>
</li>
</ul>
<p>有关该API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.SQLTransformer" target="_blank" rel="noopener">SQLTransformer Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> SQLTransformer</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"SQLTransformerExample"</span>).getOrCreate()</span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">0</span>, <span class="number">1.0</span>, <span class="number">3.0</span>),</span><br><span class="line">    (<span class="number">2</span>, <span class="number">2.0</span>, <span class="number">5.0</span>)</span><br><span class="line">], [<span class="string">"id"</span>, <span class="string">"v1"</span>, <span class="string">"v2"</span>])</span><br><span class="line">sqlTrans = SQLTransformer(</span><br><span class="line">    statement=<span class="string">"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__"</span>)</span><br><span class="line">sqlTrans.transform(df).show()</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+---+---+---+---+----+</span><br><span class="line">| id| v1| v2| v3|  v4|</span><br><span class="line">+---+---+---+---+----+</span><br><span class="line">|  <span class="number">0</span>|<span class="number">1.0</span>|<span class="number">3.0</span>|<span class="number">4.0</span>| <span class="number">3.0</span>|</span><br><span class="line">|  <span class="number">2</span>|<span class="number">2.0</span>|<span class="number">5.0</span>|<span class="number">7.0</span>|<span class="number">10.0</span>|</span><br><span class="line">+---+---+---+---+----+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/sql_transformer.py” in the Spark repo.</p>
<h3 id="VectorAssembler"><a href="#VectorAssembler" class="headerlink" title="VectorAssembler"></a><strong>VectorAssembler</strong></h3><p><code>VectorAssembler</code>是一个将给定的列列表组合成单个向量列的transoformer。对于将原始特征和由不同特征变换器生成的特征组合成一个特征向量，以便训练诸如逻辑回归和决策树等ML模型是有用的。 VectorAssembler接受以下输入列类型：所有数字类型，布尔类型和向量类型。在每一行中，输入列的值将按照指定的顺序连接成一个向量。</p>
<ul>
<li>Examples</li>
</ul>
<p>请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler" target="_blank" rel="noopener">VectorAssembler Python文档</a> 以获取有关API的更多详细信息。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorAssembler</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"VectorAssemblerExample"</span>).getOrCreate()</span><br><span class="line">dataset = spark.createDataFrame(</span><br><span class="line">    [(<span class="number">0</span>, <span class="number">18</span>, <span class="number">1.0</span>, Vectors.dense([<span class="number">0.0</span>, <span class="number">10.0</span>, <span class="number">0.5</span>]), <span class="number">1.0</span>)],</span><br><span class="line">    [<span class="string">"id"</span>, <span class="string">"hour"</span>, <span class="string">"mobile"</span>, <span class="string">"userFeatures"</span>, <span class="string">"clicked"</span>])</span><br><span class="line"></span><br><span class="line">assembler = VectorAssembler(</span><br><span class="line">    inputCols=[<span class="string">"hour"</span>, <span class="string">"mobile"</span>, <span class="string">"userFeatures"</span>],</span><br><span class="line">    outputCol=<span class="string">"features"</span>)</span><br><span class="line"></span><br><span class="line">output = assembler.transform(dataset)</span><br><span class="line">print(<span class="string">"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'"</span>)</span><br><span class="line">output.select(<span class="string">"features"</span>, <span class="string">"clicked"</span>).show(truncate=<span class="keyword">False</span>)</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Assembled columns <span class="string">'hour'</span>, <span class="string">'mobile'</span>, <span class="string">'userFeatures'</span> to vector column <span class="string">'features'</span></span><br><span class="line">+-----------------------+-------+</span><br><span class="line">|features               |clicked|</span><br><span class="line">+-----------------------+-------+</span><br><span class="line">|[<span class="number">18.0</span>,<span class="number">1.0</span>,<span class="number">0.0</span>,<span class="number">10.0</span>,<span class="number">0.5</span>]|<span class="number">1.0</span>    |</span><br><span class="line">+-----------------------+-------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/vector_assembler_example.py” in the Spark repo.</p>
<h3 id="QuantileDiscretizer"><a href="#QuantileDiscretizer" class="headerlink" title="QuantileDiscretizer"></a><strong>QuantileDiscretizer</strong></h3><p>QuantileDiscretizer将带有连续特征的列转换成具有分类分类特征的列。bins的数量通过numBuckets参数设置。如果输入的独特值不足以创建足够多的分位数，则所用桶的数量可能会小于此值。</p>
<p>NaN values：NaN值将在QuantileDiscretizer拟合过程中从列中移除。这将产生一个Bucketizer预测模型。在转换期间，当在数据集中发现NaN值时Bucketizer会引发错误，但是用户也可以通过设置handleInvalid来选择保留或删除数据集中的NaN值。如果用户选择保留NaN值，他们将被专门处理，并放入他们自己的bucket中，例如，如果使用4个bucket，那么非NaN数据将被放入bucket[0-3]，但是NaN将是算在一个特殊的bucket[4]里。</p>
<p>Algorithm：使用近似算法（有关详细说明，请参阅<a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions" target="_blank" rel="noopener">approxQuantile</a>文档 ）来选择bin的范围。近似的精度可以用relativeError参数来控制 。设置为零时，计算确切的分位数（注意：精确计算分位数是一个耗费的操作）。下部和上部bin边界会是-Infinity和+Infinity以来涵盖所有实数值。</p>
<ul>
<li>Examples</li>
</ul>
<p>请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.QuantileDiscretizer" target="_blank" rel="noopener">QuantileDiscretizer Python文档</a> 以获取有关API的更多详细信息。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> QuantileDiscretizer</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"QuantileDiscretizerExample"</span>).getOrCreate()</span><br><span class="line">data = [(<span class="number">0</span>, <span class="number">18.0</span>), (<span class="number">1</span>, <span class="number">19.0</span>), (<span class="number">2</span>, <span class="number">8.0</span>), (<span class="number">3</span>, <span class="number">5.0</span>), (<span class="number">4</span>, <span class="number">2.2</span>)]</span><br><span class="line">df = spark.createDataFrame(data, [<span class="string">"id"</span>, <span class="string">"hour"</span>])</span><br><span class="line"></span><br><span class="line">discretizer = QuantileDiscretizer(numBuckets=<span class="number">3</span>, inputCol=<span class="string">"hour"</span>, outputCol=<span class="string">"result"</span>)</span><br><span class="line"></span><br><span class="line">result = discretizer.fit(df).transform(df)</span><br><span class="line">result.show()</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+---+----+------+</span><br><span class="line">| id|hour|result|</span><br><span class="line">+---+----+------+</span><br><span class="line">|  <span class="number">0</span>|<span class="number">18.0</span>|   <span class="number">2.0</span>|</span><br><span class="line">|  <span class="number">1</span>|<span class="number">19.0</span>|   <span class="number">2.0</span>|</span><br><span class="line">|  <span class="number">2</span>| <span class="number">8.0</span>|   <span class="number">1.0</span>|</span><br><span class="line">|  <span class="number">3</span>| <span class="number">5.0</span>|   <span class="number">1.0</span>|</span><br><span class="line">|  <span class="number">4</span>| <span class="number">2.2</span>|   <span class="number">0.0</span>|</span><br><span class="line">+---+----+------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/quantile_discretizer_example.py” in the Spark repo.</p>
<h3 id="Imputer"><a href="#Imputer" class="headerlink" title="Imputer"></a><strong>Imputer</strong></h3><p><code>Imputer</code> transformer使用平均值或位于列的中位数填充数据集中缺少的值。输入列应该是 DoubleType或FloatType。目前Imputer不支持分类特征，并可能为包含分类特征的列创建不正确的值。</p>
<p>注意：输入列中的所有null值都被视为缺失，所以也被归类。</p>
<ul>
<li>Examples</li>
</ul>
<p>有关API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Imputer" target="_blank" rel="noopener">Imputer Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> Imputer</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"ImputerExample"</span>).getOrCreate()</span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">1.0</span>, float(<span class="string">"nan"</span>)),</span><br><span class="line">    (<span class="number">2.0</span>, float(<span class="string">"nan"</span>)),</span><br><span class="line">    (float(<span class="string">"nan"</span>), <span class="number">3.0</span>),</span><br><span class="line">    (<span class="number">4.0</span>, <span class="number">4.0</span>),</span><br><span class="line">    (<span class="number">5.0</span>, <span class="number">5.0</span>)</span><br><span class="line">], [<span class="string">"a"</span>, <span class="string">"b"</span>])</span><br><span class="line"></span><br><span class="line">imputer = Imputer(inputCols=[<span class="string">"a"</span>, <span class="string">"b"</span>], outputCols=[<span class="string">"out_a"</span>, <span class="string">"out_b"</span>])</span><br><span class="line">model = imputer.fit(df)</span><br><span class="line"></span><br><span class="line">model.transform(df).show()</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+---+---+-----+-----+</span><br><span class="line">|  a|  b|out_a|out_b|</span><br><span class="line">+---+---+-----+-----+</span><br><span class="line">|<span class="number">1.0</span>|NaN|  <span class="number">1.0</span>|  <span class="number">4.0</span>|</span><br><span class="line">|<span class="number">2.0</span>|NaN|  <span class="number">2.0</span>|  <span class="number">4.0</span>|</span><br><span class="line">|NaN|<span class="number">3.0</span>|  <span class="number">3.0</span>|  <span class="number">3.0</span>|</span><br><span class="line">|<span class="number">4.0</span>|<span class="number">4.0</span>|  <span class="number">4.0</span>|  <span class="number">4.0</span>|</span><br><span class="line">|<span class="number">5.0</span>|<span class="number">5.0</span>|  <span class="number">5.0</span>|  <span class="number">5.0</span>|</span><br><span class="line">+---+---+-----+-----+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/imputer_example.py” in the Spark repo.</p>
<h2 id="Feature-Selectors"><a href="#Feature-Selectors" class="headerlink" title="Feature Selectors"></a><strong>Feature Selectors</strong></h2><h3 id="VectorSlicer"><a href="#VectorSlicer" class="headerlink" title="VectorSlicer"></a><strong>VectorSlicer</strong></h3><p><code>VectorSlicer</code>是一个transformer，它将一个特征向量转换成一个新的具有原始特征的sub-array的特征向量。这对从向量列中提取特征很有用。</p>
<p>VectorSlicer接受一个具有指定索引的向量列，然后输出一个新的向量列，其值通过这些索引来选择。有两种类型的索引：</p>
<ol>
<li><p>setIndices()：代表向量中索引的整数索引。</p>
</li>
<li><p>setNames()：代表向量中特征名称的字符串索引。 这需要vector列有一个AttributeGroup，因为实现得匹配Attribute名称字段。</p>
</li>
</ol>
<p>整数和字符串的规范都是可以接受的。而且，您可以同时使用整数索引和字符串名称。必须至少选择一个特征，不允许重复的特征，所以选择的索引和名称之间就没有重叠。请注意，如果选择了特征的名称，遇到空的输入属性时将会抛出异常。</p>
<p>输出向量将首先按照选定的索引（按给定的顺序）排序，然后是选定的名称（按给定的顺序）。</p>
<ul>
<li>Examples</li>
</ul>
<p>有关API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorSlicer" target="_blank" rel="noopener">VectorSlicer Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> VectorSlicer</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> Row</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"VectorSlicerExample"</span>).getOrCreate()</span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    Row(userFeatures=Vectors.sparse(<span class="number">3</span>, &#123;<span class="number">0</span>: <span class="number">-2.0</span>, <span class="number">1</span>: <span class="number">2.3</span>&#125;)),</span><br><span class="line">    Row(userFeatures=Vectors.dense([<span class="number">-2.0</span>, <span class="number">2.3</span>, <span class="number">0.0</span>]))])</span><br><span class="line"></span><br><span class="line">slicer = VectorSlicer(inputCol=<span class="string">"userFeatures"</span>, outputCol=<span class="string">"features"</span>, indices=[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">output = slicer.transform(df)</span><br><span class="line"></span><br><span class="line">output.select(<span class="string">"userFeatures"</span>, <span class="string">"features"</span>).show()</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+--------------------+-------------+</span><br><span class="line">|        userFeatures|     features|</span><br><span class="line">+--------------------+-------------+</span><br><span class="line">|(<span class="number">3</span>,[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">-2.0</span>,<span class="number">2.3</span>])|(<span class="number">1</span>,[<span class="number">0</span>],[<span class="number">2.3</span>])|</span><br><span class="line">|      [<span class="number">-2.0</span>,<span class="number">2.3</span>,<span class="number">0.0</span>]|        [<span class="number">2.3</span>]|</span><br><span class="line">+--------------------+-------------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/vector_slicer_example.py” in the Spark repo.</p>
<h3 id="RFormula"><a href="#RFormula" class="headerlink" title="RFormula"></a><strong>RFormula</strong></h3><p><code>RFormula</code>选择由<a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html" target="_blank" rel="noopener">Rmodel formula</a>指定的列。目前我们支持R操作符的有限子集，包括’〜’，’。’，’：’，’+’和’ - ‘。其基本的操作符是：</p>
<ul>
<li>‘~’: 分开 target和terms</li>
<li>‘+’: 连接terms，“+ 0”表示删除intercept</li>
<li>‘-‘: 删除一个term，“ - 1”表示删除intercept</li>
<li>‘:’: interaction（数值相乘或二元化分类值）</li>
<li>‘.’: 除target以外的所有列</li>
</ul>
<p>假设a和都是b是double类型的列，我们使用以下简单的例子来说明RFormula的作用：</p>
<p>y ~ a + b意味着模型y ~ w0 + w1 <em> a + w2 </em> b，其中w0是截距intercept，w1, w2是系数coefficients。\<br>y ~ a + b + a:b - 1装置模型y ~ w1 <em> a + w2 </em> b + w3 <em> a </em> b，其中w1, w2, w3为系数。\<br>RFormula产生特征的一个向量列的和一个double类型列或标签的字符串类型列。就像在R中使用公式进行线性回归时一样，字符串输入列将被进行one-hot编码，而数字列将被转换为doule类型。如果标签列是字符串类型的，它将首先被转换为StringIndexer的double类型。如果DataFrame中不存在标签列，则将使用公式中指定的结果变量创建输出标签列。</p>
<ul>
<li>Examples</li>
</ul>
<p>有关API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.RFormula" target="_blank" rel="noopener">RFormula Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> RFormula</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"RFormulaExample"</span>).getOrCreate()</span><br><span class="line">dataset = spark.createDataFrame(</span><br><span class="line">    [(<span class="number">7</span>, <span class="string">"US"</span>, <span class="number">18</span>, <span class="number">1.0</span>),</span><br><span class="line">     (<span class="number">8</span>, <span class="string">"CA"</span>, <span class="number">12</span>, <span class="number">0.0</span>),</span><br><span class="line">     (<span class="number">9</span>, <span class="string">"NZ"</span>, <span class="number">15</span>, <span class="number">0.0</span>)],</span><br><span class="line">    [<span class="string">"id"</span>, <span class="string">"country"</span>, <span class="string">"hour"</span>, <span class="string">"clicked"</span>])</span><br><span class="line"></span><br><span class="line">formula = RFormula(</span><br><span class="line">    formula=<span class="string">"clicked ~ country + hour"</span>,</span><br><span class="line">    featuresCol=<span class="string">"features"</span>,</span><br><span class="line">    labelCol=<span class="string">"label"</span>)</span><br><span class="line"></span><br><span class="line">output = formula.fit(dataset).transform(dataset)</span><br><span class="line">output.select(<span class="string">"features"</span>, <span class="string">"label"</span>).show()</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">+--------------+-----+</span><br><span class="line">|      features|label|</span><br><span class="line">+--------------+-----+</span><br><span class="line">|[<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">18.0</span>]|  <span class="number">1.0</span>|</span><br><span class="line">|[<span class="number">1.0</span>,<span class="number">0.0</span>,<span class="number">12.0</span>]|  <span class="number">0.0</span>|</span><br><span class="line">|[<span class="number">0.0</span>,<span class="number">1.0</span>,<span class="number">15.0</span>]|  <span class="number">0.0</span>|</span><br><span class="line">+--------------+-----+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/rformula_example.py” in the Spark repo.</p>
<h3 id="ChiSqSelector"><a href="#ChiSqSelector" class="headerlink" title="ChiSqSelector"></a><strong>ChiSqSelector</strong></h3><p><code>ChiSqSelector</code>代表Chi-Squared特征选择。它作用于具有分类特征的已标记数据。ChiSqSelector使用<a href="https://en.wikipedia.org/wiki/Chi-squared_test" target="_blank" rel="noopener">Chi-Squared test of independence</a>来决定选择哪些特征。它支持五种选择方法：numTopFeatures，percentile，fpr，fdr，fwe：<em> numTopFeatures选择一个根据卡方检验得到的固定的数目前几个特征，这类似于产生具有最大预测能力的特征。</em> percentile类似于numTopFeatures，但只选择所有特征的一部分，而不是固定的数目。<em> fpr选择p值低于阈值的所有特征，从而控制选择的误报率。</em> fdr使用<a href="https://en.wikipedia.org/wiki/False_discovery_rate#Benjamini.E2.80.93Hochberg_procedure" target="_blank" rel="noopener">Benjamini-Hochbergprocedure</a>选择false discovery rate低于阈值的所有特征。* fwe选择p值低于阈值的所有特征,阈值由1 / numFeatures缩放，从而控制选择的family-wise error rate。默认选择方法是numTopFeatures，top特征的默认数量设置为50.用户可以使用setSelectorType选择方法。</p>
<ul>
<li>Examples</li>
</ul>
<p>有关API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.ChiSqSelector" target="_blank" rel="noopener">ChiSqSelector Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> ChiSqSelector</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"ChiSqSelectorExample"</span>).getOrCreate()</span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">    (<span class="number">7</span>, Vectors.dense([<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">18.0</span>, <span class="number">1.0</span>]), <span class="number">1.0</span>,),</span><br><span class="line">    (<span class="number">8</span>, Vectors.dense([<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">12.0</span>, <span class="number">0.0</span>]), <span class="number">0.0</span>,),</span><br><span class="line">    (<span class="number">9</span>, Vectors.dense([<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">15.0</span>, <span class="number">0.1</span>]), <span class="number">0.0</span>,)], [<span class="string">"id"</span>, <span class="string">"features"</span>, <span class="string">"clicked"</span>])</span><br><span class="line"></span><br><span class="line">selector = ChiSqSelector(numTopFeatures=<span class="number">1</span>, featuresCol=<span class="string">"features"</span>,</span><br><span class="line">                         outputCol=<span class="string">"selectedFeatures"</span>, labelCol=<span class="string">"clicked"</span>)</span><br><span class="line"></span><br><span class="line">result = selector.fit(df).transform(df)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"ChiSqSelector output with top %d features selected"</span> % selector.getNumTopFeatures())</span><br><span class="line">result.show()</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">ChiSqSelector output <span class="keyword">with</span> top <span class="number">1</span> features selected</span><br><span class="line">+---+------------------+-------+----------------+</span><br><span class="line">| id|          features|clicked|selectedFeatures|</span><br><span class="line">+---+------------------+-------+----------------+</span><br><span class="line">|  <span class="number">7</span>|[<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">18.0</span>,<span class="number">1.0</span>]|    <span class="number">1.0</span>|          [<span class="number">18.0</span>]|</span><br><span class="line">|  <span class="number">8</span>|[<span class="number">0.0</span>,<span class="number">1.0</span>,<span class="number">12.0</span>,<span class="number">0.0</span>]|    <span class="number">0.0</span>|          [<span class="number">12.0</span>]|</span><br><span class="line">|  <span class="number">9</span>|[<span class="number">1.0</span>,<span class="number">0.0</span>,<span class="number">15.0</span>,<span class="number">0.1</span>]|    <span class="number">0.0</span>|          [<span class="number">15.0</span>]|</span><br><span class="line">+---+------------------+-------+----------------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/chisq_selector_example.py” in the Spark repo.</p>
<h2 id="Locality-Sensitive-Hashing"><a href="#Locality-Sensitive-Hashing" class="headerlink" title="Locality Sensitive Hashing"></a><strong>Locality Sensitive Hashing</strong></h2><p><a href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing" target="_blank" rel="noopener">Locality Sensitive Hashing (LSH)</a>是一类重要的散列技术，常用于聚类，近似最近邻搜索和大数据集异常值检测。</p>
<p>LSH的总体思路是使用一系列函数（“LSH families”）将数据点散列到桶中，使得彼此靠近的数据点高概率地出现在同一个桶中，而彼此相距很远的数据点很有可能在不同的桶里。LSH family正式定义如下。</p>
<p>在一个度量空间中(M, d)中，M一个集合，d是一个基于M的距离函数，LSH族是满足下列性质的函数族h：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">存在p,q属于M，</span><br><span class="line">d(p,q) &lt;= r1 =&gt; Pr(h(p)) = h(q) &gt;= p1</span><br><span class="line">d(p,1) &gt;= r2 =&gt; Pr(h(p)) = h(q) &lt;= p2 </span><br><span class="line">则这个LSH族被称为 (r1, r2, p1, p2)-敏感的。</span><br></pre></td></tr></table></figure></p>
<p>在Spark中，不同的LSH families在不同的类中实现（例如MinHash），并且在每个类中提供用于feature transformation(特征变换)，approximate simility join(最近似连接)和approximate nearest neighbor最近邻的APIs。</p>
<p>在LSH中，我们将false positive定义为一对散列到同一个桶中远距输入特征(with d(p,q)≥r2)，并且将一个false negative定义为一对被散列到不同的桶中近距特征(with d(p,q)≤r1)</p>
<h3 id="LSH-Operations"><a href="#LSH-Operations" class="headerlink" title="LSH Operations"></a><strong>LSH Operations</strong></h3><p>我们描述了LSH可以应用的主要操作类型。一个拟合的LSH模型具有下列每个操作的方法。</p>
<h4 id="Feature-Transformation"><a href="#Feature-Transformation" class="headerlink" title="Feature Transformation"></a>Feature Transformation</h4><p>Feature Transformation是添加哈希值作为新列的基本功能。这对降维有用。用户可以通过设置inputCol和outputCol来指定输入和输出列名。</p>
<p>LSH还支持多个LSH哈希表。用户可以通过设置numHashTables来指定哈希表的数量。这也用于在approximate similarity join和approximate nearest neighbo的<a href="approximate similarity join and approximate nearest neighbo">OR-amplification</a>。增加哈希表的数量会提高精度，但同时也会增加通信成本和运行时间。</p>
<p>outputCol的类型是Seq[Vector]，其中数组的维度等于numHashTables，vectors的维度当前设置为1。在未来的版本中，我们将实现AND-amplification，使得用户可以指定这些vectors的维度。</p>
<h4 id="Approximate-Similarity-Join"><a href="#Approximate-Similarity-Join" class="headerlink" title="Approximate Similarity Join"></a>Approximate Similarity Join</h4><p>Approximate Similarity Join输入两个数据集，近似地返回数据集中那些距离小于用户定义的阈值的数行对。Approximate similarity join支持连接两个不同的数据集和self-joining(自连接)。自加入会产生一些重复的对。</p>
<p>Approximate similarity join接受转换和未转换的数据集作为输入。如果使用未转换的数据集，则会自动进行转换。在这种情况下，hash signture(哈希签名)将被创建为outputCol。</p>
<p>在连接的数据集中，可以在datasetA和datasetB中查询原始数据集。距离列将被添加到输出数据集，以显示返回的每对行之间的真实距离。</p>
<h4 id="Approximate-Nearest-Neighbor-Search"><a href="#Approximate-Nearest-Neighbor-Search" class="headerlink" title="Approximate Nearest Neighbor Search"></a>Approximate Nearest Neighbor Search</h4><p>Approximate nearest neighbor search需要（拥有特征向量s的）数据集和一个关键字（单个特征向量），并且它近似地返回数据集中最接近这个向量的指定数量的行。</p>
<p>Approximate nearest neighbor search接受转换和未转换的数据集作为输入。如果使用未转换的数据集，则会自动进行转换。在这种情况下，哈希签名将被创建为outputCol。</p>
<p>距离列将被添加到输出数据集，以显示每个输出行和搜索键之间的真实距离。</p>
<p>注意：当散列桶中没有足够的候选项时，Approximate nearest neighbor search将返回少于l行。</p>
<h3 id="LSH-Algorithms"><a href="#LSH-Algorithms" class="headerlink" title="LSH Algorithms"></a><strong>LSH Algorithms</strong></h3><h4 id="Bucketed-Random-Projection-for-Euclidean-Distance"><a href="#Bucketed-Random-Projection-for-Euclidean-Distance" class="headerlink" title="Bucketed Random Projection for Euclidean Distance"></a>Bucketed Random Projection for Euclidean Distance</h4><p><a href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing#Stable_distributions" target="_blank" rel="noopener">Bucketed Random Projection</a>是一个基于欧氏距离的LSH family。欧几里德距离定义如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">d(x,y) = sqrt(sum((xi - yi)**2))</span><br></pre></td></tr></table></figure></p>
<p> 其LSH族将特征向量投影到随机单位向量上，并将投影结果分成哈希桶：<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">h(x) = [x·v/r]</span><br></pre></td></tr></table></figure></p>
<p>其中r是用户定义的桶长度。桶长度可以用来控制散列桶的平均大小（从而控制桶的数量）。较大的桶长度（即，较少的桶）增加了特征被散列到相同桶的可能性（增加了true and false positives）。</p>
<p>Bucketed Random Projection接受任意向量作为输入特征，同时支持稀疏和密集向量。\<br>有关API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.BucketedRandomProjectionLSH" target="_blank" rel="noopener">BucketedRandomProjectionLSH Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> BucketedRandomProjectionLSH</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> col</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"BucketedRandomProjectionLshExample"</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line">dataA = [(<span class="number">0</span>, Vectors.dense([<span class="number">1.0</span>, <span class="number">1.0</span>]),),</span><br><span class="line">         (<span class="number">1</span>, Vectors.dense([<span class="number">1.0</span>, <span class="number">-1.0</span>]),),</span><br><span class="line">         (<span class="number">2</span>, Vectors.dense([<span class="number">-1.0</span>, <span class="number">-1.0</span>]),),</span><br><span class="line">         (<span class="number">3</span>, Vectors.dense([<span class="number">-1.0</span>, <span class="number">1.0</span>]),)]</span><br><span class="line">dfA = spark.createDataFrame(dataA, [<span class="string">"id"</span>, <span class="string">"features"</span>])</span><br><span class="line"></span><br><span class="line">dataB = [(<span class="number">4</span>, Vectors.dense([<span class="number">1.0</span>, <span class="number">0.0</span>]),),</span><br><span class="line">         (<span class="number">5</span>, Vectors.dense([<span class="number">-1.0</span>, <span class="number">0.0</span>]),),</span><br><span class="line">         (<span class="number">6</span>, Vectors.dense([<span class="number">0.0</span>, <span class="number">1.0</span>]),),</span><br><span class="line">         (<span class="number">7</span>, Vectors.dense([<span class="number">0.0</span>, <span class="number">-1.0</span>]),)]</span><br><span class="line">dfB = spark.createDataFrame(dataB, [<span class="string">"id"</span>, <span class="string">"features"</span>])</span><br><span class="line"></span><br><span class="line">key = Vectors.dense([<span class="number">1.0</span>, <span class="number">0.0</span>])</span><br><span class="line"></span><br><span class="line">brp = BucketedRandomProjectionLSH(inputCol=<span class="string">"features"</span>, outputCol=<span class="string">"hashes"</span>, bucketLength=<span class="number">2.0</span>,</span><br><span class="line">                                  numHashTables=<span class="number">3</span>)</span><br><span class="line">model = brp.fit(dfA)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Feature Transformation</span></span><br><span class="line">print(<span class="string">"The hashed dataset where hashed values are stored in the column 'hashes':"</span>)</span><br><span class="line">model.transform(dfA).show(truncate=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the locality sensitive hashes for the input rows, then perform approximate</span></span><br><span class="line"><span class="comment"># similarity join.</span></span><br><span class="line"><span class="comment"># We could avoid computing hashes by passing in the already-transformed dataset, e.g.</span></span><br><span class="line"><span class="comment"># `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`</span></span><br><span class="line">print(<span class="string">"Approximately joining dfA and dfB on Euclidean distance smaller than 1.5:"</span>)</span><br><span class="line">model.approxSimilarityJoin(dfA, dfB, <span class="number">1.5</span>, distCol=<span class="string">"EuclideanDistance"</span>)\</span><br><span class="line">    .select(col(<span class="string">"datasetA.id"</span>).alias(<span class="string">"idA"</span>),</span><br><span class="line">            col(<span class="string">"datasetB.id"</span>).alias(<span class="string">"idB"</span>),</span><br><span class="line">            col(<span class="string">"EuclideanDistance"</span>)).show(truncate=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the locality sensitive hashes for the input rows, then perform approximate nearest</span></span><br><span class="line"><span class="comment"># neighbor search.</span></span><br><span class="line"><span class="comment"># We could avoid computing hashes by passing in the already-transformed dataset, e.g.</span></span><br><span class="line"><span class="comment"># `model.approxNearestNeighbors(transformedA, key, 2)`</span></span><br><span class="line">print(<span class="string">"Approximately searching dfA for 2 nearest neighbors of the key:"</span>)</span><br><span class="line">model.approxNearestNeighbors(dfA, key, <span class="number">2</span>).show(truncate=<span class="keyword">False</span>)</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">The hashed dataset where hashed values are stored <span class="keyword">in</span> the column <span class="string">'hashes'</span>:</span><br><span class="line">+---+-----------+-----------------------+</span><br><span class="line">|id |features   |hashes                 |</span><br><span class="line">+---+-----------+-----------------------+</span><br><span class="line">|<span class="number">0</span>  |[<span class="number">1.0</span>,<span class="number">1.0</span>]  |[[<span class="number">-1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>]] |</span><br><span class="line">|<span class="number">1</span>  |[<span class="number">1.0</span>,<span class="number">-1.0</span>] |[[<span class="number">0.0</span>], [<span class="number">-1.0</span>], [<span class="number">0.0</span>]] |</span><br><span class="line">|<span class="number">2</span>  |[<span class="number">-1.0</span>,<span class="number">-1.0</span>]|[[<span class="number">0.0</span>], [<span class="number">-1.0</span>], [<span class="number">-1.0</span>]]|</span><br><span class="line">|<span class="number">3</span>  |[<span class="number">-1.0</span>,<span class="number">1.0</span>] |[[<span class="number">-1.0</span>], [<span class="number">0.0</span>], [<span class="number">-1.0</span>]]|</span><br><span class="line">+---+-----------+-----------------------+</span><br><span class="line"></span><br><span class="line">Approximately joining dfA <span class="keyword">and</span> dfB on Euclidean distance smaller than <span class="number">1.5</span>:</span><br><span class="line">+---+---+-----------------+</span><br><span class="line">|idA|idB|EuclideanDistance|</span><br><span class="line">+---+---+-----------------+</span><br><span class="line">|<span class="number">0</span>  |<span class="number">4</span>  |<span class="number">1.0</span>              |</span><br><span class="line">|<span class="number">2</span>  |<span class="number">7</span>  |<span class="number">1.0</span>              |</span><br><span class="line">|<span class="number">1</span>  |<span class="number">4</span>  |<span class="number">1.0</span>              |</span><br><span class="line">|<span class="number">0</span>  |<span class="number">6</span>  |<span class="number">1.0</span>              |</span><br><span class="line">|<span class="number">3</span>  |<span class="number">6</span>  |<span class="number">1.0</span>              |</span><br><span class="line">|<span class="number">3</span>  |<span class="number">5</span>  |<span class="number">1.0</span>              |</span><br><span class="line">|<span class="number">1</span>  |<span class="number">7</span>  |<span class="number">1.0</span>              |</span><br><span class="line">|<span class="number">2</span>  |<span class="number">5</span>  |<span class="number">1.0</span>              |</span><br><span class="line">+---+---+-----------------+</span><br><span class="line"></span><br><span class="line">Approximately searching dfA <span class="keyword">for</span> <span class="number">2</span> nearest neighbors of the key:</span><br><span class="line">+---+----------+----------------------+-------+</span><br><span class="line">|id |features  |hashes                |distCol|</span><br><span class="line">+---+----------+----------------------+-------+</span><br><span class="line">|<span class="number">0</span>  |[<span class="number">1.0</span>,<span class="number">1.0</span>] |[[<span class="number">-1.0</span>], [<span class="number">0.0</span>], [<span class="number">0.0</span>]]|<span class="number">1.0</span>    |</span><br><span class="line">|<span class="number">1</span>  |[<span class="number">1.0</span>,<span class="number">-1.0</span>]|[[<span class="number">0.0</span>], [<span class="number">-1.0</span>], [<span class="number">0.0</span>]]|<span class="number">1.0</span>    |</span><br><span class="line">+---+----------+----------------------+-------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/bucketed_random_projection_lsh_example.py” in the Spark repo.</p>
<h4 id="MinHash-for-Jaccard-Distance"><a href="#MinHash-for-Jaccard-Distance" class="headerlink" title="MinHash for Jaccard Distance"></a>MinHash for Jaccard Distance</h4><p><a href="https://en.wikipedia.org/wiki/MinHash" target="_blank" rel="noopener">MinHash</a>是用于计算Jaccard距离的LSH族，其中输入特征是自然数集合。两个集合的Jaccard距离由它们的交集和并集决定：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">d(A,B) = 1 -|A ∩ B| / |A ∪ B|</span><br></pre></td></tr></table></figure></p>
<p>MinHash 对集合中的每个元素应用随机哈希函数g，并取所有哈希值的最小值：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">h(A) = min(g(a))  ,a∈A</span><br></pre></td></tr></table></figure></p>
<p>MinHash的输入集表示为二元向量，其中向量索引表示元素本身，向量中的非零值表示集合中元素的存在。虽然支持密集和稀疏向量，但通常建议使用稀疏向量来提高效率。例如，Vectors.sparse(10, Array[(2, 1.0), (3, 1.0), (5, 1.0)])意味着空间中有10个元素。该集合包含元素2，元素3和元素5.所有非零值都被视为二进制“1”值。</p>
<p>注意：空集不能被MinHash转换，这意味着任何输入向量必须至少有一个非零的entry。</p>
<p>有关API的更多详细信息，请参阅<a href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.MinHashLSH" target="_blank" rel="noopener">MinHashLSH Python文档</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.ml.feature <span class="keyword">import</span> MinHashLSH</span><br><span class="line"><span class="keyword">from</span> pyspark.ml.linalg <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> col</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"MinHashLSHExample"</span>).getOrCreate()</span><br><span class="line">dataA = [(<span class="number">0</span>, Vectors.sparse(<span class="number">6</span>, [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]),),</span><br><span class="line">         (<span class="number">1</span>, Vectors.sparse(<span class="number">6</span>, [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]),),</span><br><span class="line">         (<span class="number">2</span>, Vectors.sparse(<span class="number">6</span>, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">4</span>], [<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]),)]</span><br><span class="line">dfA = spark.createDataFrame(dataA, [<span class="string">"id"</span>, <span class="string">"features"</span>])</span><br><span class="line"></span><br><span class="line">dataB = [(<span class="number">3</span>, Vectors.sparse(<span class="number">6</span>, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]),),</span><br><span class="line">         (<span class="number">4</span>, Vectors.sparse(<span class="number">6</span>, [<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]),),</span><br><span class="line">         (<span class="number">5</span>, Vectors.sparse(<span class="number">6</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>], [<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>]),)]</span><br><span class="line">dfB = spark.createDataFrame(dataB, [<span class="string">"id"</span>, <span class="string">"features"</span>])</span><br><span class="line"></span><br><span class="line">key = Vectors.sparse(<span class="number">6</span>, [<span class="number">1</span>, <span class="number">3</span>], [<span class="number">1.0</span>, <span class="number">1.0</span>])</span><br><span class="line"></span><br><span class="line">mh = MinHashLSH(inputCol=<span class="string">"features"</span>, outputCol=<span class="string">"hashes"</span>, numHashTables=<span class="number">5</span>)</span><br><span class="line">model = mh.fit(dfA)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Feature Transformation</span></span><br><span class="line">print(<span class="string">"The hashed dataset where hashed values are stored in the column 'hashes':"</span>)</span><br><span class="line">model.transform(dfA).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the locality sensitive hashes for the input rows, then perform approximate</span></span><br><span class="line"><span class="comment"># similarity join.</span></span><br><span class="line"><span class="comment"># We could avoid computing hashes by passing in the already-transformed dataset, e.g.</span></span><br><span class="line"><span class="comment"># `model.approxSimilarityJoin(transformedA, transformedB, 0.6)`</span></span><br><span class="line">print(<span class="string">"Approximately joining dfA and dfB on distance smaller than 0.6:"</span>)</span><br><span class="line">model.approxSimilarityJoin(dfA, dfB, <span class="number">0.6</span>, distCol=<span class="string">"JaccardDistance"</span>)\</span><br><span class="line">    .select(col(<span class="string">"datasetA.id"</span>).alias(<span class="string">"idA"</span>),</span><br><span class="line">            col(<span class="string">"datasetB.id"</span>).alias(<span class="string">"idB"</span>),</span><br><span class="line">            col(<span class="string">"JaccardDistance"</span>)).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the locality sensitive hashes for the input rows, then perform approximate nearest</span></span><br><span class="line"><span class="comment"># neighbor search.</span></span><br><span class="line"><span class="comment"># We could avoid computing hashes by passing in the already-transformed dataset, e.g.</span></span><br><span class="line"><span class="comment"># `model.approxNearestNeighbors(transformedA, key, 2)`</span></span><br><span class="line"><span class="comment"># It may return less than 2 rows when not enough approximate near-neighbor candidates are</span></span><br><span class="line"><span class="comment"># found.</span></span><br><span class="line">print(<span class="string">"Approximately searching dfA for 2 nearest neighbors of the key:"</span>)</span><br><span class="line">model.approxNearestNeighbors(dfA, key, <span class="number">2</span>).show()</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure></p>
<p>output:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">The hashed dataset where hashed values are stored <span class="keyword">in</span> the column <span class="string">'hashes'</span>:</span><br><span class="line">+---+--------------------+--------------------+</span><br><span class="line">| id|            features|              hashes|</span><br><span class="line">+---+--------------------+--------------------+</span><br><span class="line">|  <span class="number">0</span>|(<span class="number">6</span>,[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">1.0</span>,<span class="number">1.</span>..|[[<span class="number">-8.91727E8</span>], [-...|</span><br><span class="line">|  <span class="number">1</span>|(<span class="number">6</span>,[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1.0</span>,<span class="number">1.</span>..|[[<span class="number">-1.81795643E9</span>],...|</span><br><span class="line">|  <span class="number">2</span>|(<span class="number">6</span>,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>],[<span class="number">1.0</span>,<span class="number">1.</span>..|[[<span class="number">-1.33587497E8</span>],...|</span><br><span class="line">+---+--------------------+--------------------+</span><br><span class="line"></span><br><span class="line">Approximately joining dfA <span class="keyword">and</span> dfB on distance smaller than <span class="number">0.6</span>:</span><br><span class="line">+---+---+---------------+</span><br><span class="line">|idA|idB|JaccardDistance|</span><br><span class="line">+---+---+---------------+</span><br><span class="line">|  <span class="number">1</span>|  <span class="number">4</span>|            <span class="number">0.5</span>|</span><br><span class="line">|  <span class="number">1</span>|  <span class="number">5</span>|            <span class="number">0.5</span>|</span><br><span class="line">|  <span class="number">2</span>|  <span class="number">5</span>|            <span class="number">0.5</span>|</span><br><span class="line">|  <span class="number">0</span>|  <span class="number">5</span>|            <span class="number">0.5</span>|</span><br><span class="line">+---+---+---------------+</span><br><span class="line"></span><br><span class="line">Approximately searching dfA <span class="keyword">for</span> <span class="number">2</span> nearest neighbors of the key:</span><br><span class="line">+---+--------------------+--------------------+-------+</span><br><span class="line">| id|            features|              hashes|distCol|</span><br><span class="line">+---+--------------------+--------------------+-------+</span><br><span class="line">|  <span class="number">0</span>|(<span class="number">6</span>,[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">1.0</span>,<span class="number">1.</span>..|[[<span class="number">-8.91727E8</span>], [-...|   <span class="number">0.75</span>|</span><br><span class="line">|  <span class="number">1</span>|(<span class="number">6</span>,[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1.0</span>,<span class="number">1.</span>..|[[<span class="number">-1.81795643E9</span>],...|   <span class="number">0.75</span>|</span><br><span class="line">+---+--------------------+--------------------+-------+</span><br></pre></td></tr></table></figure></p>
<p>Find full example code at “examples/src/main/python/ml/min_hash_lsh_example.py” in the Spark repo.</p>
<h2 id="结束"><a href="#结束" class="headerlink" title="结束"></a>结束</h2>
      
    </div>
    
    
    
    
    
    <div>
    
      <div>
    
        <div style="text-align:center;color: #8959a8;font-size:20px;">-------------阅读完毕<i class="fa fa-paw"></i>吐槽一番吧~-------------</div>
    
</div>
    
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/TF-IDF/" rel="tag"><i class="fa fa-tag"></i> TF-IDF</a>
          
            <a href="/tags/Word2Vec/" rel="tag"><i class="fa fa-tag"></i> Word2Vec</a>
          
            <a href="/tags/OneHotEncoding/" rel="tag"><i class="fa fa-tag"></i> OneHotEncoding</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        
          <div class="wp_rating">
            <div id="wpac-rating"></div>
          </div>
        

        

        
          
          <div id="needsharebutton-postbottom">
            <span class="btn">
              <i class="fa fa-share-alt" aria-hidden="true"></i>
            </span>
          </div>
        
        </div>
      
      

      
        <div>
          <div class="recommended_posts">
    
    
    <div class="note primary">
        <h4>推荐文章 </h4>
            <ul>
                
                    <li><a href="https://blog.writeathink.cn/2018/01/22/SparkMLlib-Clustering/">SparkMLlib-Clustering</a></li>
                
                    <li><a href="https://blog.writeathink.cn/2018/01/22/SparkMLlib-Classification-and-Regression/">SparkMLlib-Classification-and-Regression</a></li>
                
                    <li><a href="https://blog.writeathink.cn/2018/01/19/sparkmllib-pipeline/">SparkMLlib-Pipeline</a></li>
                
                    <li><a href="https://blog.writeathink.cn/2018/01/19/sparkmllib-basic/">SparkMLlib-Basic</a></li>
                
            </ul>
    </div>
     
</div>
        </div>
      

      <div>
        
          
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

  <!-- JS库 sweetalert 可修改路径 -->
  <script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script>
  <script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>
  <p><span>本文标题:</span><a href="/2018/01/21/sparkmllib-working-with-features/">SparkMLlib-Working-with-Features</a></p>
  <p><span>文章作者:</span><a href="/" title="访问 cgDeepLearn 的个人博客">cgDeepLearn</a></p>
  <p><span>发布时间:</span>2018年01月21日 - 17:01</p>
  <p><span>最后更新:</span>2018年03月01日 - 17:03</p>
  <p><span>原始链接:</span><a href="/2018/01/21/sparkmllib-working-with-features/" title="SparkMLlib-Working-with-Features">https://blog.writeathink.cn/2018/01/21/sparkmllib-working-with-features/</a>
    <span class="copy-path"  title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="https://blog.writeathink.cn/2018/01/21/sparkmllib-working-with-features/"  aria-label="复制成功！"></i></span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p>  
</div>
<script> 
    var clipboard = new Clipboard('.fa-clipboard');
      $(".fa-clipboard").click(function(){
      clipboard.on('success', function(){
        swal({   
          title: "",   
          text: '复制成功',
          icon: "success", 
          showConfirmButton: true
          });
        });
    });  
</script>

        
      </div>
      
      

      
        <div>
          <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechat.jpg" alt="cgDeepLearn 微信支付"/>
        <p>微信支付</p>
      </div>
    

    

    

  </div>
</div>

        </div>
      

      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/01/22/SparkMLlib-Classification-and-Regression/" rel="prev" title="SparkMLlib-Classification-and-Regression">
                <i class="fa fa-chevron-left"></i> SparkMLlib-Classification-and-Regression
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/01/19/sparkmllib-pipeline/" rel="next" title="SparkMLlib-Pipeline">
                SparkMLlib-Pipeline <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    
    </footer>

    




  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <a href="/" class="site-author-image" rel="start" style="border:none">
              <img class="site-author-image" itemprop="image"
                src="/uploads/images/avatar.gif"
                alt="cgDeepLearn" />
              </a>
            
              <p class="site-author-name" itemprop="name">cgDeepLearn</p>
              <p class="site-description motion-element" itemprop="description">Code Rocks!</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">文章</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">59</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/cgDeepLearn" target="_blank" title="Github">
                      
                        <i class="fa fa-fw fa-github"></i>Github</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://music.163.com/#/user/home?id=36485986" target="_blank" title="网易云音乐">
                      
                        <i class="fa fa-fw fa-music"></i>网易云音乐</a>
                  </span>
                
            </div>
          

          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-globe"></i>
                链链看
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://writeathink.cn/" title="😆 writeathink 😆" target="_blank" rel="external nofollow">😆 writeathink 😆</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://cgdeeplearn.github.io/about/" title="关于此博客" target="_blank" rel="external nofollow">关于此博客</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Feature-Extractors"><span class="nav-number">1.</span> <span class="nav-text">Feature Extractors</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TF-IDF"><span class="nav-number">1.1.</span> <span class="nav-text">TF-IDF</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Word2Vec"><span class="nav-number">1.2.</span> <span class="nav-text">Word2Vec</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CountVectorizer"><span class="nav-number">1.3.</span> <span class="nav-text">CountVectorizer</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Feature-Transformers"><span class="nav-number">2.</span> <span class="nav-text">Feature Transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tokenizer"><span class="nav-number">2.1.</span> <span class="nav-text">Tokenizer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#StopWordsRemover"><span class="nav-number">2.2.</span> <span class="nav-text">StopWordsRemover</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#n-gram"><span class="nav-number">2.3.</span> <span class="nav-text">n-gram</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Binarizer"><span class="nav-number">2.4.</span> <span class="nav-text">Binarizer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PCA"><span class="nav-number">2.5.</span> <span class="nav-text">PCA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PolynomialExpansion"><span class="nav-number">2.6.</span> <span class="nav-text">PolynomialExpansion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Discrete-Cosine-Transform-DCT"><span class="nav-number">2.7.</span> <span class="nav-text">Discrete Cosine Transform(DCT)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#StringIndexer"><span class="nav-number">2.8.</span> <span class="nav-text">StringIndexer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#IndexToString"><span class="nav-number">2.9.</span> <span class="nav-text">IndexToString</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#OneHotEncoding"><span class="nav-number">2.10.</span> <span class="nav-text">OneHotEncoding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VectorIndexer"><span class="nav-number">2.11.</span> <span class="nav-text">VectorIndexer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Interaction"><span class="nav-number">2.12.</span> <span class="nav-text">Interaction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Normalizer"><span class="nav-number">2.13.</span> <span class="nav-text">Normalizer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#StandardScaler"><span class="nav-number">2.14.</span> <span class="nav-text">StandardScaler</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MinMaxScaler"><span class="nav-number">2.15.</span> <span class="nav-text">MinMaxScaler</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MaxAbsScaler"><span class="nav-number">2.16.</span> <span class="nav-text">MaxAbsScaler</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bucketizer"><span class="nav-number">2.17.</span> <span class="nav-text">Bucketizer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ElementwiseProduct"><span class="nav-number">2.18.</span> <span class="nav-text">ElementwiseProduct</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SQLTransformer"><span class="nav-number">2.19.</span> <span class="nav-text">SQLTransformer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VectorAssembler"><span class="nav-number">2.20.</span> <span class="nav-text">VectorAssembler</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#QuantileDiscretizer"><span class="nav-number">2.21.</span> <span class="nav-text">QuantileDiscretizer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Imputer"><span class="nav-number">2.22.</span> <span class="nav-text">Imputer</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Feature-Selectors"><span class="nav-number">3.</span> <span class="nav-text">Feature Selectors</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#VectorSlicer"><span class="nav-number">3.1.</span> <span class="nav-text">VectorSlicer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RFormula"><span class="nav-number">3.2.</span> <span class="nav-text">RFormula</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ChiSqSelector"><span class="nav-number">3.3.</span> <span class="nav-text">ChiSqSelector</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Locality-Sensitive-Hashing"><span class="nav-number">4.</span> <span class="nav-text">Locality Sensitive Hashing</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LSH-Operations"><span class="nav-number">4.1.</span> <span class="nav-text">LSH Operations</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Feature-Transformation"><span class="nav-number">4.1.1.</span> <span class="nav-text">Feature Transformation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Approximate-Similarity-Join"><span class="nav-number">4.1.2.</span> <span class="nav-text">Approximate Similarity Join</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Approximate-Nearest-Neighbor-Search"><span class="nav-number">4.1.3.</span> <span class="nav-text">Approximate Nearest Neighbor Search</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LSH-Algorithms"><span class="nav-number">4.2.</span> <span class="nav-text">LSH Algorithms</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Bucketed-Random-Projection-for-Euclidean-Distance"><span class="nav-number">4.2.1.</span> <span class="nav-text">Bucketed Random Projection for Euclidean Distance</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MinHash-for-Jaccard-Distance"><span class="nav-number">4.2.2.</span> <span class="nav-text">MinHash for Jaccard Distance</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#结束"><span class="nav-number">5.</span> <span class="nav-text">结束</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>

  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        
<div class="copyright">&copy; 2017 &mdash; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="heart">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">cgDeepLearn</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">51.9k</span>
  
</div>











<div class="weixin-box">
  <div class="weixin-menu">
    <div class="weixin-hover">
      <div class="weixin-description">别扫我~</div>
    </div>
  </div>
</div>
        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user-circle-o"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      <i class="fa fa-eye"></i>
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
  

  
  
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js"></script>
  

  
  
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/jquery_lazyload/1.9.7/jquery.lazyload.min.js"></script>
  

  
  
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/velocity/1.2.1/velocity.min.js"></script>
  

  
  
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/velocity/1.2.1/velocity.ui.min.js"></script>
  

  
  
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'zmCAwXGTK2bRhJxfvugh16C0-gzGzoHsz',
        appKey: 'FrwRKt14p639teOiqUuU7D6j',
        placeholder: 'ヾ(✿ﾟ▽ﾟ)ノ 来呀，快活呀!',
        avatar:'wavatar',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("zmCAwXGTK2bRhJxfvugh16C0-gzGzoHsz", "FrwRKt14p639teOiqUuU7D6j");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">

  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
      pbOptions = {};
      
          pbOptions.iconStyle = "box";
      
          pbOptions.boxForm = "horizontal";
      
          pbOptions.position = "bottomCenter";
      
          pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-postbottom', pbOptions);
    
    
  </script>

  
  <script type="text/javascript">
  wpac_init = window.wpac_init || [];
  wpac_init.push({widget: 'Rating', id: 9269,
    el: 'wpac-rating',
    color: 'fc6423'
  });
  (function() {
    if ('WIDGETPACK_LOADED' in window) return;
    WIDGETPACK_LOADED = true;
    var mc = document.createElement('script');
    mc.type = 'text/javascript';
    mc.async = true;
    mc.src = '//embed.widgetpack.com/widget.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
  })();
  </script>


  

  

  


  <!-- 
<div id="hexo-helper-live2d">
  <canvas id="live2dcanvas" width="150" height="300"></canvas>
</div>
<style>
  #live2dcanvas{
    position: fixed;
    width: 150px;
    height: 300px;
    opacity:0.7;
    right: 0px;
    z-index: 999;
    pointer-events: none;
    bottom: -20px;
  }
</style>
<script type="text/javascript" src="/live2d/device.min.js"></script>
<script type="text/javascript">
const loadScript = function loadScript(c,b){var a=document.createElement("script");a.type="text/javascript";"undefined"!=typeof b&&(a.readyState?a.onreadystatechange=function(){if("loaded"==a.readyState||"complete"==a.readyState)a.onreadystatechange=null,b()}:a.onload=function(){b()});a.src=c;document.body.appendChild(a)};
(function(){
  if((typeof(device) != 'undefined') && (device.mobile())){
    document.getElementById("live2dcanvas").style.width = '75px';
    document.getElementById("live2dcanvas").style.height = '150px';
  }else
    if (typeof(device) === 'undefined') console.error('Cannot find current-device script.');
  loadScript("/live2d/script.js", function(){loadlive2d("live2dcanvas", "/live2d/assets/z16.model.json", 0.5);});
})();
</script>
 -->

  <!-- 页面点击小红心 
  <script type="text/javascript" src="/js/src/love.js"></script>
  -->
  <!-- keywords -->
  <script type="text/javascript" src="/js/src/keywords.js"></script>


</body>
</html>
